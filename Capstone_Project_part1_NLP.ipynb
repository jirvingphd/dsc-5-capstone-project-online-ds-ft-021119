{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mHDiTjr5lm_U"
   },
   "source": [
    "# Capstone Project: Predicting the S&P 500 Using Trump's Part1: NLP\n",
    "\n",
    "- James M. Irving\n",
    "- james.irving.phd@gmail.com\n",
    "- Updated as of 06/18/21\n",
    "\n",
    "Flatiron Full Time Data Science 021119 Cohort\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">- **Note: this notebook (`Capstone_Project_part1_NLP.ipynb`) is one of 3 project notebooks..** \n",
    "    1. **Tweet Preprocessing and NLP Classifications**\n",
    "    2. Time Series Modeling of S&P 500\n",
    "    3. Combined NLP + Time Series Modeling with S&P500 and Trump's Tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üìö ABSTRACT:\n",
    "\n",
    "> Stock Market prices are notoriously difficult to model, but advances in machine learning algorithms in recent years provide renewed possibilities in accurately modeling market performance. One notable addition in modern machine learning is that of Natural Language Processing (NLP). For those modeling a specific stock, performing NLP feature extraction and analysis on the collection of news headlines, shareholder documents, or social media postings that mention the company can provide additional information about the human/social elements to predicting market behaviors. These insights could not be captured by historical price data and technical indicators alone.\n",
    "\n",
    "> President Donald J. Trump is one of the most prolific users of social media, specifically Twitter, using it as a direct messaging channel to his followers, avoiding the traditional filtering and restriction that normally controls the public influence of the President of the United States. An additional element of the presidency that Trump has avoided is that of financial transparency and divesting of assets. Historically, this is done in order to avoid conflicts of interest, apparent or actual. The president is also known to target companies directly with his Tweets, advocating for specific changes/decisions by the company, or simply airing his greivances. This leads to the natural question, how much influence *does* President Trump exert over the financial markets? \n",
    "\n",
    "> To explore this question, we built multiple types of models attempting to answer this question, using the S&P500 as our market index. First, we built a classification model to predict the change in stock price 60 mins after the tweet. We trained Word2Vec embeddings on President Trump's tweets since his election, which we used as the embedding layer for LSTM and GRU neural networks. \n",
    "\n",
    "> We next build a baseline time series regression model, using historical price data alone to predict price by trading-hour. We then built upon this, adding several technical indicators of market performance as additional features. \n",
    "Finally, we combined the predicitons of our classification model, as well as several other metrics about the tweets (sentiment scores, # of retweets/favorites, upper-to-lowercase ratio,etc.) to see if combining all of these sources of information could explain even more of the variance in stock market prices. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents Legend"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- üìö: Info sections\n",
    "- üïπ: Coding sections\n",
    "    - üéõ: **yperparameters to tune**\n",
    "    - üèãÔ∏è: fitting models\n",
    "    - ü§î: New Things to Potentially Try \n",
    "- Use the Table of Contents view on the left sidebar to find the relevant sections (button looks like a bulleted list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Capstone-Project:-Predicting-the-S&amp;P-500-Using-Trump's-Part1:-NLP\" data-toc-modified-id=\"Capstone-Project:-Predicting-the-S&amp;P-500-Using-Trump's-Part1:-NLP-1\">Capstone Project: Predicting the S&amp;P 500 Using Trump's Part1: NLP</a></span></li><li><span><a href=\"#üìö-ABSTRACT:\" data-toc-modified-id=\"üìö-ABSTRACT:-2\">üìö ABSTRACT:</a></span><ul class=\"toc-item\"><li><span><a href=\"#Table-of-Contents-Legend\" data-toc-modified-id=\"Table-of-Contents-Legend-2.1\">Table of Contents Legend</a></span></li><li><span><a href=\"#üìö-MAIN-QUESTION:\" data-toc-modified-id=\"üìö-MAIN-QUESTION:-2.2\">üìö MAIN QUESTION:</a></span><ul class=\"toc-item\"><li><span><a href=\"#REFERENCES-/-INSPIRATION:\" data-toc-modified-id=\"REFERENCES-/-INSPIRATION:-2.2.1\">REFERENCES / INSPIRATION:</a></span></li></ul></li><li><span><a href=\"#OVERVIEW-OF-DATA/FEATURES-USED-PER-MODEL\" data-toc-modified-id=\"OVERVIEW-OF-DATA/FEATURES-USED-PER-MODEL-2.3\">OVERVIEW OF DATA/FEATURES USED PER MODEL</a></span><ul class=\"toc-item\"><li><span><a href=\"#FINAL-MODEL:-COMBINING-STOCK-MARKET-DATA,--NLP-CLASSIFICATION,-AND-OTHER-TWEET-METRICS\" data-toc-modified-id=\"FINAL-MODEL:-COMBINING-STOCK-MARKET-DATA,--NLP-CLASSIFICATION,-AND-OTHER-TWEET-METRICS-2.3.1\">FINAL MODEL: COMBINING STOCK MARKET DATA,  NLP CLASSIFICATION, AND OTHER TWEET METRICS</a></span></li></ul></li><li><span><a href=\"#OSEMN-FRAMEWORK\" data-toc-modified-id=\"OSEMN-FRAMEWORK-2.4\">OSEMN FRAMEWORK</a></span><ul class=\"toc-item\"><li><span><a href=\"#OBTAIN\" data-toc-modified-id=\"OBTAIN-2.4.1\"><a href=\"#OBTAIN\">OBTAIN</a></a></span></li><li><span><a href=\"#SCRUB\" data-toc-modified-id=\"SCRUB-2.4.2\"><a href=\"#SCRUB\">SCRUB</a></a></span></li><li><span><a href=\"#EXPLORE-/-VISUALIZE\" data-toc-modified-id=\"EXPLORE-/-VISUALIZE-2.4.3\"><a href=\"#EXPLORE/VISUALIZE\">EXPLORE / VISUALIZE</a></a></span></li><li><span><a href=\"#MODELING-(Initial)\" data-toc-modified-id=\"MODELING-(Initial)-2.4.4\"><a href=\"#INITIAL-MODELING\">MODELING (Initial)</a></a></span></li><li><span><a href=\"#iNTERPRETATION\" data-toc-modified-id=\"iNTERPRETATION-2.4.5\">iNTERPRETATION</a></span></li></ul></li></ul></li><li><span><a href=\"#OBTAIN\" data-toc-modified-id=\"OBTAIN-3\">OBTAIN</a></span><ul class=\"toc-item\"><li><ul class=\"toc-item\"><li><span><a href=\"#üìö-DATA-SOURCES:\" data-toc-modified-id=\"üìö-DATA-SOURCES:-3.0.1\">üìö DATA SOURCES:</a></span></li></ul></li></ul></li><li><span><a href=\"#SCRUB\" data-toc-modified-id=\"SCRUB-4\">SCRUB</a></span><ul class=\"toc-item\"><li><ul class=\"toc-item\"><li><span><a href=\"#Importing-Functions\" data-toc-modified-id=\"Importing-Functions-4.0.1\">Importing Functions</a></span></li></ul></li><li><span><a href=\"#üìö-TRUMP'S-TWEETS\" data-toc-modified-id=\"üìö-TRUMP'S-TWEETS-4.1\">üìö TRUMP'S TWEETS</a></span><ul class=\"toc-item\"><li><span><a href=\"#Natural-Language-Processing-Info\" data-toc-modified-id=\"Natural-Language-Processing-Info-4.1.1\">Natural Language Processing Info</a></span></li><li><span><a href=\"#üéõTweet-Processing\" data-toc-modified-id=\"üéõTweet-Processing-4.1.2\">üéõTweet Processing</a></span></li><li><span><a href=\"#Calculating-delta_price_class-for-Each-Tweet\" data-toc-modified-id=\"Calculating-delta_price_class-for-Each-Tweet-4.1.3\">Calculating <code>delta_price_class</code> for Each Tweet</a></span></li><li><span><a href=\"#Add-Delta-Stock-Price-Data---For-Each-Tweet\" data-toc-modified-id=\"Add-Delta-Stock-Price-Data---For-Each-Tweet-4.1.4\">Add Delta Stock Price Data - For <em>Each</em> Tweet</a></span></li><li><span><a href=\"#Determining-Cutoffs-for-Delta-Price-Classes\" data-toc-modified-id=\"Determining-Cutoffs-for-Delta-Price-Classes-4.1.5\">Determining Cutoffs for Delta Price Classes</a></span></li></ul></li><li><span><a href=\"#EXPLORE/VISUALIZE\" data-toc-modified-id=\"EXPLORE/VISUALIZE-4.2\">EXPLORE/VISUALIZE</a></span><ul class=\"toc-item\"><li><span><a href=\"#Delta-Price-Classes\" data-toc-modified-id=\"Delta-Price-Classes-4.2.1\">Delta Price Classes</a></span></li><li><span><a href=\"#NLP-by-Delta-Price-Class\" data-toc-modified-id=\"NLP-by-Delta-Price-Class-4.2.2\">NLP by Delta Price Class</a></span></li></ul></li></ul></li><li><span><a href=\"#INITIAL-MODELING\" data-toc-modified-id=\"INITIAL-MODELING-5\">INITIAL MODELING</a></span><ul class=\"toc-item\"><li><span><a href=\"#TWEET-DELTA-PRICE-CLASSIFICATON\" data-toc-modified-id=\"TWEET-DELTA-PRICE-CLASSIFICATON-5.1\">TWEET DELTA PRICE CLASSIFICATON</a></span><ul class=\"toc-item\"><li><span><a href=\"#Creating-Word-Embeddings-with-Word2Vec\" data-toc-modified-id=\"Creating-Word-Embeddings-with-Word2Vec-5.1.1\">Creating Word Embeddings with Word2Vec</a></span></li><li><span><a href=\"#Class-Balancing\" data-toc-modified-id=\"Class-Balancing-5.1.2\">Class Balancing</a></span></li><li><span><a href=\"#Tokenization,-X,y-train-test-split\" data-toc-modified-id=\"Tokenization,-X,y-train-test-split-5.1.3\">Tokenization, X,y train-test-split</a></span></li><li><span><a href=\"#\" data-toc-modified-id=\"-5.1.4\"></a></span></li></ul></li><li><span><a href=\"#Updated-Modeling-Functions\" data-toc-modified-id=\"Updated-Modeling-Functions-5.2\">Updated Modeling Functions</a></span></li><li><span><a href=\"#Model-0\" data-toc-modified-id=\"Model-0-5.3\">Model 0</a></span><ul class=\"toc-item\"><li><span><a href=\"#Troubleshooting-06/17/21\" data-toc-modified-id=\"Troubleshooting-06/17/21-5.3.1\">Troubleshooting 06/17/21</a></span></li></ul></li></ul></li><li><span><a href=\"#BOOKMARK-06/17/21\" data-toc-modified-id=\"BOOKMARK-06/17/21-6\">BOOKMARK 06/17/21</a></span><ul class=\"toc-item\"><li><ul class=\"toc-item\"><li><span><a href=\"#Model-0A-Summary\" data-toc-modified-id=\"Model-0A-Summary-6.0.1\">Model 0A Summary</a></span></li></ul></li><li><span><a href=\"#Model-0B\" data-toc-modified-id=\"Model-0B-6.1\">Model 0B</a></span><ul class=\"toc-item\"><li><span><a href=\"#Model-0B-Summary\" data-toc-modified-id=\"Model-0B-Summary-6.1.1\">Model 0B Summary</a></span></li></ul></li></ul></li><li><span><a href=\"#FORECASTING-STOCK-MARKET-PRICE\" data-toc-modified-id=\"FORECASTING-STOCK-MARKET-PRICE-7\">FORECASTING STOCK MARKET PRICE</a></span><ul class=\"toc-item\"><li><span><a href=\"#Loading-&amp;-Processing-Stock-Data-(SCRUB)\" data-toc-modified-id=\"Loading-&amp;-Processing-Stock-Data-(SCRUB)-7.1\">Loading &amp; Processing Stock Data (SCRUB)</a></span></li><li><span><a href=\"#Load-in-raw-text-file-with-minute-resolutin-S&amp;P-500-prices\" data-toc-modified-id=\"Load-in-raw-text-file-with-minute-resolutin-S&amp;P-500-prices-7.2\">Load in raw text file with minute-resolutin S&amp;P 500 prices</a></span></li><li><span><a href=\"#Model-1:-Using-Price-as-only-feature\" data-toc-modified-id=\"Model-1:-Using-Price-as-only-feature-7.3\">Model 1: Using Price as only feature</a></span><ul class=\"toc-item\"><li><span><a href=\"#Model-1-Summary\" data-toc-modified-id=\"Model-1-Summary-7.3.1\">Model 1 Summary</a></span></li></ul></li><li><span><a href=\"#Model-2:-Stock-Price-+-Technical-Indicators\" data-toc-modified-id=\"Model-2:-Stock-Price-+-Technical-Indicators-7.4\">Model 2: Stock Price + Technical Indicators</a></span><ul class=\"toc-item\"><li><span><a href=\"#Technical-Indicator-Details\" data-toc-modified-id=\"Technical-Indicator-Details-7.4.1\">Technical Indicator Details</a></span></li><li><span><a href=\"#Model-2:-Summary\" data-toc-modified-id=\"Model-2:-Summary-7.4.2\">Model 2: Summary</a></span></li></ul></li></ul></li><li><span><a href=\"#COMBINING-TWEET-STATS,-NLP-CLASSIFICATION,-AND-MARKET-DATA\" data-toc-modified-id=\"COMBINING-TWEET-STATS,-NLP-CLASSIFICATION,-AND-MARKET-DATA-8\">COMBINING TWEET STATS, NLP CLASSIFICATION, AND MARKET DATA</a></span><ul class=\"toc-item\"><li><ul class=\"toc-item\"><li><span><a href=\"#Loading-in-NLP-Model-for-Predictions\" data-toc-modified-id=\"Loading-in-NLP-Model-for-Predictions-8.0.1\">Loading in NLP Model for Predictions</a></span></li><li><span><a href=\"#Get-Predictions-for-Hour-Binned-Tweets\" data-toc-modified-id=\"Get-Predictions-for-Hour-Binned-Tweets-8.0.2\">Get Predictions for Hour-Binned Tweets</a></span></li></ul></li><li><span><a href=\"#Model-3:-Stock-Price-+-Indicators-+-NLP-Preds-&amp;-Tweet-Features\" data-toc-modified-id=\"Model-3:-Stock-Price-+-Indicators-+-NLP-Preds-&amp;-Tweet-Features-8.1\">Model 3: Stock Price + Indicators + NLP Preds &amp; Tweet Features</a></span><ul class=\"toc-item\"><li><span><a href=\"#Finalize-colums-for-final-model\" data-toc-modified-id=\"Finalize-colums-for-final-model-8.1.1\">Finalize colums for final model</a></span></li><li><span><a href=\"#Model-3-Summary\" data-toc-modified-id=\"Model-3-Summary-8.1.2\">Model 3 Summary</a></span></li></ul></li><li><span><a href=\"#Model-X:-XGB-Regression-+-Feature-Importance\" data-toc-modified-id=\"Model-X:-XGB-Regression-+-Feature-Importance-8.2\">Model X: XGB Regression + Feature Importance</a></span><ul class=\"toc-item\"><li><span><a href=\"#Model-Interpretation\" data-toc-modified-id=\"Model-Interpretation-8.2.1\">Model Interpretation</a></span></li><li><span><a href=\"#Model-X-Summary\" data-toc-modified-id=\"Model-X-Summary-8.2.2\">Model X Summary</a></span></li></ul></li></ul></li><li><span><a href=\"#Summary\" data-toc-modified-id=\"Summary-9\">Summary</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìö MAIN QUESTION:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> #### **Can the Twitter activity of Donald Trump explain fluctuations in the stock market?**\n",
    "\n",
    "**We will use a combination of traditional stock market forecasting combined with Natural Language Processing and word embeddings from President Trump's tweets to predict fluctuations in the stock market (using S&P 500 as index).**\n",
    "\n",
    "- **Question 1: Can we predict if stock prices will go up or down at a fixed time point, based on the language in Trump's tweets?**\n",
    "    - [NLP Model 0](#Model-0)<br><br>\n",
    "    \n",
    "- **Question 2: How well can explain stock market fluctuations using only historical price data?**\n",
    "    - [Stock Market Model 1](#Model-1:-Using-Price-as-only-feature)<br><br>\n",
    "- **Question 3: Does adding technical market indicators to our model improve its ability to predict stock prices?**\n",
    "    - [Stock Market Model 2](#Model-2:-Stock-Price-+-Technical-Indicators)<br><br>\n",
    "- **Question 4: Can the NLP predictions from Question 1, combined with all of the features from Question 3, as well as additional information regarding Trump's Tweets explain even more of the stock market fluctuations?**\n",
    "    - Stock Market Model 3\n",
    "    - Stock Market Model X<br><br>\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### REFERENCES / INSPIRATION:\n",
    "\n",
    "1. **Stanford Scientific Poster Using NLP ALONE to predict if stock prices increase or decrease 5 mins after Trump tweets.**  \n",
    "    - [Poster PDF LINK](http://cs229.stanford.edu/proj2017/final-posters/5140843.pdf)\n",
    "    - Best accuracy was X, goal 1 is to create a classifier on a longer timescale with superior results.\n",
    "    \n",
    "\n",
    "2. **TowardsDataScience Blog Plost on \"Using the latest advancements in deep learning to predict stock price movements.\"** \n",
    "     - [Blog Post link](https://towardsdatascience.com/aifortrading-2edd6fac689d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wofiIIk6hQtc"
   },
   "source": [
    "## OVERVIEW OF DATA/FEATURES USED PER MODEL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wofiIIk6hQtc"
   },
   "source": [
    "\n",
    "#### TWITTER DATA - CLASSIFICATION MODEL\n",
    "**Trained Word2Vec embeddings on collection of Donal Trump's Tweets.**\n",
    "- Used negative skip-gram method and negative sampling to best represent infrequently used words.\n",
    "    \n",
    "**Classified tweets based on change in stock price (delta_price)**\n",
    "- Calculated price change from time of tweet to 60 mins later.\n",
    "    - \"No Change\" if the delta price was < \\\\$0.05 \n",
    "    - \"Increase\" if delta price was >+\\\\$0.05\n",
    "    - \"Decrease if delta price was >-\\\\$0.05\n",
    "    \n",
    "*NOTE: This model's predictions will become a feature in our final model.*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wofiIIk6hQtc"
   },
   "source": [
    "#### STOCK MARKET (S&P 500) DATA :\n",
    "##### TIME SERIES FORECASTING USING MARKET DATA\n",
    "**Model 1: Use price alone to forecast hourly price.**\n",
    "- Train model using time sequences of 7-trading-hours (1 day) to predict the following hour. \n",
    "    * [x] ~~SARIMAX model~~\n",
    "    * [x] LSTM neural network \n",
    "\n",
    "**Model 2: Use price combined with technical indicators.**\n",
    "    * LSTM neural network\n",
    "- **Calculate 7 technical indicators from S&P 500 hourly closing price.**\n",
    "    * [x] 7 days moving average \n",
    "    * [x] 21 days moving average\n",
    "    * [x] exponential moving average\n",
    "    * [x] momentum\n",
    "    * [x] Bollinger bands\n",
    "    * [x] MACD\n",
    "    \n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wofiIIk6hQtc"
   },
   "source": [
    "### FINAL MODEL: COMBINING STOCK MARKET DATA,  NLP CLASSIFICATION, AND OTHER TWEET METRICS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wofiIIk6hQtc"
   },
   "source": [
    "- **FEATURES FOR FINAL MODEL:**<br><br>\n",
    "    - **Stock Data:**\n",
    "        * [x] 7 days moving average \n",
    "        * [x] 21 days moving average\n",
    "        * [x] exponential moving average\n",
    "        * [x] momentum\n",
    "        * [x] Bollinger bands\n",
    "        * [x] MACD<br><br>\n",
    "    - **Tweet Data:**\n",
    "        * [x] 'delta_price' prediction classification for body of tweets from prior hour (model 0)\n",
    "        * [x] Number of tweets in hour\n",
    "        * [x] Ratio of uppercase:lowercase ratio (case_ratio)\n",
    "        * [x] Total # of favorites for the tweets\n",
    "        * [x] Total # of retweets for the tweets\n",
    "        * [x] Sentiment Scores:\n",
    "            - [x] Individual negative, neutral, and positive sentiment scores\n",
    "            - [x] Compound Sentiment Score (combines all 3)\n",
    "            - [x] sentiment class (+/- compound score)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OSEMN FRAMEWORK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [OBTAIN](#OBTAIN)\n",
    "- Obtaining 1-min resolution stock market data (S&P 500 Index)\n",
    "- Obtain batch of historical tweets by President Trump \n",
    "\n",
    "### [SCRUB](#SCRUB)\n",
    "1. **[Tweets](#TRUMP'S-TWEETS)**\n",
    "    - Preprocessing for Natural Language Processing<br><br>\n",
    "2. **[Stock Market](#Loading-&-Processing-Stock-Data-(SCRUB))**\n",
    "    - Time frequency conversion\n",
    "    - Technical Indicator Calculation\n",
    "\n",
    "### [EXPLORE / VISUALIZE](#EXPLORE/VISUALIZE)\n",
    "- [Tweet Delta Price Classes](#Delta-Price-Classes) \n",
    "- [NLP Figures / Example Tweets](#Natural-Language-Processing)\n",
    "- [S&P 500 Price](#Model-1:-Using-Price-as-only-feature)\n",
    "- [S&P 500 Technical Indicators](#Technical-Indicator-Details)\n",
    "\n",
    "### [MODELING (Initial)](#INITIAL-MODELING)\n",
    "- [Delta-Stock-Price NLP Classifier](#TWEET-DELTA-PRICE-CLASSIFICATON)\n",
    "- [S&P 500 Neural Network (price only)] ( )\n",
    "\n",
    "### iNTERPRETATION \n",
    "- Delta-Stock-Price NLP Models\n",
    "    - Model 0A Summary\n",
    "    - Model 0B Summary\n",
    "    \n",
    "- Stock-Market-Forecasting\n",
    "    - Model 1 Summary\n",
    "    - Model 2 Summary\n",
    "    - Model 3 Summary\n",
    "    - Model 4 Summary\n",
    "- Final Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OBTAIN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wofiIIk6hQtc"
   },
   "source": [
    "### üìö DATA SOURCES:\n",
    "\n",
    "* **All Donald Trump tweets from 12/01/2016 (pre-inaugaration day) to end of 08/23/2018**\n",
    "    *          Extracted from http://www.trumptwitterarchive.com/\n",
    "\n",
    "* **Minute-resolution data for the S&P500 covering the same time period.**\n",
    "    *         IVE S&P500 Index from - http://www.kibot.com/free_historical_data.aspx\n",
    "    \n",
    "    \n",
    "* NOTE: Both sources required manual extraction and both 1-min historical stock data and batch-historical-tweet data are difficult to obtain without paying \\\\$150-\\\\$2000 monthly developer memberships. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SCRUB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-18T00:31:06.557966Z",
     "start_time": "2021-06-18T00:31:05.014941Z"
    }
   },
   "outputs": [],
   "source": [
    "## Personal Functions \n",
    "# Note: the bs_ds package on pip is not compatible with python 3.8+\n",
    "# Therefore I am importing it locally instead\n",
    "\n",
    "## IMPORT CUSTOM CAPSTONE FUNCTIONS\n",
    "%load_ext autoreload\n",
    "%autoreload 2 \n",
    "import bsds as bs\n",
    "from bsds import ihelp,ihelp_menu,reload, inspect_variables\n",
    "from bsds import functions_combined_BEST as ji\n",
    "from bsds import functions_io as io\n",
    "# from bsds.imports import *\n",
    "\n",
    "\n",
    "## The Basics \n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "#Set pd.set_options for tweet visibility\n",
    "pd.set_option('display.max_colwidth',200)\n",
    "pd.set_option('display.max_columns',50)\n",
    "\n",
    "\n",
    "print(f\"Pandas v: \\t {pd.__version__:>5}\")\n",
    "print(f\"Numpy v: \\t{np.__version__:>5}\")\n",
    "print(f\"Seaborn v:\\t {sns.__version__:>5}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-18T00:31:07.112162Z",
     "start_time": "2021-06-18T00:31:06.560186Z"
    }
   },
   "outputs": [],
   "source": [
    "## NLP TOOLS\n",
    "import nltk\n",
    "nltk.download('vader_lexicon')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-18T00:31:08.109478Z",
     "start_time": "2021-06-18T00:31:07.114898Z"
    }
   },
   "outputs": [],
   "source": [
    "# Import plotly and cufflinks for iplots\n",
    "import plotly\n",
    "import cufflinks as cf\n",
    "from plotly import graph_objs as go\n",
    "from plotly.offline import iplot\n",
    "cf.go_offline()\n",
    "\n",
    "# Suppress warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-18T00:31:08.159043Z",
     "start_time": "2021-06-18T00:31:08.112873Z"
    }
   },
   "outputs": [],
   "source": [
    "## IMPORT CONVENIENCE FUNCTIONS\n",
    "from pprint import pprint\n",
    "# import qgrid\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-18T00:31:08.247700Z",
     "start_time": "2021-06-18T00:31:08.161049Z"
    }
   },
   "outputs": [],
   "source": [
    "file_dict = io.def_filename_dictionary(load_prior=False, save_directory=True)\n",
    "# file_dict = ji.load_filename_directory()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìö TRUMP'S TWEETS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Natural Language Processing Info\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "To prepare Donal Trump's tweets for modeling, **it is essential to preprocess the text** and simplify its contents.\n",
    "<br><br>\n",
    "1. **At a minimum, things like:**\n",
    "    - punctuation\n",
    "    - numbers\n",
    "    - upper vs lowercase letters<br>\n",
    "    ***must*** be addressed before any initial analyses. I refer tho this initial cleaning as **\"minimal cleaning\"** of the text content<br>\n",
    "    \n",
    "> Version 1 of the tweet processing removes these items, as well as the removal of any urls in a tweet. The resulting data column is referred to here as \"content_min_clean\".\n",
    "\n",
    "<br><br>\n",
    "2. It is **always recommended** that go a step beyond this and<br> remove **commonly used words that contain little information** <br>for our machine learning algorithms. Words like: (the,was,he,she, it,etc.)<br> are called **\"stopwords\"**, and it is critical to address them as well.\n",
    "\n",
    "> Version 2 of the tweet processing removes these items and the resulting data column is referred here as `cleaned_stopped_content`\n",
    "\n",
    "<br>\n",
    "\n",
    "3. Additionally, many analyses **need the text tokenzied** into a list of words<br> and not in a natural sentence format. Instead, they are a list of words (**tokens**) separated by \",\", which tells the algorithm what should be considered one word.<br><br>For the tweet processing, I used a version of tokenization, called `regexp_tokenziation` <br>which uses pattern of letters and symbols (the `expression`) <br>that indicate what combination of alpha numeric characters should be considered a single token.<br><br>The pattern I used was `\"([a-zA-Z]+(?:'[a-z]+)?)\"`, which allows for words such as \"can't\" that contain \"'\" in the middle of word. This processes was actually applied in order to process Version 1 and 2 of the Tweets, but the resulting text was put back into sentence form. \n",
    "\n",
    "> Version 3 of the tweets keeps the text in their regexp-tokenized form and is reffered to as `cleaned_stopped_tokens`\n",
    "<br>\n",
    "\n",
    "4. While not always required, it is often a good idea to reduce similar words down to a shared core.\n",
    "There are often **multiple variants of the same word with the same/simiar meaning**,<br> but one may plural **(i.e. \"democrat\" and \"democrats\")**, or form of words is different **(i.e. run, running).**<br> Simplifying words down to the basic core word (or word *stem*) is referred to as **\"stemming\"**. <br><br> A more advanced form of this also understands things like words that are just in a **different tense** such as  i.e.  **\"ran\", \"run\", \"running\"**. This process is called  **\"lemmatization**, where the words are reduced to their simplest form, called \"**lemmas**\"<br>  \n",
    "> Version 4 of the tweets are all reduced down to their word lemmas, futher aiding the algorithm in learning the meaning of the texts.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üéõTweet Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-18T00:31:08.294203Z",
     "start_time": "2021-06-18T00:31:08.249262Z"
    }
   },
   "outputs": [],
   "source": [
    "## SHow interactive code menus\n",
    "SHOW_MENUS = False\n",
    "\n",
    "if SHOW_MENUS:\n",
    "    ihelp_menu([ji.load_raw_twitter_file,\n",
    "               ji.make_stopwords_list,\n",
    "               ji.full_twitter_df_processing,\n",
    "               ji.full_sentiment_analysis])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-18T00:31:16.295265Z",
     "start_time": "2021-06-18T00:31:08.296212Z"
    }
   },
   "outputs": [],
   "source": [
    "## Load in raw csv of twitter_data, create date_time_index, rename columns\n",
    "raw_tweets = file_dict['twitter_df']['raw_tweet_file']\n",
    "twitter_df = ji.load_raw_twitter_file(filename=raw_tweets, \n",
    "                         date_as_index=True,\n",
    "                         rename_map={'text': 'content',\n",
    "                                     'created_at': 'date'})\n",
    "\n",
    "## Create list of stopwords for twitter processing\n",
    "stop_words = ji.make_stopwords_list(incl_punc=True, incl_nums=True,\n",
    "                                    add_custom=['http','https',\n",
    "                                                '...','‚Ä¶','``',\n",
    "                                                'co','‚Äú','‚Äú','‚Äô','‚Äò','‚Äù',\n",
    "                                                \"n't\",\"''\",'u','s',\"'s\",\n",
    "                                                '|','\\\\|','amp',\"i'm\",\"mr\"])\n",
    "\n",
    "## Process twitter data: \n",
    "# 1. create minimally cleaned column `content_min_clean` with urls\n",
    "twitter_df = ji.full_twitter_df_processing(twitter_df,\n",
    "                                           raw_tweet_col='content',\n",
    "                                           name_for_cleaned_tweet_col='content_cleaned',\n",
    "                                           name_for_stopped_col='cleaned_stopped_content', \n",
    "                                           name_for_tokenzied_stopped_col='cleaned_stopped_tokens',\n",
    "                                           use_col_for_case_ratio=None, \n",
    "                                           use_col_for_sentiment='content_min_clean',\n",
    "                                           RT=True, urls=True, hashtags=True, mentions=True,\n",
    "                                           str_tags_mentions=True,\n",
    "                                           stopwords_list=stop_words, force=False)\n",
    "## Display Index information\n",
    "ji.index_report(twitter_df,label='twitter_df')\n",
    "\n",
    "## Check for strings that exceed the correct tweet length\n",
    "keep_idx = ji.check_length_string_column(twitter_df, 'content_min_clean',length_cutoff=400,display_describe=False)\n",
    "## verify no issues arise.\n",
    "if keep_idx.isna().sum()>0:\n",
    "    raise Exception('')\n",
    "else:\n",
    "    twitter_df=twitter_df[keep_idx]\n",
    "    print(f'removed {np.sum(keep_idx==False)}')\n",
    "\n",
    "ji.check_length_string_column(twitter_df, 'content_min_clean',length_cutoff=400,return_keep_idx=False)\n",
    "twitter_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-18T00:31:16.354562Z",
     "start_time": "2021-06-18T00:31:16.298773Z"
    }
   },
   "outputs": [],
   "source": [
    "## PREVIEW THE DIFFERENT VERSION OF THE CLEANED DATA\n",
    "ji.display_same_tweet_diff_cols(twitter_df )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-18T00:31:16.799260Z",
     "start_time": "2021-06-18T00:31:16.358622Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## can also use function to pull up tweets by index\n",
    "ji.display_same_tweet_diff_cols(twitter_df, index='08-23-2019')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-18T00:31:16.922602Z",
     "start_time": "2021-06-18T00:31:16.801377Z"
    }
   },
   "outputs": [],
   "source": [
    "## Search all tweets for occurances of specific words\n",
    "word = 'russia'\n",
    "idx_russia_tweets = ji.search_for_tweets_with_word(twitter_df, word =word,\n",
    "                                     display_n=5, from_column='content',\n",
    "                                     return_index=True, display_df=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating `delta_price_class` for Each Tweet\n",
    "#### Using S&P 500 Price  1-min-resolution for `delta_price`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-18T00:31:16.967859Z",
     "start_time": "2021-06-18T00:31:16.924439Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "if SHOW_MENUS:\n",
    "    ihelp_menu([ji.load_raw_stock_data_from_txt, \n",
    "                ji.set_timeindex_freq])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-18T00:31:17.019716Z",
     "start_time": "2021-06-18T00:31:16.969981Z"
    }
   },
   "outputs": [],
   "source": [
    "# ## Load in \n",
    "# fname = file_dict['stock_df']['raw_csv_file']\n",
    "# stock_df = ji.load_raw_stock_data_from_txt(filename = fname,\n",
    "#                                                verbose=2)\n",
    "# fig = ji.plotly_time_series(stock_df, y_col='BidClose',as_figure=True)#,fig_dim=(300,500))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add Delta Stock Price Data - For *Each* Tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-18T00:31:17.067506Z",
     "start_time": "2021-06-18T00:31:17.021963Z"
    }
   },
   "outputs": [],
   "source": [
    "if SHOW_MENUS:\n",
    "    ihelp_menu(ji.load_twitter_df_stock_price)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-18T00:31:28.597971Z",
     "start_time": "2021-06-18T00:31:17.069391Z"
    }
   },
   "outputs": [],
   "source": [
    "print(f\"[i] # number of tweets = {twitter_df.shape[0]}\")\n",
    "\n",
    "## add stock_price for twitter_df\n",
    "null_ratio = ji.check_null_small(twitter_df,null_index_column='case_ratio')\n",
    "\n",
    "print(f'[!] {len(null_ratio)} null values for \"case_ratio\" are tweets containing only urls. Dropping...')\n",
    "twitter_df.dropna(subset=['is_retweet','case_ratio'],inplace=True)\n",
    "print(f\"[i] New # of tweets = {twitter_df.shape[0]}\\n\")\n",
    "\n",
    "\n",
    "twitter_df = ji.load_twitter_df_stock_price(twitter_df, \n",
    "                                           get_stock_prices_per_tweet=True,\n",
    "                                           price_mins_after_tweet=60)\n",
    "\n",
    "ji.index_report(twitter_df);\n",
    "\n",
    "\n",
    "idx_null_delta = ji.check_null_small(twitter_df,null_index_column='delta_price');\n",
    "print(f\"[!] {len(idx_null_delta)} null values for 'delta_price' were off-hour tweets,\\\n",
    "more than 1 day before the market reopened. Dropping...\")\n",
    "twitter_df.dropna(subset=['delta_price'], inplace=True)\n",
    "\n",
    "print(f\"\\n[i] Final # of tweets = {twitter_df.shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-18T00:31:28.644101Z",
     "start_time": "2021-06-18T00:31:28.599973Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if SHOW_MENUS:\n",
    "    qdf = ji.column_report(twitter_df)\n",
    "    qdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Determining Cutoffs for Delta Price Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-18T00:31:28.697526Z",
     "start_time": "2021-06-18T00:31:28.646123Z"
    }
   },
   "outputs": [],
   "source": [
    "## Examine delta_price\n",
    "print(\"CURRENT # OF POSTITIVE AND NEGATIVE PRICE DELTAS:\")\n",
    "print(twitter_df['delta_price_class'].value_counts())\n",
    "\n",
    "## Examining Changes to classes if use a \"No Change\" cutoff of $0.05\n",
    "delta_price = twitter_df['delta_price']\n",
    "small_pos =[ 0 < x <.05 for x in delta_price] #/len(delta_price)\n",
    "small_neg = [-.05<x <0 for x in delta_price]\n",
    "\n",
    "sum_pos = np.sum(small_pos)\n",
    "sum_neg = np.sum(small_neg)\n",
    "print('\\nCHANGES TO CLASSES IF USING ATHRESHOLD OF $0.05:\\n','---'*12)\n",
    "print(f'# Positive Delta -> \"No Change\" = {sum_pos}')\n",
    "print(f'# Negative Delta -> \"No Change\" = {sum_neg}')\n",
    "print(f'# of Unchanged Classifications =  {len(delta_price)-(sum_pos+sum_neg)}')#\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-18T00:31:28.771246Z",
     "start_time": "2021-06-18T00:31:28.699695Z"
    }
   },
   "outputs": [],
   "source": [
    "## BIN DELTA PRICE CLASS\n",
    "bins = pd.IntervalIndex.from_tuples([\n",
    "    (-np.inf,-.05),\n",
    "    (-.05,.05),\n",
    "    (.05,np.inf)],\n",
    "    closed='left')\n",
    "\n",
    "indexer= bins.get_indexer(twitter_df['delta_price'])\n",
    "twitter_df['indexer'] = indexer\n",
    "\n",
    "# remap -1,0,1 to classes\n",
    "mapper ={-1:np.nan,\n",
    "         0:0,\n",
    "         1:1,\n",
    "         2:2}\n",
    "mapper2 = {0:'neg',\n",
    "          1:'no_change',\n",
    "          2:'pos'}\n",
    "\n",
    "\n",
    "twitter_df['delta_price_class_int']= twitter_df['indexer'].apply(lambda x: mapper[x])\n",
    "twitter_df['delta_price_class'] = twitter_df['delta_price_class_int'].apply(lambda x: mapper2[x])\n",
    "\n",
    "\n",
    "res1 = twitter_df['delta_price_class_int'].value_counts().reset_index()\n",
    "res2 = twitter_df['delta_price_class'].value_counts().reset_index()\n",
    "res_df =pd.concat([res1,res2],axis=1)\n",
    "\n",
    "\n",
    "res_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-18T00:31:29.620428Z",
     "start_time": "2021-06-18T00:31:28.773352Z"
    }
   },
   "outputs": [],
   "source": [
    "fname = file_dict['twitter_df']['twitter_df_post_stock_price']\n",
    "twitter_df.to_csv(fname)\n",
    "print(f\"[io] Saved twitter_df to {fname}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-18T00:31:29.709021Z",
     "start_time": "2021-06-18T00:31:29.622395Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "twitter_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EXPLORE/VISUALIZE \n",
    "### Delta Price Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-18T00:31:30.280819Z",
     "start_time": "2021-06-18T00:31:29.711011Z"
    }
   },
   "outputs": [],
   "source": [
    "# print(, np.sum(small_neg), len(delta_price))\n",
    "ji.plotly_price_histogram(twitter_df,show_fig=True,as_figure=False)\n",
    "ji.plotly_pie_chart(twitter_df, column_to_plot='delta_price_class',show_fig=True, as_figure=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NLP by Delta Price Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-18T00:31:30.405682Z",
     "start_time": "2021-06-18T00:31:30.282723Z"
    }
   },
   "outputs": [],
   "source": [
    "nlp_df = twitter_df.loc[twitter_df['delta_price_class']!='no_change'].copy()\n",
    "nlp_df.dropna(inplace=True)\n",
    "nlp_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-18T00:31:30.497364Z",
     "start_time": "2021-06-18T00:31:30.407594Z"
    }
   },
   "outputs": [],
   "source": [
    "# Generate wordclouds\n",
    "twitter_df_groups,twitter_group_text = ji.get_group_texts_for_word_cloud(nlp_df, \n",
    "                                                                      text_column='cleaned_stopped_lemmas', \n",
    "                                                                      groupby_column='delta_price_class')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-18T00:31:33.123207Z",
     "start_time": "2021-06-18T00:31:30.504042Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "ji.compare_word_clouds(text1=twitter_df_groups['pos']['joined'],\n",
    "                       label1='Stock Market Increased',\n",
    "                       text2= twitter_df_groups['neg']['joined'],\n",
    "                       label2='Stock Market Decreased',\n",
    "                       twitter_shaped = True, verbose=1,\n",
    "                       suptitle_y_loc=0.75,\n",
    "                       suptitle_text='Most Frequent Words by Stock Price +/- Change',\n",
    "                       wordcloud_cfg_dict={'collocations':True},\n",
    "                       save_file=True,filepath_folder='',\n",
    "                       png_filename=file_dict['nlp_figures']['word_clouds_compare'],\n",
    "                      **{'subplot_titles_fontdict':{'fontsize':26,'fontweight':'bold'},\n",
    "                        'suptitle_fontdict':{'fontsize':40,'fontweight':'bold'},\n",
    "                         'group_colors':{'group1':'green','group2':'red'},\n",
    "                        });"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-18T00:31:33.301471Z",
     "start_time": "2021-06-18T00:31:33.128137Z"
    }
   },
   "outputs": [],
   "source": [
    "## Comparing words ONLY unique to each group\n",
    "df_pos_words, df_neg_words = ji.compare_freq_dists_unique_words(text1=twitter_df_groups['pos']['text_tokens'],\n",
    "                                                                label1='Price Increased',\n",
    "                                                                text2=twitter_df_groups['neg']['text_tokens'],\n",
    "                                                                label2='Price Decreased',\n",
    "                                                                top_n=20, display_dfs=True,\n",
    "                                                                return_as_dicts=False)\n",
    "\n",
    "pos_freq_dict, neg_freq_dict = ji.compare_freq_dists_unique_words(text1=twitter_df_groups['pos']['text_tokens'],\n",
    "                                                                label1='Price Increased',\n",
    "                                                                text2=twitter_df_groups['neg']['text_tokens'],\n",
    "                                                                label2='Price Decreased',\n",
    "                                                                top_n=20, display_dfs=False,\n",
    "                                                                return_as_dicts=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-18T00:31:35.250346Z",
     "start_time": "2021-06-18T00:31:33.303462Z"
    }
   },
   "outputs": [],
   "source": [
    "## WORDCLOUD OF WORDS UNIQUE TO TWEETS THAT INCREASED VS DECREASED STOCK PRICE\n",
    "ji.reload(ji)\n",
    "\n",
    "ji.compare_word_clouds(text1= pos_freq_dict,label1='Stock Price Increased',\n",
    "                       text2=neg_freq_dict, label2='Stock Price Decreased',\n",
    "                       twitter_shaped=True, from_freq_dicts=True,\n",
    "                       suptitle_y_loc=0.75,wordcloud_cfg_dict={'collocations':True},\n",
    "                       suptitle_text='Words Unique to Stock Price +/- Change',\n",
    "                       save_file=True,filepath_folder='',\n",
    "                       png_filename=file_dict['nlp_figures']['word_clouds_compare_unique'],\n",
    "                       **{'subplot_titles_fontdict':\n",
    "                         {'fontsize':26,\n",
    "                         'fontweight':'bold'},\n",
    "                        'suptitle_fontdict':{\n",
    "                         'fontsize':40,\n",
    "                         'fontweight':'bold'},\n",
    "                         'group_colors':{\n",
    "                             'group1':'green','group2':'red'}\n",
    "                        });"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-18T00:31:35.753736Z",
     "start_time": "2021-06-18T00:31:35.252290Z"
    }
   },
   "outputs": [],
   "source": [
    "reload(ji)\n",
    "ji.make_tweet_bigrams_by_group(twitter_df_groups)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# INITIAL MODELING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TWEET DELTA PRICE CLASSIFICATON"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Word Embeddings with Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-18T00:31:35.808569Z",
     "start_time": "2021-06-18T00:31:35.755917Z"
    }
   },
   "outputs": [],
   "source": [
    "# # class Word2vecParams():\n",
    "# #     \"\"\"Class for tracking modelparams used for word2vec\"\"\"\n",
    "\n",
    "# #     def __init__(self,params=None,verbose=1):\n",
    "# #         '''Creates empty dataframe, last_params attribute, displays message'''\n",
    "# #         self.last_params = {}\n",
    "# #         self._df_ = pd.DataFrame(columns= ['text_column', 'window', 'min_count',\n",
    "# #                                            'epochs','sg', 'hs', 'negative', 'ns_exponent'])\n",
    "# #         if params is not None:\n",
    "            \n",
    "# #             self._df_.loc[self.get_now()] = params\n",
    "# #             self.last_params=params\n",
    "# #         else:\n",
    "# #             self.last_params ={'text_column': '',\n",
    "# #                                'window':'',\n",
    "# #                                'min_count':'',\n",
    "# #                                'epochs': '',\n",
    "# #                                'sg':'',\n",
    "# #                                'hs':'',\n",
    "# #                                'negative':'',\n",
    "# #                                'ns_exponent':''\n",
    "# #                               }\n",
    "# #         if verbose>0:\n",
    "# #             print('[i] call .params_template() for dict to copy/pate.')\n",
    "# # #             print('[i] call .show_info() to display param meanings.')\n",
    "\n",
    "# #     def get_df(self):\n",
    "# #         \"\"\"returns dataframe of all attempts\"\"\"\n",
    "# #         return self._df_\n",
    "    \n",
    "# #     def get_now(self):\n",
    "# #         \"\"\"calls external function to get current timestamp\"\"\"\n",
    "# #         import functions_io as io\n",
    "# #         return io.get_now()        \n",
    "            \n",
    "# #     def print_params(self):\n",
    "# #         \"\"\"prints self.last_params\"\"\"\n",
    "# #         print(self.last_params)\n",
    "        \n",
    "# #     def params_template(self):\n",
    "# #         \"\"\"Prints a template dictionary of possible values. Copy-paste ready.\"\"\"\n",
    "# #         print('#TEMPLATE(call.show_info() for details:')\n",
    "# #         print(self._template_)\n",
    "\n",
    "    \n",
    "# #     def append(self, params_dict):\n",
    "# #         \"\"\"Appends internal dataframe using the current time as the index\"\"\"\n",
    "# #         ts = self.get_now()\n",
    "# #         self._df_.loc[ts] = pd.Series(params_dict)\n",
    "# #         self.last_params = params_dict\n",
    "# #         print('- params saved.')\n",
    "    \n",
    "# #     def show_info(self):\n",
    "# #         from IPython.display import display\n",
    "# #         import pandas as pd\n",
    "# #         info_df = self.info\n",
    "# # #         info_df = info_df.style\n",
    "# #         capt_text = 'Word2Vec Model Params'\n",
    "# #         with pd.option_context('display.max_colwidth',0, 'display.colheader_justify','left'):\n",
    "# #             table_style =[{'selector':'caption',\n",
    "# #             'props':[('font-size','1.1em'),('color','darkblue'),('font-weight','bold'),\n",
    "# #             ('vertical-align','0%')]}]\n",
    "# #             dfs = info_df.style.set_caption(capt_text).set_properties(**{'width':'400px',\n",
    "# #             'text-align':'left',\n",
    "# # #             'padding':'1em',\n",
    "# #             'font-size':'1.2em'}).set_table_styles(table_style)\n",
    "# #             display(dfs)\n",
    "                \n",
    "# #     _template_=\"\"\"\n",
    "# #         w2vparams = {\n",
    "# #         'text_column': 'cleaned_stopped_lemmas',\n",
    "# #         'window':3-5,\n",
    "# #         'min_count':1-3,\n",
    "# #         'epochs':10-20,\n",
    "# #         'sg':0 or 1, \n",
    "# #         'hs':'0 or 1,\n",
    "# #         'negative': 0 or 5-20 ,\n",
    "# #         'ns_exponent':-1.0 to 1.0\n",
    "# #         }\"\"\"\n",
    "        \n",
    "# #     info = pd.DataFrame.from_dict({\n",
    "# #         'text_column':'column to sample',\n",
    "# #         'window':'# of words per window',\n",
    "# #         'min_count':'# of times word must appear',\n",
    "# #         'epochs':'# of training epochs',\n",
    "# #         'sg':\"embedding method. 1=skip-gram, 0=cbow (default)\",\n",
    "# #         'hs':\"1=hierarchical softmax (default). 0 = use negative sampling (if 'negative' is non-zero)\",\n",
    "# #         'negative':\"5-20, default=0. (# number of 'noisy' words to remove by negative sampling)\",\n",
    "# #         'ns_exponent':\"weight for sampling low/high freq words. 0.0=sample all equally, 1.0=sample by word frequency, (-) values = sample infrequent words more\"},\n",
    "# #         orient='index',columns=['info'])\n",
    "    \n",
    "           \n",
    "# def get_kwargs(params):\n",
    "#     ## get kwargs for make_word2vecmodel\n",
    "#     kwarg_keys =['sg','hs','negative','ns_exponent']\n",
    "#     kwargs = {k:params[k] for k in kwarg_keys}\n",
    "#     return kwargs\n",
    "\n",
    "# w2vParams = bs.Word2vecParams()        \n",
    "# w2vParams.params_template()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-18T00:31:35.961189Z",
     "start_time": "2021-06-18T00:31:35.810480Z"
    }
   },
   "outputs": [],
   "source": [
    "twitter_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-18T00:31:36.013257Z",
     "start_time": "2021-06-18T00:31:35.962896Z"
    }
   },
   "outputs": [],
   "source": [
    "# ## FITING WORD2VEC AND TOKENIZER    \n",
    "# params = {\n",
    "# 'text_column': 'content_min_clean',\n",
    "# 'window':3,\n",
    "# 'min_count':2,\n",
    "# 'epochs':10,\n",
    "# 'sg':0, \n",
    "# 'hs':1,\n",
    "# 'negative':0,\n",
    "# 'ns_exponent':0.0\n",
    "# }\n",
    "# # model_kwds=  get_kwargs(params)    \n",
    "\n",
    "# # text_data = twitter_df[params['text_column']]a\n",
    "# ## using df_tokenize for full body of a text for word2vec\n",
    "# word2vec_model = ji.make_word2vec_model(twitter_df,\n",
    "#                                         text_column = params['text_column'],\n",
    "#                                         window = params['window'],\n",
    "#                                         min_count= params['min_count'],\n",
    "#                                         epochs = params['epochs'],\n",
    "#                                         verbose=1,\n",
    "#                                         return_full=True,\n",
    "#                                         **model_kwds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-18T00:31:36.505740Z",
     "start_time": "2021-06-18T00:31:36.015240Z"
    }
   },
   "outputs": [],
   "source": [
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import Word2Vec\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-18T00:31:37.100177Z",
     "start_time": "2021-06-18T00:31:36.507871Z"
    }
   },
   "outputs": [],
   "source": [
    "twitter_df['simple_clean'] = twitter_df['content'].map( lambda x: simple_preprocess(x, deacc=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-18T00:31:37.250067Z",
     "start_time": "2021-06-18T00:31:37.102023Z"
    }
   },
   "outputs": [],
   "source": [
    "twitter_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-18T00:31:38.905912Z",
     "start_time": "2021-06-18T00:31:37.252201Z"
    }
   },
   "outputs": [],
   "source": [
    "## set the embedding size (normally I'd do 100, but doing 50 for time)\n",
    "EMBEDDING_SIZE = 300\n",
    "# EMBEDDING_SIZE = 100\n",
    "## intiitalize the w2v odel\n",
    "w2v_model = Word2Vec(twitter_df['simple_clean'], size=EMBEDDING_SIZE, window=5,\n",
    "                     min_count=3, workers=4, seed=321)\n",
    "\n",
    "w2v_model.corpus_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-18T00:31:39.514702Z",
     "start_time": "2021-06-18T00:31:38.907847Z"
    }
   },
   "outputs": [],
   "source": [
    "## Train w2v model\n",
    "w2v_model.train(twitter_df['simple_clean'],total_words=w2v_model.corpus_total_words,\n",
    "                epochs=w2v_model.epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-18T00:31:39.577703Z",
     "start_time": "2021-06-18T00:31:39.516419Z"
    }
   },
   "outputs": [],
   "source": [
    "wv = w2v_model.wv\n",
    "len(wv.index2word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-18T00:31:39.654994Z",
     "start_time": "2021-06-18T00:31:39.579751Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# w2vParams.append(params)\n",
    "\n",
    "# wv = word2vec_model.wv\n",
    "\n",
    "### USING WORD VECTOR MATH TO GET A FEEL FOR QUALITY OF MODEL\n",
    "# wv = word2vec_model.wv\n",
    "def V(string,wv=wv):\n",
    "    return wv.get_vector(string)\n",
    "def equals(vector,wv=wv):\n",
    "    return wv.similar_by_vector(vector)\n",
    "\n",
    "list_of_equations = [\"V('republican')-V('honor')\",\n",
    "                    \"V('man')+V('power')\",\n",
    "                     \"V('russia')+V('honor')\",\n",
    "                     \"V('china')+V('tariff')\",\n",
    "                     \"V('trump')+V('lie')\"]\n",
    "for eqn in list_of_equations:\n",
    "    print(f'\\n* {eqn} =')\n",
    "    cmd = f\"print(equals({eqn}))\"\n",
    "    eval(cmd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-18T00:31:39.735346Z",
     "start_time": "2021-06-18T00:31:39.657472Z"
    }
   },
   "outputs": [],
   "source": [
    "ji.dict_dropdown(file_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-18T00:31:39.795980Z",
     "start_time": "2021-06-18T00:31:39.737067Z"
    }
   },
   "outputs": [],
   "source": [
    "# import functions_io as io\n",
    "# reload(io)\n",
    "# io.save_word2vec(word2vec_model,file_dict,parms_dict=w2vParams.last_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Class Balancing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-18T00:31:39.861643Z",
     "start_time": "2021-06-18T00:31:39.798013Z"
    }
   },
   "outputs": [],
   "source": [
    "# twitter_df['simple_clean']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-18T00:31:39.937984Z",
     "start_time": "2021-06-18T00:31:39.863839Z"
    }
   },
   "outputs": [],
   "source": [
    "## Select smaller subset of twitter_df for df_tokenize\n",
    "columns_for_model_0 = ['delta_price_class','delta_price','pre_tweet_price',\n",
    "                       'post_tweet_price','delta_time','B_ts_rounded','B_ts_post_tweet','content',\n",
    "                       'content_min_clean','cleaned_stopped_content','cleaned_stopped_tokens',\n",
    "                       'cleaned_stopped_lemmas','delta_price_class_int',\n",
    "                       'simple_clean'\n",
    "                      ]\n",
    "\n",
    "df_tokenize=twitter_df[columns_for_model_0].copy()\n",
    "ji.check_class_balance(df_tokenize,'delta_price_class_int',as_raw=True, as_percent=False)\n",
    "ji.check_class_balance(df_tokenize,'delta_price_class',as_raw=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-18T00:31:40.058223Z",
     "start_time": "2021-06-18T00:31:39.940396Z"
    }
   },
   "outputs": [],
   "source": [
    "ihelp_menu([ji.undersample_df_to_match_classes])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-18T00:31:40.372823Z",
     "start_time": "2021-06-18T00:31:40.059931Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "## RESTRICTING TIME DELTAS FOR MODEL\n",
    "remove_delta_time_tweets=True\n",
    "\n",
    "## RESAMPLING \n",
    "undersample_to_match_classes = True# True\n",
    "class_column='delta_price_class'\n",
    "class_list_to_keep = None # None=all classes or ['neg','pos']\n",
    "\n",
    "## Display results\n",
    "show_tweet_versions = True\n",
    "\n",
    "\n",
    "print('[0] INITIAL CLASS COUNTS.')\n",
    "## Print initial class balance\n",
    "ji.check_class_balance(df_tokenize,col=class_column);\n",
    "\n",
    "## REMOVE TWEETS BASED ON TIME BETWEEN TWEET AND STOCK PRICE VALUE\n",
    "if remove_delta_time_tweets:\n",
    "    ## SAMPLE ONLY TWEETS WITHIN 1 DAY OF STOCK MARKET PRICE DATA\n",
    "    df_sampled = df_tokenize.loc[df_tokenize['delta_time']<'1 day']\n",
    "    print(f\"[1] # OF DAYS REMOVED BY 'delta_time' = {df_tokenize.shape[0]-df_sampled.shape[0]}\")\n",
    "    ji.check_class_balance(df_sampled, col=class_column, as_raw=True, as_percent=False)\n",
    "else:\n",
    "    print('[1] Skipping removing tweets by time_delta')\n",
    "    df_sampled = df_tokenize\n",
    "    \n",
    "    \n",
    "## UNDERSAMPLE FROM UNBALANCED CLASSES\n",
    "if undersample_to_match_classes:\n",
    "    \n",
    "    ## Print status\n",
    "    if class_list_to_keep is None:\n",
    "        print_class_list= list(df_sampled[class_column].unique())\n",
    "    else:\n",
    "        print_class_list = class_list_to_keep\n",
    "    print(f'[2] RESAMPLING DF TO MATCH SMALLEST CLASS.\\n\\tBalancing: {print_class_list}')\n",
    "    \n",
    "    ## RESAMPLE TO MATCH CLASSES\n",
    "    df_sampled = ji.undersample_df_to_match_classes(df_sampled,\n",
    "                                                    class_column=class_column,\n",
    "                                                    class_values_to_keep=class_list_to_keep,verbose=0)\n",
    "    ji.check_class_balance(df_sampled,col=class_column, as_percent=False)\n",
    "    \n",
    "    \n",
    "else:\n",
    "    print('\\n[2] Skipping balancing classes and keeping all 3 classes.')\n",
    "\n",
    "## Display final output\n",
    "dash = '---'*20\n",
    "print(f\"\\n\\n [i] Final class balance:\")\n",
    "ji.check_class_balance(df_sampled,col=class_column)\n",
    "\n",
    "display(df_sampled.head(2))\n",
    "\n",
    "show_tweet_versions=True\n",
    "if show_tweet_versions:\n",
    "    ji.display_same_tweet_diff_cols(df_sampled,\n",
    "                                    columns = ['content' ,'content_min_clean',\n",
    "                                               'cleaned_stopped_content',\n",
    "                                               'cleaned_stopped_tokens',\n",
    "                                              'cleaned_stopped_lemmas'],as_md=True) \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-18T00:31:40.466461Z",
     "start_time": "2021-06-18T00:31:40.374766Z"
    }
   },
   "outputs": [],
   "source": [
    "df_sampled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-18T00:31:40.531820Z",
     "start_time": "2021-06-18T00:31:40.468329Z"
    }
   },
   "outputs": [],
   "source": [
    "ji.check_class_balance(df_sampled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-18T00:31:40.594509Z",
     "start_time": "2021-06-18T00:31:40.533967Z"
    }
   },
   "outputs": [],
   "source": [
    "# ihelp_menu([ji.make_word2vec_model,ji.get_wv_from_word2vec,\n",
    "#             ji.train_test_val_split])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization, X,y train-test-split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-18T00:31:44.958302Z",
     "start_time": "2021-06-18T00:31:40.596559Z"
    }
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing import text, sequence\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-18T00:31:45.047042Z",
     "start_time": "2021-06-18T00:31:44.959931Z"
    }
   },
   "outputs": [],
   "source": [
    "X = df_sampled['simple_clean']\n",
    "y = to_categorical(df_sampled['delta_price_class_int'],num_classes=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-18T00:31:45.140885Z",
     "start_time": "2021-06-18T00:31:45.049058Z"
    }
   },
   "outputs": [],
   "source": [
    "## Get training/test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15)\n",
    "\n",
    "# ji.check_y_class_balance(data=[y_train,y_test])\n",
    "print(X_train.shape, y_train.shape)\n",
    "print(X_test.shape, y_test.shape)\n",
    "# del X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-18T00:31:45.342594Z",
     "start_time": "2021-06-18T00:31:45.142820Z"
    }
   },
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words=len(wv.vocab))\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "\n",
    "## Use tokenizer to convert texts to sequences\n",
    "X_train_seq = tokenizer.texts_to_sequences(X_train)\n",
    "X_test_seq = tokenizer.texts_to_sequences(X_test)\n",
    "\n",
    "## whatn does 1 sequence look like?\n",
    "print(X_train_seq[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-18T00:31:45.429971Z",
     "start_time": "2021-06-18T00:31:45.344627Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "## can lookup the words via the tokenizer's index_word \n",
    "print(' '.join([tokenizer.index_word[w] for w in X_train_seq[0]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-18T00:31:45.517317Z",
     "start_time": "2021-06-18T00:31:45.432269Z"
    }
   },
   "outputs": [],
   "source": [
    "# what is the len of each sequence?\n",
    "seq_lens = [len(x) for x in X_test_seq]\n",
    "MAX_SEQUENCE_LENGTH= max(seq_lens)\n",
    "MAX_SEQUENCE_LENGTH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-18T00:31:45.631504Z",
     "start_time": "2021-06-18T00:31:45.519705Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train_pad = sequence.pad_sequences(X_train_seq,MAX_SEQUENCE_LENGTH)\n",
    "X_test_pad = sequence.pad_sequences(X_test_seq,MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "\n",
    "## Save word indices\n",
    "word_index = tokenizer.index_word\n",
    "reverse_index = {v:k for k,v in word_index.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-18T00:31:45.718348Z",
     "start_time": "2021-06-18T00:31:45.633690Z"
    }
   },
   "outputs": [],
   "source": [
    "## prepare y\n",
    "\n",
    "# Changed for class imblanace  #\n",
    "# y = to_categorical(df_sampled['delta_price_class_int'],num_classes=3)\n",
    "# y=df_sampled['delta_price_class_int'].values\n",
    "\n",
    "# wv = ji.get_wv_from_word2vec(word2vec_model)\n",
    "\n",
    "# tokenizer = Tokenizer(num_words=len(wv.vocab))\n",
    "\n",
    "# ## FIGURE OUT WHICH VERSION TO USE WITH SERIES:\n",
    "# tokenizer.fit_on_texts(text_data)\n",
    "# # return integer-encoded sentences\n",
    "# X = tokenizer.texts_to_sequences(text_data)\n",
    "\n",
    "# MAX_SEQUENCE_LENGTH=35\n",
    "# max([len(x) for x in X])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-18T00:31:45.803166Z",
     "start_time": "2021-06-18T00:31:45.720964Z"
    }
   },
   "outputs": [],
   "source": [
    "len(word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-18T00:31:45.899847Z",
     "start_time": "2021-06-18T00:31:45.805172Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Updated Modeling Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-18T00:31:45.997058Z",
     "start_time": "2021-06-18T00:31:45.902734Z"
    }
   },
   "outputs": [],
   "source": [
    "def plot_history(history,model,figsize=(8,4)):\n",
    "    \"\"\"Takes a keras history and model and plots \n",
    "    all metrics in separate plots for each metric\"\"\"\n",
    "#     print(header,'\\t[i] MODEL HISTORY',header,sep='\\n')\n",
    "\n",
    "    ## Make a dataframe out of history\n",
    "    res_df = pd.DataFrame(history.history)#.plot()\n",
    "\n",
    "    ## Plot Losses\n",
    "    plot_kws = dict(marker='o',ls=':',lw=2,figsize=figsize)\n",
    "\n",
    "    ## Plot all metrics\n",
    "    metrics_list = model.metrics_names\n",
    "\n",
    "    for metric in metrics_list:\n",
    "        ax = res_df[[col for col in res_df.columns if metric in col]].plot(**plot_kws)\n",
    "        ax.set(xlabel='Epoch',ylabel=metric,title=metric)\n",
    "        ax.grid()\n",
    "        ax.xaxis.set_major_locator(mpl.ticker.MaxNLocator(integer=True))\n",
    "        plt.show()\n",
    "        \n",
    "        \n",
    "\n",
    "def evaluate_scores(model,X_train,y_train,label='Training',verbose=0):\n",
    "    \"\"\"Evaluates a keras model and prints the scores using the provided label.\"\"\"\n",
    "    train_scores  = model.evaluate(X_train,y_train,verbose=verbose)#score()\n",
    "    for i,metric in enumerate(model.metrics_names):\n",
    "        print(f\"\\t{label} {metric}: {train_scores[i]:.3f}\")\n",
    "        \n",
    "        \n",
    "        \n",
    "def classification_report_cm(model, X_train,y_train,label='TRAINING DATA',\n",
    "                            cm_figsize=(6,6),normalize='true',cmap='Greens'):\n",
    "    \"\"\"Gets predictions from a Keras neural network and get \n",
    "    classification report and confusion matrix.\"\"\"\n",
    "    ## Print report header, get preds, get class report, and conf matrix\n",
    "    header =  '==='*24\n",
    "    print(header,f\"\\t[i] CLASSIFICATION REPORT - {label}\",header,sep='\\n')\n",
    "    print()\n",
    "    \n",
    "    ## Get predictions\n",
    "    y_hat_train = model.predict(X_train)\n",
    "    \n",
    "    ## convert to 1D targets\n",
    "    y_train_class =y_train.argmax(axis=1)\n",
    "    y_hat_train_class = y_hat_train.argmax(axis=1)\n",
    "    \n",
    "    \n",
    "    ## Get classification report \n",
    "    print(metrics.classification_report(y_train_class,y_hat_train_class))\n",
    "    print()\n",
    "    \n",
    "    \n",
    "    ## Plot the confusion Matrix\n",
    "    cm = metrics.confusion_matrix(y_train_class, y_hat_train_class,\n",
    "                                  normalize=normalize)\n",
    "    \n",
    "    fig,ax = plt.subplots(figsize=cm_figsize)\n",
    "    sns.heatmap(cm, cmap=cmap, annot=True,square=True,ax=ax)\n",
    "    ax.set(ylabel='True Class',xlabel='Predicted Class',\n",
    "           title='Confusion Matrix - Training Data')    \n",
    "    plt.show()\n",
    "\n",
    "    \n",
    "    \n",
    "def evaluate_network(model, X_test, y_test, history=None, \n",
    "                        X_train = None, y_train = None,\n",
    "                        history_figsize = (8,4), cm_figsize=(8,8),\n",
    "                        cmap='Greens', normalize='true'):\n",
    "    \"\"\"Gets predictions and evaluates a classification model using\n",
    "    sklearn.\n",
    "\n",
    "    Args:\n",
    "        model (classifier): a fit keras classification model.\n",
    "        X_test (tensor/array): X data\n",
    "        y_test (tensor/array): y data\n",
    "        history (History object): model history from .fit\n",
    "        X_train (tensor/array): If provided, compare model.score \n",
    "                                for train and test. Defaults to None.\n",
    "        y_train (Series or Array, optional): If provided, compare model.score \n",
    "                                for train and test. Defaults to None.\n",
    "                                \n",
    "        history_figsize (tuple): figsize for each metric's history plot.\n",
    "        cm_figsize (tuple): figsize for confusion matrix plot\n",
    "      \n",
    "        cmap (str, optional): Colormap for confusion matrix. Defaults to 'Greens'.\n",
    "        normalize (str, optional): normalize argument for plot_confusion_matrix. \n",
    "                                    Defaults to 'true'.  \n",
    "    \"\"\"\n",
    "    \n",
    "    header =  '==='*24\n",
    "    \n",
    "    ## First, Plot History, if provided.\n",
    "    if history is not None:\n",
    "        print(header,'\\t[i] MODEL HISTORY',header,sep='\\n')\n",
    "        plot_history(history,model,figsize=history_figsize)\n",
    "        \n",
    "        \n",
    "    ## Evaluate Network for loss/acc scores\n",
    "    print(header,\"\\t[i] EVALUATING MODEL\",header,sep='\\n')\n",
    "    print()\n",
    "    if X_train is not None:\n",
    "        try:\n",
    "            evaluate_scores(model,X_train,y_train,label='Training')\n",
    "            print()\n",
    "\n",
    "        except Exception as e:\n",
    "            print(\"Error evaluating for accuracy for training data:\")\n",
    "            print(e)\n",
    "        \n",
    "\n",
    "    ## Evaluate test data\n",
    "    evaluate_scores(model,X_test,y_test,label='Test')\n",
    "    print(\"\\n\")\n",
    "\n",
    "    \n",
    "    ## Report for training data\n",
    "    if X_train is not None:\n",
    "        classification_report_cm(model, X_train, y_train, cmap=cmap,\n",
    "                                 normalize=normalize,\n",
    "                                 label='TRAINING DATA',cm_figsize=cm_figsize)       \n",
    "        print('\\n'*2)\n",
    "    ## Report for test data\n",
    "    classification_report_cm(model,X_test,y_test, cmap=cmap,\n",
    "                             normalize=normalize,\n",
    "                             label='TEST DATA',cm_figsize=cm_figsize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Troubleshooting 06/17/21"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-18T00:31:46.083367Z",
     "start_time": "2021-06-18T00:31:45.999191Z"
    }
   },
   "outputs": [],
   "source": [
    "## Set the max words equal to tokenizer's word index\n",
    "MAX_WORDS = len(tokenizer.word_index)\n",
    "MAX_WORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-18T00:31:46.172106Z",
     "start_time": "2021-06-18T00:31:46.085530Z"
    }
   },
   "outputs": [],
   "source": [
    "## Save num classes for final layer\n",
    "n_classes = y_train.shape[1]\n",
    "n_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-18T00:31:46.259536Z",
     "start_time": "2021-06-18T00:31:46.174016Z"
    }
   },
   "outputs": [],
   "source": [
    "## check class balance\n",
    "y_tr_classes = pd.Series(y_train.argmax(axis=1))\n",
    "y_tr_classes.value_counts(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-18T00:31:46.349551Z",
     "start_time": "2021-06-18T00:31:46.261490Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "## Get the array of weights for each unique class\n",
    "weights= compute_class_weight(\n",
    "           'balanced',\n",
    "            np.unique(y_tr_classes),\n",
    "            y_tr_classes)\n",
    "weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-18T00:31:46.443604Z",
     "start_time": "2021-06-18T00:31:46.351991Z"
    }
   },
   "outputs": [],
   "source": [
    "## Turn the weights into a dict with the class name as the key\n",
    "weights_dict = dict(zip( np.unique(y_tr_classes),weights))\n",
    "weights_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-18T00:31:46.531503Z",
     "start_time": "2021-06-18T00:31:46.445436Z"
    }
   },
   "outputs": [],
   "source": [
    "## Saving the total number of words as vocab size\n",
    "vocab_size = len(tokenizer.index_word)\n",
    "# EMBEDDING_SIZE = wv.vector_size\n",
    "## Doubel check current embedding size and vocab size\n",
    "vocab_size,EMBEDDING_SIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-18T00:31:46.640992Z",
     "start_time": "2021-06-18T00:31:46.533637Z"
    }
   },
   "outputs": [],
   "source": [
    "### make a metrix of embedding weights\n",
    "embedding_matrix = np.zeros((vocab_size+1, EMBEDDING_SIZE))\n",
    "\n",
    "## for each item in the word index\n",
    "for word, i in tokenizer.word_index.items():\n",
    "\n",
    "    ## if word in w2vec model, fill in the embedding matrix\n",
    "     if word in wv:\n",
    "        embedding_vector = wv[word]\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-18T00:31:46.727857Z",
     "start_time": "2021-06-18T00:31:46.643484Z"
    }
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-18T00:31:46.820533Z",
     "start_time": "2021-06-18T00:31:46.730157Z"
    }
   },
   "outputs": [],
   "source": [
    "embedding_layer = layers.Embedding(vocab_size+1,EMBEDDING_SIZE,\n",
    "                                  weights=[embedding_matrix],\n",
    "                                  input_length=MAX_SEQUENCE_LENGTH,\n",
    "                                  trainable=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-18T00:31:46.906511Z",
     "start_time": "2021-06-18T00:31:46.822637Z"
    }
   },
   "outputs": [],
   "source": [
    "#https://stackoverflow.com/questions/66207609/notimplementederror-cannot-convert-a-symbolic-tensor-lstm-2-strided-slice0-t/66207610\n",
    "import numpy\n",
    "numpy.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-18T00:31:46.990627Z",
     "start_time": "2021-06-18T00:31:46.908373Z"
    }
   },
   "outputs": [],
   "source": [
    "# !pip install -U numpy==1.18.5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-18T00:31:47.087510Z",
     "start_time": "2021-06-18T00:31:47.006366Z"
    }
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras import callbacks, models, layers, optimizers, regularizers\n",
    "import tensorflow as tf\n",
    "import matplotlib as mpl\n",
    "# def make_model():\n",
    "#     \"\"\"Make a neural network with a new emebdding layer, \n",
    "#     an LSTM layer with 25 unit, and a final Dense layer appropriate for the task\"\"\"\n",
    "#     model = models.Sequential()\n",
    "#     model.add(layers.Embedding(MAX_WORDS+1, EMBEDDING_SIZE))\n",
    "#     model.add(layers.LSTM(25, return_sequences=False))\n",
    "#     # model.add(layers.GlobalAveragePooling1D())\n",
    "#     model.add(layers.Dense(n_classes, activation='softmax'))\n",
    "    \n",
    "#     model.compile(loss='categorical_crossentropy', optimizer='adam',\n",
    "#                  metrics=['accuracy', tf.keras.metrics.Recall(name='recall')])\n",
    "#     display(model.summary())\n",
    "#     return model\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ## make model and fit \n",
    "# model= make_model()\n",
    "# history = model.fit(X_train_pad, y_train, batch_size=256, epochs=3)#,\n",
    "# #                     validation_split=0.2, workers=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-18T00:31:47.458089Z",
     "start_time": "2021-06-18T00:31:47.094267Z"
    }
   },
   "outputs": [],
   "source": [
    "# ca\n",
    "\n",
    "early_stop = callbacks.EarlyStopping(monitor='val_loss',#mode='min',min_delta=.001\n",
    "                                     patience=3,verbose=2)\n",
    "# callbacks=[early_stop]\n",
    "\n",
    "## Make model infrastructure:\n",
    "model0 = models.Sequential()\n",
    "\n",
    "model0.add(embedding_layer)\n",
    "\n",
    "# model0.add(layers.SpatialDropout1D(0.2))\n",
    "# model0.add(layers.LSTM(units=100, return_sequences=False,\n",
    "#                        dropout=0.3,recurrent_dropout=0.3,\n",
    "#                        kernel_regularizer=regularizers.l2(.01)))\n",
    "model0.add(layers.Bidirectional(layers.LSTM(units=100, return_sequences=False,\n",
    "                       dropout=0.3,recurrent_dropout=0.3,\n",
    "                       kernel_regularizer=regularizers.l2(.01))))\n",
    "\n",
    "model0.add(layers.Dense(3, activation='softmax'))\n",
    "\n",
    "model0.compile(loss='categorical_crossentropy',optimizer='adam',#\"adam\",\n",
    "               metrics=['accuracy',tf.keras.metrics.Recall()])#,'val_acc'])#, callbacks=callbacks)\n",
    "model0.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-18T00:33:18.752419Z",
     "start_time": "2021-06-18T00:31:47.460237Z"
    }
   },
   "outputs": [],
   "source": [
    "## set param\n",
    "num_epochs = 10\n",
    "validation_split = 0.2\n",
    "\n",
    "clock = bs.Clock()\n",
    "clock.tic()\n",
    "dashes = '---'*20\n",
    "print(f\"{dashes}\\n\\tFITTING MODEL:\\n{dashes}\")\n",
    "\n",
    "history0 = model0.fit(X_train_pad, y_train, \n",
    "                      epochs=num_epochs,\n",
    "                      verbose=True, \n",
    "                      validation_split=validation_split,\n",
    "                      batch_size=32,\n",
    "                      callbacks=[early_stop],\n",
    "                      class_weight=weights_dict\n",
    "                     )\n",
    "\n",
    "# clock.toc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-18T00:33:27.793190Z",
     "start_time": "2021-06-18T00:33:18.755415Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "evaluate_network(model0,X_test_pad,y_test,history0,\n",
    "                X_train = X_train_pad,y_train=y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-18T00:33:28.210225Z",
     "start_time": "2021-06-18T00:33:27.795669Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.dummy import DummyClassifier\n",
    "clf = DummyClassifier(strategy='stratified')\n",
    "bs.ml.fit_and_time_model(clf, X_train_pad, y_train.argmax(axis=1),\n",
    "                        X_test_pad,y_test.argmax(axis=1))\n",
    "\n",
    "# clf.fit(X_train_pad, y_train.argmax(axis=1))\n",
    "# preds = clf.predict(X_test_pad)\n",
    "# preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-18T00:33:28.319423Z",
     "start_time": "2021-06-18T00:33:28.212535Z"
    }
   },
   "outputs": [],
   "source": [
    "# y_train.argmax()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-18T00:33:28.432020Z",
     "start_time": "2021-06-18T00:33:28.322096Z"
    }
   },
   "outputs": [],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BOOKMARK 06/17/21"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-18T00:33:28.538664Z",
     "start_time": "2021-06-18T00:33:28.434577Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# cm_fname = file_dict['model_0A']['fig_conf_mat.ext']\n",
    "# hist_fname = file_dict['model_0A']['fig_keras_history.ext']\n",
    "# summary_fname = file_dict['model_0A']['model_summary']\n",
    "\n",
    "# df_class_report0A,fig0A=ji.evaluate_classification(model0,history0,\n",
    "#                                                    X_train_pad, X_test_pad,\n",
    "#                                                    y_train, y_test, \n",
    "#                                                    binary_classes=False,\n",
    "#                                                    conf_matrix_classes=['Decrease','No Change','Increase'],\n",
    "#                                                    normalize_conf_matrix=True, \n",
    "#                                                    save_history=True, history_filename=hist_fname,\n",
    "#                                                    save_conf_matrix_png=True, conf_mat_filename=cm_fname,\n",
    "#                                                    save_summary=True,summary_filename=summary_fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-18T00:33:28.647613Z",
     "start_time": "2021-06-18T00:33:28.541652Z"
    }
   },
   "outputs": [],
   "source": [
    "# ji.dict_dropdown(file_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-18T00:33:28.756983Z",
     "start_time": "2021-06-18T00:33:28.650073Z"
    }
   },
   "outputs": [],
   "source": [
    "# reload(ji)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-18T00:33:28.864592Z",
     "start_time": "2021-06-18T00:33:28.759519Z"
    }
   },
   "outputs": [],
   "source": [
    "# save_me_as_model_0A=True\n",
    "# save_me_as_pred_nlp = False\n",
    "\n",
    "# ji.reload(ji)\n",
    "# if save_me_as_pred_nlp:\n",
    "#     model_key='nlp_model_for_predictions'\n",
    "\n",
    "# elif save_me_as_model_0A:\n",
    "#     model_key='model_0A'    \n",
    "    \n",
    "# filename = file_dict[model_key]['base_filename']\n",
    "# nlp_files = ji.save_model_weights_params(model0,check_if_exists=True,auto_increment_name=True, \n",
    "#                                          auto_filename_suffix=True,filename_prefix=filename)\n",
    "\n",
    "# file_dict[model_key]['output_filenames'] = nlp_files\n",
    "\n",
    "# ji.update_file_directory(file_dict)\n",
    "# ji.dict_dropdown(file_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-18T00:33:28.977338Z",
     "start_time": "2021-06-18T00:33:28.866695Z"
    }
   },
   "outputs": [],
   "source": [
    "ihelp(model0.get_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 0A Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 0B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-18T00:33:29.120829Z",
     "start_time": "2021-06-18T00:33:28.979483Z"
    }
   },
   "outputs": [],
   "source": [
    "## GRU Model\n",
    "from tensorflow.keras import models, layers, optimizers, regularizers\n",
    "\n",
    "model0B = models.Sequential()\n",
    "\n",
    "## Get and add embedding_layer\n",
    "embedding_layer = ji.make_keras_embedding_layer(wv, X_train)\n",
    "model0B.add(embedding_layer)\n",
    "model0B.add(layers.SpatialDropout1D(0.2))\n",
    "model0B.add(layers.GRU(units=100, dropout=0.5, recurrent_dropout=0.2,return_sequences=True)) \n",
    "model0B.add(layers.GRU(units=100, dropout=0.5, recurrent_dropout=0.2))\n",
    "\n",
    "# model0.add(layers.Dense(units=50, activation='relu'))#, activation='tan' # activation='relu'))#removed 08/21\n",
    "\n",
    "model0B.add(layers.Dense(3, activation='softmax'))\n",
    "\n",
    "model0B.compile(loss='categorical_crossentropy',optimizer=\"adam\",metrics=['acc'])#,'val_acc'])#, callbacks=callbacks)\n",
    "model0B.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-18T00:33:29.123055Z",
     "start_time": "2021-06-18T00:31:08.307Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "ji.reload(ji)\n",
    "num_epochs = 10\n",
    "\n",
    "clock = bs.Clock()\n",
    "clock.tic()\n",
    "historyB = model0B.fit(X_train, y_train, epochs=num_epochs, verbose=True, validation_split=0.1,\n",
    "                     batch_size=300)#, class_weight=class_weight)#callbacks=callbacks,, validation_data=(X_val))\n",
    "clock.toc()\n",
    "\n",
    "\n",
    "model_key = \"model_0B\"\n",
    "cm_fname = file_dict[model_key]['fig_conf_mat.ext']\n",
    "hist_fname = file_dict[model_key]['fig_keras_history.ext']\n",
    "summary_fname = file_dict[model_key]['model_summary']\n",
    "\n",
    "df_class_report0B, fig0B  = ji.evaluate_classification(model0B, historyB, \n",
    "                           X_train, X_test, y_train,y_test,\n",
    "                           conf_matrix_classes=['Decrease','No Change','Increase'],\n",
    "                           binary_classes=False, normalize_conf_matrix=True, \n",
    "                           save_history=True, history_filename=hist_fname, \n",
    "                           save_conf_matrix_png=True, conf_mat_filename=cm_fname,\n",
    "                           save_summary=True,summary_filename=summary_fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-18T00:33:29.124672Z",
     "start_time": "2021-06-18T00:31:08.324Z"
    }
   },
   "outputs": [],
   "source": [
    "save_me_as_model_0B=True\n",
    "save_me_as_pred_nlp = False\n",
    "\n",
    "ji.reload(ji)\n",
    "if save_me_as_pred_nlp:\n",
    "    model_key='nlp_model_for_predictions'\n",
    "\n",
    "elif save_me_as_model_0B:\n",
    "    model_key='model_0B'    \n",
    "    \n",
    "filename = file_dict[model_key]['base_filename']\n",
    "nlp_files = ji.save_model_weights_params(model0B,check_if_exists=True,auto_increment_name=True, \n",
    "                                         auto_filename_suffix=True,filename_prefix=filename)\n",
    "\n",
    "file_dict[model_key]['output_filenames'] = nlp_files\n",
    "\n",
    "ji.update_file_directory(file_dict)\n",
    "ji.dict_dropdown(file_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 0B Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-18T00:33:29.126335Z",
     "start_time": "2021-06-18T00:31:08.355Z"
    }
   },
   "outputs": [],
   "source": [
    "ji.inspect_variables(locals())\n",
    "del_me= ['one_hot_results','nlp_df','text_data']#list of variable names\n",
    "for me in del_me:    \n",
    "    try: \n",
    "        exec(f'del {me}')\n",
    "        print(f'del {me} succeeded')\n",
    "    except:\n",
    "        print(f'del {me} failed')\n",
    "        continue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FORECASTING STOCK MARKET PRICE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading & Processing Stock Data (SCRUB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-18T00:33:29.128209Z",
     "start_time": "2021-06-18T00:31:08.375Z"
    }
   },
   "outputs": [],
   "source": [
    "# DISPLAY CODE TO BE USED BELOW TO LOAD AND PROCESS STOCK DATA\n",
    "functions_used=['ji.load_processed_stock_data', # This script combines the oriignal 4 used:\n",
    "                'ji.load_raw_stock_data_from_txt',\n",
    "                'ji.set_timeindex_freq','ji.custom_BH_freq',\n",
    "               'ji.get_technical_indicators']\n",
    "\n",
    "ji.ihelp_menu(functions_used)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load in raw text file with minute-resolutin S&P 500 prices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 1: Using Price as only feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-18T00:33:29.129823Z",
     "start_time": "2021-06-18T00:31:08.396Z"
    }
   },
   "outputs": [],
   "source": [
    "# try:\n",
    "#     stock_df\n",
    "# except: \n",
    "#     print('loading')\n",
    "#     stock_df = ji.load_processed_stock_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-18T00:33:29.131497Z",
     "start_time": "2021-06-18T00:31:08.412Z"
    }
   },
   "outputs": [],
   "source": [
    "reload(ji)\n",
    "fname = file_dict['stock_df']['raw_csv_file']\n",
    "raw_stock_df = ji.load_raw_stock_data_from_txt(filename = fname,\n",
    "                                               verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-18T00:33:29.133134Z",
     "start_time": "2021-06-18T00:31:08.428Z"
    }
   },
   "outputs": [],
   "source": [
    "fig = ji.plotly_time_series(raw_stock_df, y_col='BidClose',as_figure=True)\n",
    "stock_df = ji.get_technical_indicators(raw_stock_df,make_price_from='BidClose')\n",
    "del raw_stock_df\n",
    "\n",
    "# SELECT DESIRED COLUMNS\n",
    "stock_df = stock_df[[\n",
    "    'price','ma7','ma21','26ema','12ema','MACD','20sd',\n",
    "    'upper_band','lower_band','ema','momentum']]\n",
    "\n",
    "# Make stock_price for twitter functions\n",
    "stock_df.dropna(inplace=True)\n",
    "ji.index_report(stock_df)\n",
    "display(stock_df.head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-18T00:33:29.134690Z",
     "start_time": "2021-06-18T00:31:08.443Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ihelp_menu([ji.train_test_split_by_last_days,\n",
    "           ji.make_scaler_library,\n",
    "           ji.transform_cols_from_library,\n",
    "           ji.make_train_test_series_gens])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-18T00:33:29.136195Z",
     "start_time": "2021-06-18T00:31:08.459Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "## SPECIFY # OF TRAINING TEST DAYS \n",
    "num_test_days=5\n",
    "num_train_days= 260\n",
    "### SPECIFY Number of days included in each X_sequence (each prediction)\n",
    "days_for_x_window=1\n",
    "\n",
    "# Calculate number of rows to bin for x_windows\n",
    "periods_per_day = ji.get_day_window_size_from_freq( stock_df, ji.custom_BH_freq() )\n",
    "\n",
    "\n",
    "## Get the number of rows for x_window \n",
    "x_window = periods_per_day * days_for_x_window#data_params['days_for_x_window'] \n",
    "print(f'X_window size = {x_window} -- ({days_for_x_window} day(s) * {periods_per_day} rows/day)\\n')\n",
    "\n",
    "## Train-test-split by the # of days\n",
    "df_train, df_test = ji.train_test_split_by_last_days(stock_df,\n",
    "                                                     periods_per_day =periods_per_day, \n",
    "                                                     num_test_days   = num_test_days,\n",
    "                                                     num_train_days  = num_train_days,\n",
    "                                                     verbose=1, iplot=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-18T00:33:29.137684Z",
     "start_time": "2021-06-18T00:31:08.475Z"
    }
   },
   "outputs": [],
   "source": [
    "###### RESCALE DATA USING MinMaxScalers FIT ON TRAINING DATA's COLUMNS ######\n",
    "display(df_train.head(2).style.set_caption('df_train - pre-scaling'))\n",
    "\n",
    "scaler_library, df_train = ji.make_scaler_library(df_train, transform=True, verbose=1)\n",
    "\n",
    "df_test = ji.transform_cols_from_library(df_test, col_list=None,\n",
    "                                         scaler_library=scaler_library,\n",
    "                                         inverse=False)\n",
    "display(df_train.head(2).style.set_caption('df_train - post-scaling'))\n",
    "\n",
    "# Show transformed dataset\n",
    "# display( df_train.head(2).round(3).style.set_caption('training data - scaled'))\n",
    "\n",
    "# Create timeseries generators\n",
    "train_generator, test_generator = ji.make_train_test_series_gens( \n",
    "    df_train['price'], df_test['price'], \n",
    "    x_window=x_window,n_features=1,batch_size=1, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-18T00:33:29.139153Z",
     "start_time": "2021-06-18T00:31:08.490Z"
    }
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras.layers import Bidirectional, Dense, LSTM, Dropout\n",
    "from tensorflow.keras.regularizers import l2\n",
    "\n",
    "# Specifying input shape (size of samples, rank of samples?)\n",
    "n_input = x_window\n",
    "n_features = 1 # just stock Price\n",
    "\n",
    "print(f'input shape: ({n_input},{n_features})')\n",
    "input_shape=(n_input, n_features)\n",
    "\n",
    "# Create model architecture\n",
    "model1 = Sequential()\n",
    "model1.add(LSTM(units=50, input_shape =input_shape,return_sequences=True))#,  kernel_regularizer=l2(0.01),recurrent_regularizer=l2(0.01),\n",
    "#     model.add(Dropout(0.2))\n",
    "model1.add(LSTM(units=50, activation='relu'))\n",
    "model1.add(Dense(1))\n",
    "\n",
    "model1.compile(loss=ji.my_rmse, metrics=['acc'],\n",
    "              optimizer=optimizers.Nadam())\n",
    "\n",
    "display(model1.summary())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-18T00:33:29.140736Z",
     "start_time": "2021-06-18T00:31:08.505Z"
    }
   },
   "outputs": [],
   "source": [
    "## FIT MODEL\n",
    "dashes = '---'*20\n",
    "print(f\"{dashes}\\n\\tFITTING MODEL:\\n{dashes}\")\n",
    "\n",
    "## set params\n",
    "epochs=5\n",
    "\n",
    "# override keras warnings\n",
    "ji.quiet_mode(True,True,True)\n",
    "\n",
    "# Instantiating clock timer\n",
    "clock = bs.Clock()\n",
    "clock.tic('')\n",
    "\n",
    "# Fit the model\n",
    "history = model1.fit_generator(train_generator,\n",
    "                               epochs=epochs,\n",
    "                               verbose=2, \n",
    "                               use_multiprocessing=True,\n",
    "                               workers=3)\n",
    "\n",
    "\n",
    "clock.toc('')\n",
    "\n",
    "\n",
    "model_key = \"model_1\"\n",
    "hist_fname = file_dict[model_key]['fig_keras_history.ext']\n",
    "summary_fname = file_dict[model_key]['model_summary']\n",
    "\n",
    "# eval_results = ji.evaluate_model_plot_history(model1, train_generator, test_generator)\n",
    "ji.evaluate_regression_model(model1,history,\n",
    "                             train_generator=train_generator,\n",
    "                             test_generator=test_generator,\n",
    "                            true_test_series=df_test['price'],\n",
    "                            true_train_series =df_train['price'],\n",
    "                             save_history=True,history_filename=hist_fname,\n",
    "                             save_summary=True, summary_filename=summary_fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-18T00:33:29.142450Z",
     "start_time": "2021-06-18T00:31:08.520Z"
    }
   },
   "outputs": [],
   "source": [
    "### PREFER NEW WAY - GET DF_MODEL FIRST THEN GET EVALUATE_REGRESSION INFORMATION?\n",
    "## Get true vs pred data as a dataframe and iplot\n",
    "df_model1 = ji.get_model_preds_df(model1, \n",
    "                                  test_generator = test_generator,\n",
    "                                  true_train_series = df_train['price'],\n",
    "                                  true_test_series = df_test['price'],\n",
    "                                  include_train_data=True,\n",
    "                                  inverse_tf = True, \n",
    "                                  scaler = scaler_library['price'],\n",
    "                                  preds_from_gen = True, \n",
    "                                  preds_from_train_preds = True, \n",
    "                                  preds_from_test_preds = True,\n",
    "                                  iplot = True,\n",
    "                                  verbose=0)\n",
    "#                                   subplot_mode='lines+markers')\n",
    "    \n",
    "# Get evaluation metrics\n",
    "df_results1, dfs_results1, df_shifted1 =\\\n",
    "ji.compare_eval_metrics_for_shifts(df_model1['true_test_price'],\n",
    "                                   df_model1['pred_from_gen'],\n",
    "                                   shift_list=np.arange(-4,4,1),\n",
    "                                   true_train_series_to_add=df_model1['true_train_price'],\n",
    "                                   display_results=True,\n",
    "                                   display_U_info=True,\n",
    "                                   return_results=True,\n",
    "                                   return_styled_df=True,\n",
    "                                   return_shifted_df=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-18T00:33:29.143839Z",
     "start_time": "2021-06-18T00:31:08.535Z"
    }
   },
   "outputs": [],
   "source": [
    "ji.dict_dropdown(file_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-18T00:33:29.145486Z",
     "start_time": "2021-06-18T00:31:08.551Z"
    }
   },
   "outputs": [],
   "source": [
    "reload(ji)\n",
    "save_model=True\n",
    "ji.save_model_dfs(file_dict, 'model_1',df_model1,dfs_results1,df_shifted1)\n",
    "\n",
    "filename_prefix = file_dict['model_1']['base_filename']\n",
    "if save_model ==True:\n",
    "    model_1_output_files = ji.save_model_weights_params(model1,\n",
    "                                 filename_prefix=filename_prefix,\n",
    "                                 auto_increment_name=True,\n",
    "                                 auto_filename_suffix=True, \n",
    "                                 suffix_time_format='%m-%d-%y_%I%M%p',\n",
    "                                 save_model_layer_config_xlsx=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 1 Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 2: Stock Price + Technical Indicators"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FXUD8NkFm5r8"
   },
   "source": [
    "### Technical Indicator Details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-18T00:33:29.147111Z",
     "start_time": "2021-06-18T00:31:08.569Z"
    }
   },
   "outputs": [],
   "source": [
    "# SELECT DESIRED COLUMNS\n",
    "stock_df = stock_df[[\n",
    "    'price','ma7','ma21','26ema','12ema','MACD','20sd',\n",
    "    'upper_band','lower_band','ema','momentum']]\n",
    "\n",
    "# Make stock_price for twitter functions\n",
    "stock_df.dropna(inplace=True)\n",
    "ji.index_report(stock_df)\n",
    "display(stock_df.head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-18T00:33:29.148582Z",
     "start_time": "2021-06-18T00:31:08.585Z"
    }
   },
   "outputs": [],
   "source": [
    "fig =ji.plotly_technical_indicators(stock_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FXUD8NkFm5r8"
   },
   "source": [
    "1. **7 and 21 day moving averages**\n",
    "```python\n",
    "df['ma7'] df['price'].rolling(window = 7 ).mean() #window of 7 if daily data\n",
    "df['ma21'] df['price'].rolling(window = 21).mean() #window of 21 if daily data\n",
    "```    \n",
    "2. **MACD(Moving Average Convergence Divergence)**\n",
    "\n",
    "> Moving Average Convergence Divergence (MACD) is a trend-following momentumindicator that shows the relationship between two moving averages of a security‚Äôs price. The MACD is calculated by subtracting the 26-period Exponential Moving Average (EMA) from the 12-period EMA.\n",
    "\n",
    ">The result of that calculation is the MACD line. A nine-day EMA of the MACD, called the \"signal line,\" is then plotted on top of the MACD line, which can function as a trigger for buy and sell signals. \n",
    "\n",
    "> Traders may buy the security when the MACD crosses above its signal line and sell - or short - the security when the MACD crosses below the signal line. Moving Average Convergence Divergence (MACD) indicators can be interpreted in several ways, but the more common methods are crossovers, divergences, and rapid rises/falls.  - _[from Investopedia](https://www.investopedia.com/terms/m/macd.asp)_\n",
    "\n",
    "```python\n",
    "df['ewma26'] = pd.ewma(df['price'], span=26)\n",
    "df['ewma12'] = pd.ewma(df['price'], span=12)\n",
    "df['MACD'] = (df['12ema']-df['26ema'])\n",
    "```\n",
    "3. **Exponentially weighted moving average**\n",
    "```python\n",
    "dataset['ema'] = dataset['price'].ewm(com=0.5).mean()\n",
    "```\n",
    "\n",
    "4. **Bollinger bands**\n",
    "    > \"Bollinger Bands¬Æ are a popular technical indicators used by traders in all markets, including stocks, futures and currencies. There are a number of uses for Bollinger Bands¬Æ, including determining overbought and oversold levels, as a trend following tool, and monitoring for breakouts. There are also some pitfalls of the indicators. In this article, we will address all these areas.\"\n",
    "> Bollinger bands are composed of three lines. One of the more common calculations of Bollinger Bands uses a 20-day simple moving average (SMA) for the middle band. The upper band is calculated by taking the middle band and adding twice the daily standard deviation, the lower band is the same but subtracts twice the daily std. - _[from Investopedia](https://www.investopedia.com/trading/using-bollinger-bands-to-gauge-trends/)_\n",
    "\n",
    "    - Boilinger Upper Band:<br>\n",
    "    $BOLU = MA(TP, n) + m * \\sigma[TP, n ]$<br><br>\n",
    "    - Boilinger Lower Band<br>\n",
    "    $ BOLD = MA(TP,n) - m * \\sigma[TP, n ]$\n",
    "    - Where:\n",
    "        - $MA$  = moving average\n",
    "        - $TP$ (typical price) = $(High + Low+Close)/ 3$\n",
    "        - $n$ is number of days in smoothing period\n",
    "        - $m$ is the number of standard deviations\n",
    "        - $\\sigma[TP, n]$ = Standard Deviations over last $n$ periods of $TP$\n",
    "\n",
    "```python\n",
    "# Create Bollinger Bands\n",
    "dataset['20sd'] = pd.stats.moments.rolling_std(dataset['price'],20)\n",
    "dataset['upper_band'] = dataset['ma21'] + (dataset['20sd']*2)\n",
    "dataset['lower_band'] = dataset['ma21'] - (dataset['20sd']*2)\n",
    "```\n",
    "\n",
    "\n",
    "5. **Momentum**\n",
    "> \"Momentum is the rate of acceleration of a security's price or volume ‚Äì that is, the speed at which the price is changing. Simply put, it refers to the rate of change on price movements for a particular asset and is usually defined as a rate. In technical analysis, momentum is considered an oscillator and is used to help identify trend lines.\" - _[from Investopedia](https://www.investopedia.com/articles/technical/081501.asp)_\n",
    "\n",
    "    - $ Momentum = V - V_x$\n",
    "    - Where:\n",
    "        - $ V $ = Latest Price\n",
    "        - $ V_x $ = Closing Price\n",
    "        - $ x $ = number of days ago\n",
    "\n",
    "```python\n",
    "# Create Momentum\n",
    "dataset['momentum'] = dataset['price']-1\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-18T00:33:29.150009Z",
     "start_time": "2021-06-18T00:31:08.601Z"
    }
   },
   "outputs": [],
   "source": [
    "## SPECIFY # OF TRAINING TEST DAYS \n",
    "reload(ji)\n",
    "num_test_days=10\n",
    "num_train_days=260\n",
    "### SPECIFY Number of days included in each X_sequence (each prediction)\n",
    "days_for_x_window=5\n",
    "\n",
    "# Calculate number of rows to bin for x_windows\n",
    "periods_per_day = ji.get_day_window_size_from_freq( stock_df, ji.custom_BH_freq() )\n",
    "\n",
    "\n",
    "## Get the number of rows for x_window \n",
    "x_window = periods_per_day * days_for_x_window#data_params['days_for_x_window'] \n",
    "print(f'X_window size = {x_window} -- ({days_for_x_window} day(s) * {periods_per_day} rows/day)\\n')\n",
    "\n",
    "## Train-test-split by the # of days\n",
    "df_train, df_test = ji.train_test_split_by_last_days(stock_df,\n",
    "                                                     periods_per_day =periods_per_day, \n",
    "                                                     num_test_days   = num_test_days,\n",
    "                                                     num_train_days  = num_train_days,\n",
    "                                                     verbose=1, iplot=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-18T00:33:29.151559Z",
     "start_time": "2021-06-18T00:31:08.616Z"
    }
   },
   "outputs": [],
   "source": [
    "###### RESCALE DATA USING MinMaxScalers FIT ON TRAINING DATA's COLUMNS ######\n",
    "display(df_train.head(2).style.set_caption('df_train - pre-scaling'))\n",
    "\n",
    "scaler_library, df_train = ji.make_scaler_library(df_train, transform=True, verbose=1)\n",
    "\n",
    "df_test = ji.transform_cols_from_library(df_test, col_list=None,\n",
    "                                         scaler_library=scaler_library,\n",
    "                                         inverse=False)\n",
    "display(df_train.head(2).style.set_caption('df_train - post-scaling'))\n",
    "\n",
    "# Show transformed dataset\n",
    "# display( df_train.head(2).round(3).style.set_caption('training data - scaled'))\n",
    "\n",
    "# Create timeseries generators\n",
    "train_generator, test_generator = ji.make_train_test_series_gens( \n",
    "    df_train['price'], df_test['price'], \n",
    "    x_window=x_window,n_features=1,batch_size=1, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-18T00:33:29.153541Z",
     "start_time": "2021-06-18T00:31:08.631Z"
    }
   },
   "outputs": [],
   "source": [
    "## Make new time series generators with all stock_indicators for X_sequences\n",
    "train_generator, test_generator = ji.make_train_test_series_gens(\n",
    "    train_data_series=df_train,\n",
    "    test_data_series=df_test,\n",
    "    y_cols='price',\n",
    "    x_window=x_window,\n",
    "    n_features=len(df_train.columns),\n",
    "    batch_size=1, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-18T00:33:29.155188Z",
     "start_time": "2021-06-18T00:31:08.647Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create keras model from model_params\n",
    "# import functions_combined_BEST as ji\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Bidirectional, Dense, LSTM, Dropout\n",
    "from IPython.display import display\n",
    "from tensorflow.keras.regularizers import l2\n",
    "\n",
    "# Specifying input shape (size of samples, rank of samples?)\n",
    "n_input = x_window #model_params['input_params']['n_input']\n",
    "n_features = len(df_train.columns) # model_params['input_params']['n_features']\n",
    "\n",
    "print(f'input shape: ({n_input},{n_features}')\n",
    "input_shape=(n_input, n_features)\n",
    "\n",
    "# Create model architecture\n",
    "model2 = Sequential()\n",
    "model2.add(LSTM(units=50, input_shape =input_shape,return_sequences=True))#,  kernel_regularizer=l2(0.01),recurrent_regularizer=l2(0.01),\n",
    "# model2.add(Dropout(0.2))\n",
    "model2.add(LSTM(units=50, activation='relu'))\n",
    "model2.add(Dense(1))\n",
    "\n",
    "model2.compile(loss=ji.my_rmse, metrics=['acc',ji.my_rmse],\n",
    "              optimizer=optimizers.Nadam())\n",
    "\n",
    "display(model2.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-18T00:33:29.156853Z",
     "start_time": "2021-06-18T00:31:08.662Z"
    }
   },
   "outputs": [],
   "source": [
    "epochs=5\n",
    "\n",
    "clock = bs.Clock()\n",
    "print('---'*20)\n",
    "print('\\tFITTING MODEL:')\n",
    "print('---'*20,'\\n')     \n",
    "\n",
    "# start the timer\n",
    "clock.tic('')\n",
    "\n",
    "# Fit the model\n",
    "history = model2.fit_generator(train_generator,epochs=epochs) \n",
    "clock.toc('')\n",
    "\n",
    "model_key = \"model_2\"\n",
    "hist_fname = file_dict[model_key]['fig_keras_history.ext']\n",
    "summary_fname = file_dict[model_key]['model_summary']\n",
    "\n",
    "# eval_results = ji.evaluate_model_plot_history(model1, train_generator, test_generator)\n",
    "ji.evaluate_regression_model(model2,history,\n",
    "                             train_generator=train_generator,\n",
    "                             test_generator=test_generator,\n",
    "                            true_test_series=df_test['price'],\n",
    "                            true_train_series =df_train['price'],\n",
    "                             save_history=True,history_filename=hist_fname,\n",
    "                             save_summary=True, summary_filename=summary_fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-18T00:33:29.158451Z",
     "start_time": "2021-06-18T00:31:08.678Z"
    }
   },
   "outputs": [],
   "source": [
    "### PREFER NEW WAY - GET DF_MODEL FIRST THEN GET EVALUATE_REGRESSION INFORMATION?\n",
    "## Get true vs pred data as a dataframe and iplot\n",
    "df_model2 = ji.get_model_preds_df(model2, \n",
    "                                  test_generator=test_generator,\n",
    "                                  true_train_series = df_train['price'],\n",
    "                                  true_test_series = df_test['price'],\n",
    "                                  x_window=x_window,\n",
    "                                  n_features=len(df_train.columns),\n",
    "                                  scaler=scaler_library['price'],\n",
    "                                  preds_from_gen=True, \n",
    "                                  inverse_tf=True,\n",
    "                                  iplot=True)\n",
    "\n",
    "# Compare predictions if predictions timebins shifted\n",
    "df_results2, dfs_results2, df_shifted2 =\\\n",
    "ji.compare_eval_metrics_for_shifts(df_model2['true_test_price'],\n",
    "                                   df_model2['pred_from_gen'],\n",
    "                                   shift_list=np.arange(-4,5,1),\n",
    "                                   true_train_series_to_add=df_model2['true_train_price'],\n",
    "                                   display_results=True,\n",
    "                                   return_styled_df=True,\n",
    "                                   display_U_info=False,\n",
    "                                   return_shifted_df=True,\n",
    "                                   return_results=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-18T00:33:29.160097Z",
     "start_time": "2021-06-18T00:31:08.693Z"
    }
   },
   "outputs": [],
   "source": [
    "##SAVING DFS\n",
    "ji.save_model_dfs(file_dict,'model_2',\n",
    "               df_model=df_model2,\n",
    "              df_results=dfs_results2,\n",
    "              df_shifted=df_shifted2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-18T00:33:29.161497Z",
     "start_time": "2021-06-18T00:31:08.708Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df_results2, dfs_results2, df_shifted2 =\\\n",
    "ji.compare_eval_metrics_for_shifts(df_model2['true_test_price'],\n",
    "                                   df_model2['pred_from_gen'],\n",
    "                                   shift_list=np.arange(-4,5,1),\n",
    "                                   true_train_series_to_add=df_model2['true_train_price'],\n",
    "                                   display_results=True,\n",
    "                                   return_styled_df=True,\n",
    "                                   display_U_info=False,\n",
    "                                   return_shifted_df=True,\n",
    "                                   return_results=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 2: Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COMBINING TWEET STATS, NLP CLASSIFICATION, AND MARKET DATA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Load up stock data in CBH form\n",
    "2. Load up twitter data without NLP\n",
    "3. Create time_interval_bins ...\n",
    "    - from *stock CBH* time index\n",
    "4. Check twitter_df for any tweets from 1_hour prior\n",
    "5. Extract the 'content' column and retweet/fav counts \n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-18T00:33:29.163062Z",
     "start_time": "2021-06-18T00:31:08.738Z"
    }
   },
   "outputs": [],
   "source": [
    "file_dict=ji.def_filename_dictionary(load_prior=False,save_directory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-18T00:33:29.165170Z",
     "start_time": "2021-06-18T00:31:08.753Z"
    }
   },
   "outputs": [],
   "source": [
    "# LOAD IN FULL STOCK DATASET using ClosingBig S&P500 WITH INDEX.FREQ=CBH\n",
    "fname = file_dict['stock_df']['stock_df_with_indicators']\n",
    "full_df = ji.load_processed_stock_data(processed_data_filename=fname)\n",
    "\n",
    "# SELECT DESIRED COLUMNS\n",
    "stock_df = full_df[[\n",
    "    'price','ma7','ma21','26ema','12ema','MACD',\n",
    "    '20sd','upper_band','lower_band','ema','momentum'\n",
    "]]\n",
    "\n",
    "stock_df.head()\n",
    "\n",
    "stock_df['date_time'] = stock_df.index.to_series()\n",
    "ji.index_report(stock_df)\n",
    "\n",
    "stock_df.sort_index(inplace=True)\n",
    "display(stock_df.head(2),stock_df.tail(2))\n",
    "del full_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-18T00:33:29.167067Z",
     "start_time": "2021-06-18T00:31:08.768Z"
    }
   },
   "outputs": [],
   "source": [
    "## LOAD IN RAW TWITTER DATA, NO PROCESSING\n",
    "twitter_df= ji.load_raw_twitter_file(filename='data/trumptwitterarchive_export_iphone_only__08_23_2019.csv',\n",
    "                                     date_as_index=True,\n",
    "                                     rename_map={'text': 'content', 'created_at': 'date'})\n",
    "twitter_df = ji.check_twitter_df(twitter_df,text_col='content',remove_duplicates=True, remove_long_strings=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-18T00:33:29.168702Z",
     "start_time": "2021-06-18T00:31:08.785Z"
    }
   },
   "outputs": [],
   "source": [
    "# MAKE TIME INTERVALS BASED ON BUSINESS HOUR START (09:30-10:30)\n",
    "clock = bs.Clock(verbose=1)\n",
    "clock.tic()\n",
    "\n",
    "time_intervals= \\\n",
    "ji.make_time_index_intervals(stock_df,\n",
    "                             col='date_time', \n",
    "                             closed='right',\n",
    "                             return_interval_dicts=False) \n",
    "clock.lap('time_intervals created.')\n",
    "\n",
    "\n",
    "## USE THE TIME INDEX TO FILTER OUT TWEETS FROM THE HOUR PRIOR\n",
    "twitter_df, bin_codes = ji.bin_df_by_date_intervals(twitter_df ,time_intervals)\n",
    "stock_df, bin_codes_stock = ji.bin_df_by_date_intervals(stock_df, time_intervals, column='date_time')\n",
    "\n",
    "clock.lap('bins added to dataframes')\n",
    "# display(twitter_df.head(2), stock_df.head(2))\n",
    "\n",
    "## COLLAPSE DFs BY CODED BINS\n",
    "twitter_grouped = ji.collapse_df_by_group_index_col(twitter_df,\n",
    "                                                    group_index_col='int_bins',\n",
    "                                                    drop_orig=True,\n",
    "                                                    verbose=0)\n",
    "\n",
    "stocks_grouped = ji.collapse_df_by_group_index_col(stock_df,\n",
    "                                                    drop_orig=True,\n",
    "                                                    group_index_col='int_bins', \n",
    "                                                  verbose=0)\n",
    "\n",
    "clock.toc('collapsed dfs to _grouped')\n",
    "display(twitter_grouped.head(3),stocks_grouped.head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-18T00:33:29.170247Z",
     "start_time": "2021-06-18T00:31:08.800Z"
    }
   },
   "outputs": [],
   "source": [
    "ihelp_menu(ji.merge_stocks_and_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-18T00:33:29.171689Z",
     "start_time": "2021-06-18T00:31:08.815Z"
    }
   },
   "outputs": [],
   "source": [
    "## STOCKS AND TWEETS \n",
    "df_combined = ji.merge_stocks_and_tweets(stocks_grouped, \n",
    "                                      twitter_grouped,\n",
    "                                      on='int_bins',how='left',\n",
    "                                      show_summary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-18T00:33:29.173437Z",
     "start_time": "2021-06-18T00:31:08.829Z"
    }
   },
   "outputs": [],
   "source": [
    "ji.column_report(df_combined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-18T00:33:29.175410Z",
     "start_time": "2021-06-18T00:31:08.844Z"
    }
   },
   "outputs": [],
   "source": [
    "## Check for and address new null values\n",
    "ji.check_null_small(df_combined);\n",
    "cols_to_fill_zeros = ['num_tweets','total_retweet_count','total_favorite_count']\n",
    "for col in cols_to_fill_zeros:\n",
    "    idx_null = ji.find_null_idx(df_combined, column=col)\n",
    "    df_combined.loc[idx_null,col] = 0\n",
    "\n",
    "cols_to_fill_blank_str = ['group_content','source','tweet_times','is_retweet']\n",
    "for col in cols_to_fill_blank_str:\n",
    "    idx_null = ji.find_null_idx(df_combined, column=col)\n",
    "    df_combined.loc[idx_null, col] = \"\"\n",
    "ji.check_null_small(df_combined);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-18T00:33:29.177300Z",
     "start_time": "2021-06-18T00:31:08.858Z"
    }
   },
   "outputs": [],
   "source": [
    "ji.dict_dropdown(file_dict)\n",
    "\n",
    "fname = file_dict['df_combined']['pre_nlp']\n",
    "df_combined.to_csv(fname)\n",
    "print(fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-18T00:33:29.179068Z",
     "start_time": "2021-06-18T00:31:08.873Z"
    }
   },
   "outputs": [],
   "source": [
    "## Add nlp\n",
    "df_nlp = ji.full_twitter_df_processing(df_combined,'group_content',force=True)\n",
    "ji.column_report(df_nlp, as_qgrid=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-18T00:33:29.181197Z",
     "start_time": "2021-06-18T00:31:08.887Z"
    }
   },
   "outputs": [],
   "source": [
    "df_nlp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-18T00:33:29.182966Z",
     "start_time": "2021-06-18T00:31:08.904Z"
    }
   },
   "outputs": [],
   "source": [
    "## Use case ratio null values as index to replace values\n",
    "idx_null= ji.check_null_small(df_nlp,null_index_column='case_ratio')\n",
    "df_nlp.loc[idx_null,'case_ratio'] = 0.0\n",
    "ji.check_null_small(df_nlp)\n",
    "\n",
    "## replace sentiment_class, set =-1\n",
    "cols_to_replace_misleading_values = ['sentiment_class']\n",
    "for col in cols_to_replace_misleading_values:\n",
    "    df_nlp.loc[idx_null,col] = -1\n",
    "\n",
    "## remap sentiment class\n",
    "sent_class_mapper = {'neg':0,\n",
    "                     -1:1,\n",
    "                    'pos':2}\n",
    "df_nlp['sentiment_class'] = df_nlp['sentiment_class'].apply(lambda x: sent_class_mapper[x])\n",
    "\n",
    "bool_cols_to_ints = ['has_tweets']\n",
    "for col in bool_cols_to_ints:\n",
    "    df_nlp[col] = df_nlp[col].apply(lambda x: 1 if x==True else 0)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-18T00:33:29.184760Z",
     "start_time": "2021-06-18T00:31:08.919Z"
    }
   },
   "outputs": [],
   "source": [
    "df_nlp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-18T00:33:29.186243Z",
     "start_time": "2021-06-18T00:31:08.933Z"
    }
   },
   "outputs": [],
   "source": [
    "ji.display_same_tweet_diff_cols(df_nlp.groupby('has_tweets').get_group(True),\n",
    "                                columns=['group_content','content_min_clean','cleaned_stopped_lemmas'],as_md=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-18T00:33:29.188018Z",
     "start_time": "2021-06-18T00:31:08.948Z"
    }
   },
   "outputs": [],
   "source": [
    "ji.check_twitter_df(df_nlp,char_limit=61*350)\n",
    "# get_floats = df_nlp['content_min_clean'].apply(lambda x: isinstance(x,float))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-18T00:33:29.189628Z",
     "start_time": "2021-06-18T00:31:08.962Z"
    }
   },
   "outputs": [],
   "source": [
    "fname =file_dict['df_combined']['post_nlp']\n",
    "df_nlp.to_csv(fname)\n",
    "print(f'saved to {fname}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading in NLP Model for Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-18T00:33:29.191153Z",
     "start_time": "2021-06-18T00:31:08.979Z"
    }
   },
   "outputs": [],
   "source": [
    "ji.dict_dropdown(file_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-18T00:33:29.192940Z",
     "start_time": "2021-06-18T00:31:08.993Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_most_recent_filenames(full_filename,str_to_find=None):\n",
    "    import os\n",
    "    import time\n",
    "    fparts = full_filename.split('/')\n",
    "    folder = '/'.join(fparts[0:-1])\n",
    "    name = fparts[-1]\n",
    "    \n",
    "    filelist = os.listdir(folder)\n",
    "\n",
    "    mtimes = [['file','date modified']]\n",
    "    for file in filelist:\n",
    "        if str_to_find is None:\n",
    "            mtimes.append([file, time.ctime(os.path.getmtime(folder+'/'+file))])\n",
    "        elif str_to_find in file:\n",
    "            mtimes.append([file, time.ctime(os.path.getmtime(folder+'/'+file))])\n",
    "    res = bs.list2df(mtimes)\n",
    "    res['date modified'] = pd.to_datetime(res['date modified'])\n",
    "    res.set_index('date modified',inplace=True)\n",
    "    res.sort_index(ascending=False, inplace=True)\n",
    "    \n",
    "    most_recent = res.iloc[0]\n",
    "    import re\n",
    "    re.compile(r'()')\n",
    "    \n",
    "    return    res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-18T00:33:29.194585Z",
     "start_time": "2021-06-18T00:31:09.007Z"
    }
   },
   "outputs": [],
   "source": [
    "res = get_most_recent_filenames(file_dict['model_0A']['base_filename'])\n",
    "res.iloc[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-18T00:33:29.196233Z",
     "start_time": "2021-06-18T00:31:09.021Z"
    }
   },
   "outputs": [],
   "source": [
    "reload(ji)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-18T00:33:29.197835Z",
     "start_time": "2021-06-18T00:31:09.037Z"
    }
   },
   "outputs": [],
   "source": [
    "## Load the nlp model and weights with layers set trainable=False\n",
    "base_fname = file_dict['nlp_model_for_predictions']['base_filename']\n",
    "nlp_model,df_model_layers =  ji.load_model_weights_params(base_filename= base_fname,#'models/NLP/nlp_model0B__09-02-2019_0121pm',\n",
    "                                        load_model_params=False,\n",
    "                                        load_model_layers_excel=True,\n",
    "                                        trainable=False)\n",
    "## Load in Word2Vec model from earlier\n",
    "w2v_model = io.load_word2vec(file_dict=file_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Predictions for Hour-Binned Tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-18T00:33:29.199390Z",
     "start_time": "2021-06-18T00:31:09.053Z"
    }
   },
   "outputs": [],
   "source": [
    "ihelp_menu([ji.get_tokenizer_and_text_sequences,\n",
    "           ji.replace_embedding_layer])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-18T00:33:29.201033Z",
     "start_time": "2021-06-18T00:31:09.068Z"
    }
   },
   "outputs": [],
   "source": [
    "ji.column_report(df_nlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-18T00:33:29.202946Z",
     "start_time": "2021-06-18T00:31:09.085Z"
    }
   },
   "outputs": [],
   "source": [
    "## GET X_SEQUENES FOR BINNED TWEETS AND CREATE NEW EMBEDDING LAYER FOR THEIR SIZE\n",
    "reload(ji)\n",
    "text_data=df_nlp['cleaned_stopped_lemmas']\n",
    "tokenizer, X_sequences = ji.get_tokenizer_and_text_sequences(w2v_model,text_data)\n",
    "\n",
    "new_nlp_model = ji.replace_embedding_layer(nlp_model,w2v_model,text_data,verbose=2)\n",
    "new_nlp_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-18T00:33:29.204632Z",
     "start_time": "2021-06-18T00:31:09.100Z"
    }
   },
   "outputs": [],
   "source": [
    "## GET PREDICTIONS FROM NEW MODEL\n",
    "preds = new_nlp_model.predict_classes(X_sequences)\n",
    "print(type(preds), preds.shape)\n",
    "ji.check_y_class_balance(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-18T00:33:29.206880Z",
     "start_time": "2021-06-18T00:31:09.116Z"
    }
   },
   "outputs": [],
   "source": [
    "## add to df\n",
    "df_nlp['pred_classes_int'] = preds\n",
    "mapper= {0:'neg',\n",
    "        1:'no_change',\n",
    "        2:'pos'}\n",
    "df_nlp['pred_classes'] = df_nlp['pred_classes_int'].apply(lambda x: mapper[x])\n",
    "display(df_nlp.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-18T00:33:29.208590Z",
     "start_time": "2021-06-18T00:31:09.131Z"
    }
   },
   "outputs": [],
   "source": [
    "ji.dict_dropdown(file_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-18T00:33:29.210123Z",
     "start_time": "2021-06-18T00:31:09.146Z"
    }
   },
   "outputs": [],
   "source": [
    "# fname = file_dict['df_combined']['with_preds']\n",
    "\n",
    "# df_nlp.to_csv(fname)\n",
    "# print(fname)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 3: Stock Price + Indicators + NLP Preds & Tweet Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-18T00:33:29.211830Z",
     "start_time": "2021-06-18T00:31:09.162Z"
    }
   },
   "outputs": [],
   "source": [
    "## IMPORT CUSTOM CAPSTONE FUNCTIONS\n",
    "import functions_combined_BEST as ji\n",
    "import functions_io as io\n",
    "\n",
    "from functions_combined_BEST import ihelp, ihelp_menu,\\\n",
    "reload, inspect_variables\n",
    "\n",
    "## IMPORT MY PUBLISHED PYPI PACKAGE \n",
    "import bs_ds as  bs\n",
    "from bs_ds.imports import *\n",
    "\n",
    "## IMPORT CONVENIENCE FUNCTIONS\n",
    "from pprint import pprint\n",
    "import qgrid\n",
    "import json\n",
    "\n",
    "# Import plotly and cufflinks for iplots\n",
    "import plotly\n",
    "import cufflinks as cf\n",
    "from plotly import graph_objs as go\n",
    "from plotly.offline import iplot\n",
    "cf.go_offline()\n",
    "\n",
    "# Suppress warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "#Set pd.set_options for tweet visibility\n",
    "pd.set_option('display.max_colwidth',100)\n",
    "pd.set_option('display.max_columns',50)\n",
    "\n",
    "file_dict = io.def_filename_dictionary(load_prior=False, save_directory=True)\n",
    "# file_dict = ji.load_filename_directory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-18T00:33:29.213399Z",
     "start_time": "2021-06-18T00:31:09.177Z"
    }
   },
   "outputs": [],
   "source": [
    "df_combined = pd.read_csv('data/__combined_stock_data_with_tweet_preds.csv', index_col=0,parse_dates=True)\n",
    "df_combined.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finalize colums for final model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-18T00:33:29.215340Z",
     "start_time": "2021-06-18T00:31:09.194Z"
    }
   },
   "outputs": [],
   "source": [
    "model_col_list = ['price', 'ma7', 'ma21', '26ema', '12ema', 'MACD', '20sd', 'upper_band','lower_band', 'ema', 'momentum',\n",
    "                  'has_tweets','num_tweets','case_ratio', 'compound_score','pos','neu','neg','sentiment_class',\n",
    "                  'pred_classes','pred_classes_int','total_favorite_count','total_retweet_count']\n",
    "\n",
    "df_combined = ji.set_timeindex_freq(df_combined,fill_nulls=False)\n",
    "\n",
    "df_to_model = df_combined[model_col_list].copy()#df_nlp[model_col_list].copy()\n",
    "# df_to_model.to_csv('data/_df_to_model_final_model.csv')\n",
    "df_to_model.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-18T00:33:29.216772Z",
     "start_time": "2021-06-18T00:31:09.211Z"
    }
   },
   "outputs": [],
   "source": [
    "# del_me= ['X_sequences','df_nlp','twitter_grouped','bin_codes_stock','bin_codes']#list of variable names\n",
    "# for me in del_me:    \n",
    "#     try: \n",
    "#         exec(f'del {me}')\n",
    "#         print(f'del {me} succeeded')\n",
    "#     except:\n",
    "#         print(f'del {me} succeeded')\n",
    "#         continue\n",
    "# ji.inspect_variables(locals())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-18T00:33:29.218484Z",
     "start_time": "2021-06-18T00:31:09.227Z"
    }
   },
   "outputs": [],
   "source": [
    "## SPECIFY # OF TRAINING TEST DAYS \n",
    "reload(ji)\n",
    "num_test_days=5\n",
    "num_train_days=260\n",
    "### SPECIFY Number of days included in each X_sequence (each prediction)\n",
    "days_for_x_window=1\n",
    "\n",
    "cols_to_exclude = ['pred_classes','has_tweets']\n",
    "# Calculate number of rows to bin for x_windows\n",
    "periods_per_day = ji.get_day_window_size_from_freq(df_to_model.drop(cols_to_exclude,axis=1), ji.custom_BH_freq() )\n",
    "\n",
    "\n",
    "## Get the number of rows for x_window \n",
    "x_window = periods_per_day * days_for_x_window#data_params['days_for_x_window'] \n",
    "print(f'X_window size = {x_window} -- ({days_for_x_window} day(s) * {periods_per_day} rows/day)\\n')\n",
    "\n",
    "## Train-test-split by the # of days\n",
    "df_train, df_test = ji.train_test_split_by_last_days(df_to_model.drop(cols_to_exclude,axis=1),\n",
    "                                                     periods_per_day =periods_per_day, \n",
    "                                                     num_test_days   = num_test_days,\n",
    "                                                     num_train_days  = num_train_days,\n",
    "                                                     verbose=1, iplot=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-18T00:33:29.220292Z",
     "start_time": "2021-06-18T00:31:09.244Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "###### RESCALE DATA USING MinMaxScalers FIT ON TRAINING DATA's COLUMNS ######\n",
    "display(df_train.head(2).style.set_caption('df_train - pre-scaling'))\n",
    "\n",
    "scaler_library, df_train = ji.make_scaler_library(df_train, transform=True, verbose=1)\n",
    "\n",
    "df_test = ji.transform_cols_from_library(df_test, col_list=None,\n",
    "                                         scaler_library=scaler_library,\n",
    "                                         inverse=False)\n",
    "display(df_train.head(2).style.set_caption('df_train - post-scaling'))\n",
    "\n",
    "# Show transformed dataset\n",
    "# display( df_train.head(2).round(3).style.set_caption('training data - scaled'))\n",
    "\n",
    "# Create timeseries generators\n",
    "train_generator, test_generator = ji.make_train_test_series_gens(\n",
    "    train_data_series=df_train,\n",
    "    test_data_series=df_test,\n",
    "    y_cols='price',\n",
    "    x_window=x_window,\n",
    "    n_features=len(df_train.columns),\n",
    "    batch_size=1, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-18T00:33:29.221845Z",
     "start_time": "2021-06-18T00:31:09.261Z"
    }
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras.layers import Bidirectional, Dense, LSTM, Dropout\n",
    "from IPython.display import display\n",
    "from tensorflow.keras.regularizers import l2\n",
    "\n",
    "# Specifying input shape (size of samples, rank of samples?)\n",
    "n_input =x_window\n",
    "n_features = len(df_train.columns)\n",
    "print(f'input shape: ({n_input},{n_features})')\n",
    "input_shape=(n_input, n_features)\n",
    "\n",
    "# Create model architecture\n",
    "model3 = Sequential()\n",
    "model3.add(LSTM(units=100, input_shape =input_shape,return_sequences=True,dropout=0.3,recurrent_dropout=0.3))#,  kernel_regularizer=l2(0.01),recurrent_regularizer=l2(0.01),\n",
    "model3.add(LSTM(units=100, activation='relu', return_sequences=False,dropout=0.3,recurrent_dropout=0.3))\n",
    "#     model.add(Dense(units=10, activation='relu'))\n",
    "model3.add(Dense(1))#,activation='relu'))\n",
    "\n",
    "\n",
    "model3.compile(loss=ji.my_rmse, metrics=['acc'],optimizer=optimizers.Nadam())\n",
    "    \n",
    "model3.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-18T00:33:29.223488Z",
     "start_time": "2021-06-18T00:31:09.277Z"
    }
   },
   "outputs": [],
   "source": [
    "## FIT MODEL\n",
    "dashes = '---'*20\n",
    "print(f\"{dashes}\\n\\tFITTING MODEL:\\n{dashes}\")\n",
    "\n",
    "## set params\n",
    "epochs=5\n",
    "\n",
    "# override keras warnings\n",
    "ji.quiet_mode(True,True,True)\n",
    "\n",
    "# Instantiating clock timer\n",
    "clock = bs.Clock()\n",
    "clock.tic('')\n",
    "\n",
    "# Fit the model\n",
    "history = model3.fit_generator(train_generator,\n",
    "                               epochs=epochs,\n",
    "                               verbose=2, \n",
    "                               use_multiprocessing=True,\n",
    "                               workers=3)\n",
    "clock.toc('')\n",
    "\n",
    "model_key = \"model_3\"\n",
    "hist_fname = file_dict[model_key]['fig_keras_history.ext']\n",
    "summary_fname = file_dict[model_key]['model_summary']\n",
    "\n",
    "# eval_results = ji.evaluate_model_plot_history(model1, train_generator, test_generator)\n",
    "ji.evaluate_regression_model(model3,history,\n",
    "                             train_generator=train_generator,\n",
    "                             test_generator=test_generator,\n",
    "                            true_test_series=df_test['price'],\n",
    "                            true_train_series =df_train['price'],\n",
    "                             save_history=True,history_filename=hist_fname,\n",
    "                             save_summary=True, summary_filename=summary_fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-18T00:33:29.225042Z",
     "start_time": "2021-06-18T00:31:09.293Z"
    }
   },
   "outputs": [],
   "source": [
    "### PREFER NEW WAY - GET DF_MODEL FIRST THEN GET EVALUATE_REGRESSION INFORMATION?\n",
    "## Get true vs pred data as a dataframe and iplot\n",
    "df_model3 = ji.get_model_preds_df(model3, \n",
    "                                  test_generator = test_generator,\n",
    "                                  true_train_series = df_train['price'],\n",
    "                                  true_test_series = df_test['price'],\n",
    "                                  include_train_data=True,\n",
    "                                  inverse_tf = True, \n",
    "                                  scaler = scaler_library['price'],\n",
    "                                  preds_from_gen = True, \n",
    "                                  iplot = False,\n",
    "                                  verbose=1)\n",
    "#                                   subplot_mode='lines+markers')\n",
    "ji.plotly_true_vs_preds_subplots(df_model3)\n",
    "    \n",
    "# Get evaluation metrics\n",
    "df_results3, dfs_results3, df_shifted3 =\\\n",
    "ji.compare_eval_metrics_for_shifts(df_model3['true_test_price'],\n",
    "                                   df_model3['pred_from_gen'],\n",
    "                                   shift_list=np.arange(-4,4,1),\n",
    "                                   true_train_series_to_add=df_model3['true_train_price'],\n",
    "                                   display_results=True,\n",
    "                                   display_U_info=True,\n",
    "                                   return_results=True,\n",
    "                                   return_styled_df=True,\n",
    "                                   return_shifted_df=True)\n",
    "\n",
    "\n",
    "save_model=True\n",
    "ji.save_model_dfs(file_dict, 'model_3',df_model3,dfs_results3,df_shifted3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-18T00:33:29.226581Z",
     "start_time": "2021-06-18T00:31:09.311Z"
    }
   },
   "outputs": [],
   "source": [
    "reload(ji)\n",
    "filename_prefix = file_dict['model_3']['base_filename']\n",
    "if save_model ==True:\n",
    "    model_3_output_files = bs.save_model_weights_params(model3,\n",
    "                                 filename_prefix=filename_prefix,\n",
    "                                 auto_increment_name=True,\n",
    "                                 auto_filename_suffix=True, \n",
    "                                 suffix_time_format='%m-%d-%y_%I%M%p',\n",
    "                                 save_model_layer_config_xlsx=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 3 Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model X: XGB Regression + Feature Importance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-18T00:33:29.228154Z",
     "start_time": "2021-06-18T00:31:09.341Z"
    }
   },
   "outputs": [],
   "source": [
    "## SPECIFY # OF TRAINING TEST DAYS \n",
    "reload(ji)\n",
    "num_test_days=20\n",
    "num_train_days=2*52*5\n",
    "### SPECIFY Number of days included in each X_sequence (each prediction)\n",
    "days_for_x_window=1\n",
    "\n",
    "cols_to_exclude = ['pred_classes','has_tweets']\n",
    "# Calculate number of rows to bin for x_windows\n",
    "periods_per_day = ji.get_day_window_size_from_freq(df_to_model.drop(cols_to_exclude,axis=1), ji.custom_BH_freq() )\n",
    "\n",
    "\n",
    "## Get the number of rows for x_window \n",
    "x_window = periods_per_day * days_for_x_window#data_params['days_for_x_window'] \n",
    "print(f'X_window size = {x_window} -- ({days_for_x_window} day(s) * {periods_per_day} rows/day)\\n')\n",
    "\n",
    "## Train-test-split by the # of days\n",
    "df_train, df_test = ji.train_test_split_by_last_days(df_to_model.drop(cols_to_exclude,axis=1),\n",
    "                                                     periods_per_day =periods_per_day, \n",
    "                                                     num_test_days   = num_test_days,\n",
    "                                                     num_train_days  = num_train_days,\n",
    "                                                     verbose=1, iplot=True)\n",
    "\n",
    "###### RESCALE DATA USING MinMaxScalers FIT ON TRAINING DATA's COLUMNS ######\n",
    "display(df_train.head(2).style.set_caption('df_train - pre-scaling'))\n",
    "\n",
    "scaler_library, df_train = ji.make_scaler_library(df_train, transform=True, verbose=1)\n",
    "\n",
    "df_test = ji.transform_cols_from_library(df_test, col_list=None,\n",
    "                                         scaler_library=scaler_library,\n",
    "                                         inverse=False)\n",
    "display(df_train.head(2).style.set_caption('df_train - post-scaling'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-18T00:33:29.229744Z",
     "start_time": "2021-06-18T00:31:09.356Z"
    }
   },
   "outputs": [],
   "source": [
    "## Shift price values such that the y-value being predicted is the following hour's Closing Price\n",
    "df_train['price_shifted'] = df_train['price'].shift(-1)\n",
    "df_test['price_shifted'] = df_test['price'].shift(-1)\n",
    "\n",
    "display(df_train[['price','price_shifted','momentum','ema','num_tweets',]].head(10))\n",
    "\n",
    "# Drop the couple of null values created by the shift\n",
    "df_train.dropna(subset=['price_shifted'], inplace=True)\n",
    "df_test.dropna(subset=['price_shifted'], inplace=True)\n",
    "\n",
    "## Drop columns and make train-test-X and y\n",
    "target_col = 'price_shifted'\n",
    "drop_cols = ['price_shifted','price']\n",
    "\n",
    "X_train = df_train.drop(drop_cols,axis=1)\n",
    "y_train = df_train[target_col]\n",
    "X_test = df_test.drop(drop_cols,axis=1)\n",
    "y_test = df_test[target_col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-18T00:33:29.231073Z",
     "start_time": "2021-06-18T00:31:09.372Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "from xgboost import plot_importance, plot_tree\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "\n",
    "reg = xgb.XGBRegressor(n_estimators=1000,silent=False,max_depth=4)\n",
    "\n",
    "reg.fit(X_train, y_train,\n",
    "        eval_set=[(X_train, y_train), (X_test, y_test)],\n",
    "        early_stopping_rounds=50,\n",
    "       verbose=False)\n",
    "\n",
    "\n",
    "## Get Predictions\n",
    "pred_price = reg.predict(X_test)\n",
    "pred_price_series = pd.Series(pred_price,index=df_test.index,name='pred_test_price')#.plot()\n",
    "df_xgb = pd.concat([df_train['price'].rename('true_train_price'), pred_price_series,df_test['price'].rename('true_test_price')],axis=1)\n",
    "\n",
    "\n",
    "df_results = ji.evaluate_regression(df_test['price'], pred_price_series,show_results=True);\n",
    "\n",
    "\n",
    "fig = ji.plotly_true_vs_preds_subplots(df_xgb,true_train_col='true_train_price',\n",
    "                                true_test_col='true_test_price',\n",
    "                                pred_test_columns='pred_test_price')\n",
    "\n",
    "\n",
    "## PLOT FEATURE IMPORTANCE\n",
    "feature_importance={}\n",
    "for import_type in ['weight','gain','cover']:\n",
    "    reg.importance_type = import_type\n",
    "    cur_importances = reg.feature_importances_\n",
    "    feature_importance[import_type] = pd.Series(data = cur_importances,\n",
    "                                               index=df_train.drop(drop_cols,axis=1).columns,\n",
    "                                               name=import_type)\n",
    "\n",
    "df_importance = pd.DataFrame(feature_importance)\n",
    "    \n",
    "importance_fig = df_importance.sort_values(by='weight', ascending=True).iplot(kind='barh',theme='solar',\n",
    "                                                                    title='Feature Importance',\n",
    "                                                                    xTitle='Relative Importance<br>(sum=1.0)',\n",
    "                                                                    asFigure=True)\n",
    "\n",
    "iplot(importance_fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-18T00:33:29.232474Z",
     "start_time": "2021-06-18T00:31:09.387Z"
    }
   },
   "outputs": [],
   "source": [
    "# from plotly.offline import plot,iplot\n",
    "# html_fig = plot(importance_fig,output_type='div')\n",
    "\n",
    "# with open ('html_importance_fig.html','w') as f:\n",
    "#     f.write(html_fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-18T00:33:29.233702Z",
     "start_time": "2021-06-18T00:31:09.402Z"
    }
   },
   "outputs": [],
   "source": [
    "# Compare predictions if predictions timebins shifted\n",
    "df_resultsX, dfs_resultsX, df_shiftedX =\\\n",
    "ji.compare_eval_metrics_for_shifts(df_xgb['true_test_price'],\n",
    "                                   df_xgb['pred_test_price'],\n",
    "                                   shift_list=np.arange(-4,5,1),\n",
    "                                   true_train_series_to_add=df_xgb['true_train_price'],\n",
    "                                   display_results=True,\n",
    "                                   return_styled_df=True,\n",
    "                                   display_U_info=False,\n",
    "                                   return_shifted_df=True,\n",
    "                                   return_results=True)\n",
    "df_importance.to_csv('results/modelxgb/df_importance.csv')\n",
    "\n",
    "ji.save_model_dfs(file_dict, 'model_xgb',df_xgb,dfs_resultsX,df_shiftedX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-18T00:33:29.234948Z",
     "start_time": "2021-06-18T00:31:09.418Z"
    }
   },
   "outputs": [],
   "source": [
    "tree_vis = xgb.to_graphviz(reg)#,**{'format':'svg'})\n",
    "\n",
    "tree_vis.render(\"xgb_full_model_\",format=\"pdf\",)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Interpretation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-18T00:33:29.236458Z",
     "start_time": "2021-06-18T00:31:09.434Z"
    }
   },
   "outputs": [],
   "source": [
    "import shap\n",
    "shap.initjs()\n",
    "explainer = shap.TreeExplainer(reg)\n",
    "shap_values = explainer.shap_values(X_train)\n",
    "shap_interaction_values = explainer.shap_interaction_values(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-18T00:33:29.238103Z",
     "start_time": "2021-06-18T00:31:09.449Z"
    }
   },
   "outputs": [],
   "source": [
    "shap.summary_plot(shap_interaction_values,X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-18T00:33:29.239887Z",
     "start_time": "2021-06-18T00:31:09.465Z"
    }
   },
   "outputs": [],
   "source": [
    "shap.summary_plot(shap_values, features=X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model X Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-18T00:33:29.241442Z",
     "start_time": "2021-06-18T00:31:09.482Z"
    }
   },
   "outputs": [],
   "source": [
    "# importance_fig = df_importance.sort_values(by='weight', ascending=True).iplot(kind='barh',theme='solar',\n",
    "#                                                                     title='Feature Importance',\n",
    "#                                                                     xTitle='Relative Importance<br>(sum=1.0)',\n",
    "#                                                                     asFigure=True)\n",
    "\n",
    "# iplot(importance_fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-18T00:33:29.242904Z",
     "start_time": "2021-06-18T00:31:09.510Z"
    }
   },
   "outputs": [],
   "source": [
    "dfs_list = {'Model 1':dfs_results1,\n",
    "            'Model 2':dfs_results2,\n",
    "            'Model 3':dfs_results3,\n",
    "            'XGB Regressor':dfs_resultsX}\n",
    "for k,v in dfs_list.items():\n",
    "    new_cap = f'Evaluation Metrics for {k}'\n",
    "    display(v.set_caption(new_cap))\n",
    "#     [display(x.set_cat) for x in dfs_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "xNf73gC9daS-",
    "eE3D-Avztybj"
   ],
   "name": "Capstone Project Outline + Analysis.ipynb",
   "provenance": [],
   "toc_visible": true,
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "learn-env-new",
   "language": "python",
   "name": "learn-env-new"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "226.5px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": "25"
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "oldHeight": 265.340454,
   "position": {
    "height": "40px",
    "left": "751.591px",
    "right": "20px",
    "top": "49px",
    "width": "607.773px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "varInspector_section_display": "none",
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
