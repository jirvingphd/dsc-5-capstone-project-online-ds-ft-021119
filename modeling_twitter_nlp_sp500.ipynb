{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import bs_ds as bs\n",
    "bs.big_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "#################### TIMEINDEX FUNCTIONS #####################\n",
    "def get_day_window_size_from_freq(dataset):#, freq='CBH'):\n",
    "    \n",
    "    if dataset.index.freq == custom_BH_freq():\n",
    "        return 7\n",
    "    \n",
    "    if dataset.index.freq=='T':\n",
    "        day_window_size = 60*24\n",
    "    elif dataset.index.freq=='BH':\n",
    "        day_window_size = 8\n",
    "#     elif dataset.index.freq=='CBH':\n",
    "#         day_window_size = 7\n",
    "    elif dataset.index.freq=='B':\n",
    "        day_window_size=1\n",
    "    elif dataset.index.freq=='D':\n",
    "        day_window_size=1\n",
    "        \n",
    "    else:\n",
    "        raise Exception('dataset freq=None')\n",
    "        \n",
    "    return day_window_size\n",
    "    \n",
    "\n",
    "def custom_BH_freq():\n",
    "    import pandas as pd\n",
    "    CBH = pd.tseries.offsets.CustomBusinessHour(start='09:30',end='16:30')\n",
    "    return CBH\n",
    "    \n",
    "    \n",
    "def  set_timeindex_freq(ive_df, col_to_fill=None, freq='CBH',fill_method='ffill',\n",
    "                        verbose=3): #set_tz=True,\n",
    "    \n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    \n",
    "    \n",
    "    if verbose>1:\n",
    "        # print(f\"{'Index When:':>{10}}\\t{'Freq:':>{20}}\\t{'Index Start:':>{40}}\\t{'Index End:':>{40}}\")\n",
    "        print(f\"{'Index When:'}\\t{'Freq:'}\\t{'Index Start'}\\t\\t{'Index End:'}\")\n",
    "        print(f\"Pre-Change\\t{ive_df.index.freq}\\t{ive_df.index[0]}\\t{ive_df.index[-1]}\")\n",
    "        \n",
    "    \n",
    "    if freq=='CBH':\n",
    "        freq=custom_BH_freq()\n",
    "#         start_idx = \n",
    "        \n",
    "    # Change frequency to freq\n",
    "    ive_df = ive_df.asfreq(freq,)#'min')\n",
    "    \n",
    "    #     # Set timezone\n",
    "    #     if set_tz==True:\n",
    "    #         ive_df.tz_localize()\n",
    "    #         ive_df.index = ive_df.index.tz_convert('America/New_York')\n",
    "    \n",
    "    # Report Success / Details\n",
    "    if verbose>1:\n",
    "        print(f\"Post-Change\\t{ive_df.index.freq}\\t{ive_df.index[0]}\\t{ive_df.index[-1]}\")\n",
    "\n",
    "\n",
    "    ## FILL AND TRACK TIMEPOINTS WITH MISSING DATA    \n",
    "    \n",
    "    # Helper Function for adding column to track the datapoints that were filled\n",
    "    def check_null_times(x):\n",
    "        import numpy as np\n",
    "        if np.isnan(x):\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "\n",
    "    ## CREATE A COLUMN TO TRACK ROWS TO BE FILLED\n",
    "    # If col_to_fill provided, use that column to create/judge ive_df['filled_timebin'] \n",
    "    if col_to_fill!=None:\n",
    "        ive_df['filled_timebin'] = ive_df[col_to_fill].apply(lambda x: check_null_times(x))#True if ive_df.isna().any()\n",
    "        \n",
    "    # if not provided, use all columns and sum results\n",
    "    elif col_to_fill == None:\n",
    "        # Prefill fol with 0's\n",
    "        ive_df['filled_timebin']=0\n",
    "        \n",
    "        # loop through all columns and add results of check_null_times from each loop\n",
    "    for col in ive_df.columns:\n",
    "        if ive_df[col].dtypes=='float64':\n",
    "            #ive_df['filled_timebin'] = ive_df[target_col].apply(lambda x: check_null_times(x))#True if ive_df.isna().any()\n",
    "            curr_filled_timebin_col = ive_df[col].apply(lambda x: check_null_times(x))#True if ive_df.isna().any() \n",
    "\n",
    "            # add results\n",
    "            ive_df['filled_timebin'] +=  curr_filled_timebin_col\n",
    "            \n",
    "    ive_df['filled_timebin'] = ive_df['filled_timebin'] >0\n",
    "            \n",
    "    ## FILL IN NULL VALUES\n",
    "    ive_df.fillna(method=fill_method, inplace=True)\n",
    "\n",
    "    # Report # filled\n",
    "    if verbose>0:\n",
    "        check_fill = ive_df.loc[ive_df['filled_timebin']>0]\n",
    "        print(f'\\nFilled {len(check_fill==True)}# of rows using method {fill_method}')\n",
    "    \n",
    "    # Report any remaning null values\n",
    "    if verbose>0:\n",
    "        res = ive_df.isna().sum()\n",
    "        if res.any():\n",
    "            print(f'Cols with Nulls:')\n",
    "            print(res[res>0])\n",
    "        else:\n",
    "            print('No Remaining Null Values')   \n",
    "            \n",
    "    # display header\n",
    "    if verbose>2:\n",
    "        display(ive_df.head())\n",
    "    \n",
    "    return ive_df\n",
    "\n",
    "\n",
    "# Helper Function for adding column to track the datapoints that were filled\n",
    "def check_null_times(x):\n",
    "    import numpy as np\n",
    "    if np.isnan(x):\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "\n",
    "############### TIMESERIES TESTS AND VISUALS ###############\n",
    "\n",
    "def plot_time_series(stocks_df, freq=None, fill_method='ffill',figsize=(12,4)):\n",
    "    \n",
    "    df = stocks_df.copy()\n",
    "    df.fillna(method=fill_method, inplace=True)\n",
    "    df.dropna(inplace=True)\n",
    "    \n",
    "    if (df.index.freq==None) & (freq == None):\n",
    "        xlabels=f'Time'\n",
    "    \n",
    "    elif (df.index.freq==None) & (freq != None):\n",
    "        df = df.asfreq(freq)\n",
    "        df.fillna(method=fill_method, inplace=True)\n",
    "        df.dropna(inplace=True)\n",
    "        xlabels=f'Time - Frequency = {freq}'\n",
    "\n",
    "    else:\n",
    "        xlabels=f'Time - Frequency = {df.index.freq}'\n",
    "        \n",
    "    ylabels=\"Price\"\n",
    "\n",
    "    raw_plot = df.plot(figsize=figsize)\n",
    "    raw_plot.set_title('Stock Bid Closing Price ')\n",
    "    raw_plot.set_ylabel(ylabels)\n",
    "    raw_plot.set_xlabel(xlabels)\n",
    "    \n",
    "    \n",
    "#################### TIMEINDEX FUNCTIONS #####################\n",
    "def get_day_window_size_from_freq(dataset):#, freq='CBH'):\n",
    "    \n",
    "    if dataset.index.freq == custom_BH_freq():\n",
    "        return 7\n",
    "    \n",
    "    if dataset.index.freq=='T':\n",
    "        day_window_size = 1440\n",
    "    elif dataset.index.freq=='BH':\n",
    "        day_window_size = 8\n",
    "    elif dataset.index.freq=='CBH':\n",
    "        day_window_size = 7\n",
    "    elif dataset.index.freq=='B':\n",
    "        day_window_size=1\n",
    "    elif dataset.index.freq=='D':\n",
    "        day_window_size=1\n",
    "        \n",
    "    else:\n",
    "        raise Exception('dataset freq=None')\n",
    "        \n",
    "    return day_window_size\n",
    "    \n",
    "\n",
    "def custom_BH_freq():\n",
    "    import pandas as pd\n",
    "    CBH = pd.tseries.offsets.CustomBusinessHour(start='09:30',end='16:30')\n",
    "    return CBH\n",
    "    \n",
    "    \n",
    "def  set_timeindex_freq(ive_df, col_to_fill=None, freq='CBH',fill_method='ffill',\n",
    "                        verbose=3): #set_tz=True,\n",
    "    \n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    \n",
    "    \n",
    "    if verbose>1:\n",
    "        # print(f\"{'Index When:':>{10}}\\t{'Freq:':>{20}}\\t{'Index Start:':>{40}}\\t{'Index End:':>{40}}\")\n",
    "        print(f\"{'Index When:'}\\t{'Freq:'}\\t{'Index Start'}\\t\\t{'Index End:'}\")\n",
    "        print(f\"Pre-Change\\t{ive_df.index.freq}\\t{ive_df.index[0]}\\t{ive_df.index[-1]}\")\n",
    "        \n",
    "    \n",
    "    if freq=='CBH':\n",
    "        freq=custom_BH_freq()\n",
    "#         start_idx = \n",
    "        \n",
    "    # Change frequency to freq\n",
    "    ive_df = ive_df.asfreq(freq,)#'min')\n",
    "    \n",
    "    #     # Set timezone\n",
    "    #     if set_tz==True:\n",
    "    #         ive_df.tz_localize()\n",
    "    #         ive_df.index = ive_df.index.tz_convert('America/New_York')\n",
    "    \n",
    "    # Report Success / Details\n",
    "    if verbose>1:\n",
    "        print(f\"Post-Change\\t{ive_df.index.freq}\\t{ive_df.index[0]}\\t{ive_df.index[-1]}\")\n",
    "\n",
    "\n",
    "    ## FILL AND TRACK TIMEPOINTS WITH MISSING DATA    \n",
    "    \n",
    "    # Helper Function for adding column to track the datapoints that were filled\n",
    "    def check_null_times(x):\n",
    "        import numpy as np\n",
    "        if np.isnan(x):\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "\n",
    "    ## CREATE A COLUMN TO TRACK ROWS TO BE FILLED\n",
    "    # If col_to_fill provided, use that column to create/judge ive_df['filled_timebin'] \n",
    "    if col_to_fill!=None:\n",
    "        ive_df['filled_timebin'] = ive_df[col_to_fill].apply(lambda x: check_null_times(x))#True if ive_df.isna().any()\n",
    "        \n",
    "    # if not provided, use all columns and sum results\n",
    "    elif col_to_fill == None:\n",
    "        # Prefill fol with 0's\n",
    "        ive_df['filled_timebin']=0\n",
    "        \n",
    "        # loop through all columns and add results of check_null_times from each loop\n",
    "    for col in ive_df.columns:\n",
    "        if ive_df[col].dtypes=='float64':\n",
    "            #ive_df['filled_timebin'] = ive_df[target_col].apply(lambda x: check_null_times(x))#True if ive_df.isna().any()\n",
    "            curr_filled_timebin_col = ive_df[col].apply(lambda x: check_null_times(x))#True if ive_df.isna().any() \n",
    "\n",
    "            # add results\n",
    "            ive_df['filled_timebin'] +=  curr_filled_timebin_col\n",
    "            \n",
    "    ive_df['filled_timebin'] = ive_df['filled_timebin'] >0\n",
    "            \n",
    "    ## FILL IN NULL VALUES\n",
    "    ive_df.fillna(method=fill_method, inplace=True)\n",
    "\n",
    "    # Report # filled\n",
    "    if verbose>0:\n",
    "        check_fill = ive_df.loc[ive_df['filled_timebin']>0]\n",
    "        print(f'\\nFilled {len(check_fill==True)}# of rows using method {fill_method}')\n",
    "    \n",
    "    # Report any remaning null values\n",
    "    if verbose>0:\n",
    "        res = ive_df.isna().sum()\n",
    "        if res.any():\n",
    "            print(f'Cols with Nulls:')\n",
    "            print(res[res>0])\n",
    "        else:\n",
    "            print('No Remaining Null Values')   \n",
    "            \n",
    "    # display header\n",
    "    if verbose>2:\n",
    "        display(ive_df.head())\n",
    "    \n",
    "    return ive_df\n",
    "\n",
    "\n",
    "# Helper Function for adding column to track the datapoints that were filled\n",
    "def check_null_times(x):\n",
    "    import numpy as np\n",
    "    if np.isnan(x):\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LOADING STOCK DATA "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import datetime as dt\n",
    "\n",
    "skip=True\n",
    "\n",
    "if skip==False:\n",
    "    stock_df = pd.read_csv('data/stock_df_with_tech_indicators_CBH_index.csv')#, index_col=0, parse_dates=True)\n",
    "    stock_df['date'] = pd.to_datetime(stock_df['Unnamed: 0'])\n",
    "    stock_df.drop('Unnamed: 0',axis=1, inplace=True)\n",
    "    stock_df.set_index('date',inplace=True, drop=False)\n",
    "    # stock_df = stock_df.asfreq(custom_BH_freq())\n",
    "    print(stock_df.index.freq)\n",
    "    display(stock_df.head())\n",
    "def load_stock_price_series(filename='IVE_bidask1min.txt', \n",
    "                               folderpath='data/',\n",
    "                               start_index = '2017-01-23', freq='T'):\n",
    "    import pandas as pd\n",
    "\n",
    "    # Load in the text file and set headers\n",
    "    fullfilename= folderpath+filename\n",
    "    headers = ['Date','Time','BidOpen','BidHigh','BidLow','BidClose','AskOpen','AskHigh','AskLow','AskClose']\n",
    "    stock_df = pd.read_csv(fullfilename, names=headers,parse_dates=True,usecols=['Date','Time','BidClose'])\n",
    "\n",
    "    # Create datetime index\n",
    "    date_time_index = stock_df['Date']+' '+stock_df['Time']\n",
    "    date_time_index = pd.to_datetime(date_time_index)\n",
    "    stock_df.index=date_time_index\n",
    "\n",
    "    # Select only the days after start_index\n",
    "    stock_df = stock_df[start_index:]\n",
    "\n",
    "    stock_price = stock_df['BidClose'].rename('stock_price')\n",
    "    stock_price[stock_price==0] = np.nan\n",
    "\n",
    "    return stock_price\n",
    "\n",
    "stock_price = load_stock_price_series()\n",
    "display(stock_price.head())\n",
    "\n",
    "stock_price.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SELECTING A SUBSET OF STOCK_DF COLUMNS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if skip==False:\n",
    "    stock_df = set_timeindex_freq(stock_df,verbose=0)\n",
    "    # full_df = stock_df.copy()\n",
    "    stock_df = stock_df.iloc[:,10:]\n",
    "\n",
    "    # DIsply input stock data\n",
    "    display(stock_df.head().style.set_caption('Raw Data'))\n",
    "    # plot_time_series(stock_df['price'])\n",
    "    stock_df.index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ADDING A DELTA STOCK PRICE COLUMNS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Binning stock_df\n",
    "\n",
    "- am no longer sure I want to do this.\n",
    "- could instead use twitter_df and .ceil(CBH) to create a new column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "```python\n",
    "# make new dataframe starting with date and content\n",
    "df = stock_df.copy() #'content_stopped',\n",
    "\n",
    "# create a series with all hours covering entire twitter_df\n",
    "time_index_hour_bins = pd.date_range(start = stock_df.index[0].floor('H'), end = stock_df.index[-1].ceil('H'), freq='H')#.to_period()\n",
    "\n",
    "# bin the twitter_df.index by the new time_index_hour_bins\n",
    "df['time_index_by_hour'] = pd.cut(stock_df['date'], time_index_hour_bins)\n",
    "\n",
    "# extract JUST the hour_of_day as a feature\n",
    "df['hour_of_day'] = df['date'].apply(lambda x: x.hour).apply(lambda x: dt.time(x))\n",
    "df.drop('date',axis=1,inplace=True)\n",
    "df.head()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LOADING IN TWITTER DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bs.big_pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "import inspect\n",
    "def ihelp(any_function, show_help=True, show_code=True, get_source=True): \n",
    "    \"\"\"Call on any module or functon to display:\n",
    "    - help(any_function)\n",
    "    - source_df = inspect.getsource(any_function)\n",
    "    inspect.get)\"\"\"\n",
    "    import inspect\n",
    "    import pprint as pp\n",
    "    if show_help:\n",
    "        \n",
    "        print(\"---\"*40)\n",
    "        print(\"---\"*3,'\\tHELP:\\t',\"---\"*30)\n",
    "        print(\"---\"*40)\n",
    "        help(any_function)\n",
    "#         print('\\n\\n',\"---\"*20,'\\n')\n",
    "        \n",
    "    if show_code or get_source:\n",
    "        \n",
    "        import inspect\n",
    "        source_DF = inspect.getsource(any_function)\n",
    "        print(\"---\"*40)\n",
    "        print(\"---\"*3,'\\tSOURCE:\\t',\"---\"*30)\n",
    "        print(\"---\"*40)\n",
    "        print(source_DF)    \n",
    "#         if show_code:\n",
    "\n",
    "\n",
    "#     if get_source:\n",
    "#         return source_DF\n",
    "# ihelp(bs.list2df, show_help=True)\n",
    "\n",
    "#***#\n",
    "# del(twitter_df)\n",
    "\n",
    "def load_twitter_df(overwrite=True,set_index='time_index',verbose=2,replace_na=''):\n",
    "\n",
    "    try: twitter_df\n",
    "    except NameError: twitter_df = None\n",
    "    if twitter_df is not None:\n",
    "        print('twitter_df already exists.')\n",
    "        if overwrite==True:\n",
    "            print('Overwrite=True. deleting original...')\n",
    "            del(twitter_df)\n",
    "            \n",
    "    if twitter_df is None:\n",
    "        print('loading twitter_df')\n",
    "        \n",
    "        twitter_df = pd.read_csv('data/trump_twitter_archive_df.csv', encoding='utf-8', parse_dates=True)\n",
    "        twitter_df.drop('Unnamed: 0',axis=1,inplace=True)\n",
    "\n",
    "        twitter_df['date']  = pd.to_datetime(twitter_df['date'])\n",
    "        twitter_df['time_index'] = twitter_df['date'].copy()\n",
    "        twitter_df.set_index(set_index,inplace=True,drop=True)\n",
    "\n",
    "\n",
    "        # Fill in missing values before merging with stock data\n",
    "        twitter_df.fillna(replace_na, inplace=True)\n",
    "        twitter_df.sort_index(ascending=True, inplace=True)\n",
    "\n",
    "        # RECASTING A COUPLE COLUMNS\n",
    "        twitter_df['is_retweet'] = twitter_df['is_retweet'].astype('bool')\n",
    "        twitter_df['id_str'] = twitter_df['id_str'].astype('str')\n",
    "        twitter_df['sentiment_class'] = twitter_df['sentiment_class'].astype('category')\n",
    "\n",
    "#         twitter_df.reset_index(inplace=True)\n",
    "        # Check header and daterange of index\n",
    "    if verbose>0:\n",
    "        display(twitter_df.head(2))\n",
    "        print(twitter_df.index[[0,-1]])\n",
    "    return twitter_df\n",
    "    \n",
    "    \n",
    "# twitter_df = load_twitter_df()\n",
    "\n",
    "#***#\n",
    "# DISPLAYING PLAN FOR DEALING WITH DATA\n",
    "# df_dtypes =pd.DataFrame({'type':twitter_df.dtypes})#,'null':twitter_df.isna().sum(),'count':twitter_df.count()})\n",
    "def df_column_report(twitter_df, sort_column=None, ascending=True, interactive=True):\n",
    "    from ipywidgets import interact\n",
    "    df_dtypes=pd.DataFrame()\n",
    "    df_dtypes = pd.DataFrame({'Column #': range(len(twitter_df.columns)),'Column Name':twitter_df.columns,\n",
    "                              'Data Types':twitter_df.dtypes.astype('str')}).set_index('Column Name') #.set_index('Column Name')\n",
    "    \n",
    "    decision_map = {'object':'join','int64':'sum','bool':'to_list()?','float64':'drop and recalculate'}\n",
    "    \n",
    "    df_dtypes['action'] = df_dtypes['Data Types'].map(decision_map)#column_list\n",
    "#     df_dtypes.style.set_caption('DF Columns, Dtypes, and Course of Action')\n",
    "    \n",
    "    if sort_column is not None:\n",
    "        df_dtypes.sort_values(by =sort_column,ascending=ascending, axis=0, inplace=True)\n",
    "    if interactive==False:\n",
    "        return df_dtypes\n",
    "    else: \n",
    "        \n",
    "        @interact(column= df_dtypes.columns,direction={'ascending':True,'descending':False})\n",
    "        def sort_df(column, direction):\n",
    "            return df_dtypes.sort_values(by=column,axis=0,ascending=direction)\n",
    "\n",
    "# df_dtypes =  df_column_report(twitter_df, sort_column=['Data Types','Column #'])\n",
    "# df_dtypes\n",
    "# res_df = df_column_report(twitter_df)\n",
    "\n",
    "#***#a\n",
    "# twitter_df = load_twitter_df()\n",
    "\n",
    "def make_half_hour_range(twitter_df):\n",
    "    \n",
    "    # Get timebin before the first timestamp that starts at 30m into the hour\n",
    "    ofst_30m_early=pd.offsets.Minute(-30)\n",
    "    start_idx = ofst_30m_early(twitter_df['date'].iloc[0].floor('H'))\n",
    "\n",
    "    # Get timbin after last timestamp that starts 30m into the hour.\n",
    "    ofst_30m_late =pd.offsets.Minute(30)\n",
    "    end_idx= ofst_30m_late(twitter_df['date'].iloc[-1].ceil('H'))\n",
    "\n",
    "\n",
    "    # Make time bins using the above start and end points \n",
    "    half_hour_range = pd.date_range(start =start_idx, end = end_idx, freq='30T')#.to_period()\n",
    "    half_hour_intervals = pd.interval_range(start=start_idx, end=end_idx,freq='30T',name='half_hour_bins',closed='left')\n",
    "    \n",
    "    return half_hour_intervals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ihelp(load_stock_price_series, show_help=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to merge stock-market-business hours tweets?\n",
    "- use `custom_BH_freq` to create a new column that attempts to re-cast the index as this frequency.\n",
    "    - separate the index off first to another column/dataframe.\n",
    "    - create a second column that rounds/floors/ceils the timestamp values to the correct business hour\n",
    "    - **NOTE**: -filling? revisit\n",
    "    \n",
    "- Fundamental question: how do I want to determine which stock timebin to evaluate effect of Trump tweet?\n",
    "    - time series analysis would want the tweet to be in the hour PRIOR to the timepoint to be judged.\n",
    "- **SO how to best accomplish this?**\n",
    "    - if do `twitter_df['date'].floor(H), then any tweets from within an hour would be classified as that hour's timebin,\n",
    "        - if he tweeted at 12:55, that would be considered 12 pm, so using 1 pm as the +1 hour timebin is only really 5 MINS!\n",
    "        - What if add +1hour to the `date` column's timestamps, then use `.round('H')`\n",
    "            - **This would probably be the best way to go about it!**\n",
    "            \n",
    "**TO DO:**\n",
    "- [ ] Add +1 hour to twitter_df['date'], then use .round('H')\n",
    "\n",
    "- **SIMPLER APPROACH: Write function to concatenate rows** (07/04/19)\n",
    "    - use new_df = df.resample('H').apply(my_func) \n",
    "         (or can I do 30mins instead of 'H'?)\n",
    "         \n",
    "## HOW TO INSTRUCTIONS FOR FUNCTIONS BELOW\n",
    "\n",
    "```python \n",
    "\n",
    "# #***#  \n",
    "twitter_df = load_twitter_df()\n",
    "\n",
    "half_hour_intervals = make_half_hour_range(twitter_df)\n",
    "\n",
    "twitter_df, bin_codes = bin_df_by_date_intervals(twitter_df, half_hour_intervals)\n",
    "\n",
    "group_indices = twitter_df.groupby('int_bins').groups\n",
    "group_indices = [(k,v) for k,v in group_indices.items()]\n",
    "\n",
    "\n",
    "#***#\n",
    "new_col_order = ['date','left_edge','content_raw','content_stopped','tokens_stopped',\n",
    "                  'retweet_count','favorite_count','case_ratio','sentiment_scores','compound_score','int_bins']\n",
    "\n",
    "twitter_grouped = collapse_df_by_group_indices(twitter_df, group_indices, new_col_order=new_col_order)\n",
    "# twitter_grouped.head(2)\n",
    "\n",
    "twitter_grouped['time_bin'] = twitter_grouped['left_edge'].apply(lambda x: pd.to_datetime(x[0]))\n",
    "twitter_grouped.set_index('time_bin',drop=True, inplace=True)\n",
    "twitter_grouped.head(2)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### List of Actions by Columns Types\n",
    "\n",
    "- string/object columns (twitter_df.select_dtypes('object')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # TAKING A SMALLER TWITTER_DF \n",
    "# twitter_df = twitter_df.iloc[:1000,:]\n",
    "# display(twitter_df.head())\n",
    "# type_list = twitter_df.dtypes\n",
    "# # display(type_list)\n",
    "# # display(twitter_df.index)\n",
    "# # twitter_df.set_index('time_index',inplace=True, verify_integrity=True)\n",
    "# # twitter_df.duplicated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get timebin before the first timestamp that starts at 30m into the hour\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# twitter_df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### def cut_df_by_intervals_to_number_code, concat_df_group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "#***#\n",
    "def int_to_ts(int_list, as_datetime=False, as_str=True):\n",
    "    \"\"\"Accepts one Panda's interval and returns the left and right ends as either strings or Timestamps.\"\"\"\n",
    "    if as_datetime & as_str:\n",
    "        raise Exception('Only one of `as_datetime`, or `as_str` can be True.')\n",
    "    \n",
    "    left_edges =[]\n",
    "    right_edges= []\n",
    "    \n",
    "    for interval in int_list:\n",
    "        int_str = interval.__str__()[1:-1]\n",
    "        left,right = int_str.split(',')\n",
    "        left_edges.append(left)\n",
    "        right_edges.append(right)\n",
    "        \n",
    "    \n",
    "    if as_str:\n",
    "        return left_edges, right_edges\n",
    "    \n",
    "    elif as_datetime:\n",
    "        left = pd.to_datetime(left)\n",
    "        right = pd.to_datetime(right)\n",
    "        return left,right\n",
    "    \n",
    "    \n",
    "# Step 1:     \n",
    "def bin_df_by_date_intervals(test_df,half_hour_intervals,column='date'):\n",
    "    \"\"\"\"\"\"\n",
    "    # Cut The Date column into interval bins, \n",
    "    cut_date = pd.cut(test_df[column], bins=half_hour_intervals)#,labels=list(range(len(half_hour_intervals))), retbins=True)\n",
    "    test_df['int_times'] = cut_date    \n",
    "    \n",
    "    # convert to str to be used as group names/codes\n",
    "    unique_bins = cut_date.astype('str').unique()\n",
    "    num_code = list(range(len(unique_bins)))\n",
    "    \n",
    "    # Dictioanry of number codes to be used for interval groups\n",
    "    bin_codes = dict(zip(num_code,unique_bins))#.astype('str')\n",
    "\n",
    "    \n",
    "    # Mapper dictionary to convert intervals into number codes\n",
    "    bin_codes_mapper = {v:k for k,v in bin_codes.items()}\n",
    "\n",
    "    \n",
    "    # Add column to the dataframe, then map integer code onto it\n",
    "    test_df['int_bins'] = test_df['int_times'].astype('str').map(bin_codes_mapper)\n",
    "    \n",
    "    \n",
    "    # Get the left edge of the bins to use later as index (after grouped)\n",
    "    left_out, _ =int_to_ts(test_df['int_times'])#.apply(lambda x: int_to_ts(x))    \n",
    "    test_df['left_edge'] = pd.to_datetime(left_out)\n",
    "\n",
    "    # bin codes to labels \n",
    "    bin_codes = [(k,v) for k,v in bin_codes.items()]\n",
    "    \n",
    "    return test_df, bin_codes\n",
    "\n",
    "\n",
    "def concatenate_group_data(group_df_or_series):\n",
    "    \"\"\"Accepts a series or dataframe from a groupby.get_group() loop.\n",
    "    Adds TweetFreq column for # of rows concatenate. If input is series, \n",
    "    TweetFreq=1 and series is returned.\"\"\"\n",
    "    \n",
    "    import pandas as pd\n",
    "    from pandas.api import types as tp\n",
    "    \n",
    "    if isinstance(group_df_or_series, pd.Series):\n",
    "        \n",
    "        group_data = group_df_or_series\n",
    "        \n",
    "#         group_data.index = group_df_or_series.index\n",
    "        group_data['TweetFreq'] = 1\n",
    "\n",
    "        return group_data\n",
    "    \n",
    "    # if the group is a dataframe:\n",
    "    elif isinstance(group_df_or_series, pd.DataFrame):\n",
    "        \n",
    "        df = group_df_or_series\n",
    "        \n",
    "        # create an output series to collect combined data\n",
    "        group_data = pd.Series(index=df.columns)\n",
    "        group_data['TweetFreq'] = df.shape[0]\n",
    "        \n",
    "\n",
    "        for col in df.columns:\n",
    "            \n",
    "            combined=[]\n",
    "            col_data = []\n",
    "            \n",
    "            col_data = df[col]\n",
    "            combined=col_data.values\n",
    "            \n",
    "            group_data[col] = combined\n",
    "\n",
    "    return group_data\n",
    "\n",
    "\n",
    "#***#\n",
    "def collapse_df_by_group_indices(twitter_df,group_indices, new_col_order=None):\n",
    "    \"\"\"Loops through the group_indices provided to concatenate each group into\n",
    "    a single row and combine into one dataframe with the ______ as the index\"\"\"\n",
    "\n",
    "\n",
    "    # Create a Panel to temporarily hold the group series and dataframes\n",
    "    # group_dict_to_df = {}\n",
    "    # create a dataframe with same columns as twitter_df, and index=group ids from twitter_groups\n",
    "    group_df_index = [x[0] for x in group_indices]\n",
    "    \n",
    "    \n",
    "    twitter_grouped = pd.DataFrame(columns=twitter_df.columns, index=group_df_index)\n",
    "    twitter_grouped['TweetFreq'] =0\n",
    "\n",
    "    for (idx,group_members) in group_indices:\n",
    "\n",
    "        group_df = twitter_df.loc[group_members]\n",
    "\n",
    "        combined_series = concatenate_group_data(group_df)\n",
    "\n",
    "#         twitter_grouped.loc[idx,:] = combined_series\n",
    "        twitter_grouped.loc[idx] = combined_series#.values\n",
    "\n",
    "    if new_col_order==None:\n",
    "        return twitter_grouped\n",
    "    \n",
    "    else:\n",
    "        df_out = twitter_grouped[new_col_order].copy()\n",
    "        df_out.index = group_df_index#twitter_grouped.index\n",
    "        return df_out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now that twitter df has int_bins, use them for groupby and concatenation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inspect.getsourcelines(load_twitter_df)\n",
    "# twitter_grouped.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### TROUBLESHOOTING CODE BELOW\n",
    "# twitter_df = load_twitter_df()\n",
    "\n",
    "# half_hour_intervals = make_half_hour_range(twitter_df)\n",
    "\n",
    "# cut_date = pd.cut(twitter_df['date'], bins=half_hour_intervals)#,labels=list(range(len(half_hour_intervals))), retbins=True)\n",
    "# twitter_df['int_times'] =cut_date\n",
    "# # display(twitter_df.head())\n",
    "\n",
    "\n",
    "# # convert to str to be used as group names/codes\n",
    "# unique_bins = cut_date.astype('str').unique()\n",
    "# num_code = list(range(len(unique_bins)))\n",
    "\n",
    "# # Dictioanry of number codes to be used for interval groups\n",
    "# bin_codes = dict(zip(num_code,unique_bins))#.astype('str')\n",
    "\n",
    "\n",
    "# # Mapper dictionary to convert intervals into number codes\n",
    "# bin_codes_mapper = {v:k for k,v in bin_codes.items()}\n",
    "\n",
    "\n",
    "# # Add column to the dataframe, then map integer code onto it\n",
    "# twitter_df['int_bins'] = twitter_df['int_times'].astype('str').map(bin_codes_mapper)\n",
    "# twitter_df.head()\n",
    "\n",
    "# # Get the left edge of the bins to use later as index (after grouped)\n",
    "# left_out, _ =int_to_ts(twitter_df['int_times'])#.apply(lambda x: int_to_ts(x))    \n",
    "# twitter_df['left_edge'] = pd.to_datetime(left_out)\n",
    "\n",
    "# # bin codes to labels \n",
    "# bin_codes = [(k,v) for k,v in bin_codes.items()]\n",
    "\n",
    "\n",
    "# group_indices = twitter_df.groupby('int_bins').groups\n",
    "# group_indices = [(k,v) for k,v in group_indices.items()]\n",
    "\n",
    "\n",
    "# #***#\n",
    "# new_col_order = ['date','left_edge','content_raw','content_stopped','tokens_stopped',\n",
    "#                   'retweet_count','favorite_count','case_ratio','sentiment_scores','compound_score','int_bins']\n",
    "\n",
    "# twitter_grouped = collapse_df_by_group_indices(twitter_df, group_indices, new_col_order=new_col_order)\n",
    "\n",
    "# twitter_grouped['time_bin'] = twitter_grouped['left_edge'].apply(lambda x: pd.to_datetime(x[0]))\n",
    "# twitter_grouped.set_index('time_bin',drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if skip==False:\n",
    "    # #***#  \n",
    "    twitter_df = load_twitter_df()\n",
    "\n",
    "    half_hour_intervals = make_half_hour_range(twitter_df)\n",
    "\n",
    "    twitter_df, bin_codes = bin_df_by_date_intervals(twitter_df, half_hour_intervals)\n",
    "\n",
    "    group_indices = twitter_df.groupby('int_bins').groups\n",
    "    group_indices = [(k,v) for k,v in group_indices.items()]\n",
    "\n",
    "\n",
    "    #***#\n",
    "    new_col_order = ['date','left_edge','content_raw','content_stopped','tokens_stopped',\n",
    "                      'retweet_count','favorite_count','case_ratio','sentiment_scores','compound_score','int_bins']\n",
    "\n",
    "    twitter_grouped = collapse_df_by_group_indices(twitter_df, group_indices, new_col_order=new_col_order)\n",
    "    # twitter_grouped.head(2)\n",
    "\n",
    "    twitter_grouped['time_bin'] = twitter_grouped['left_edge'].apply(lambda x: pd.to_datetime(x[0]))\n",
    "    twitter_grouped.set_index('time_bin',drop=True, inplace=True)\n",
    "    twitter_grouped.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pause"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### INTERACTIVE PLOT TIME SERIES\n",
    "\n",
    "- Make a verison of plot_time_series using some version of interact to be able to ADJUST THE TIME AXIS\n",
    "\n",
    "- https://github.com/bloomberg/bqplot/issues/712"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if skip==False:\n",
    "    # Make a Layout with the range slider and labels as two different boxes, one on top of the other\n",
    "\n",
    "    start_date = stock_df.index[0]\n",
    "    end_date  = stock_df.index[-1]\n",
    "\n",
    "    def make_date_range_slider(start_date,end_date,freq='D'):\n",
    "\n",
    "        from ipywidgets import interact, interactive, interaction, Label, Box, Layout\n",
    "        import ipywidgets as iw\n",
    "        from datetime import datetime\n",
    "\n",
    "        # specify the date range from user input\n",
    "        dates = pd.date_range(start_date, end_date,freq=freq)\n",
    "\n",
    "        # specify formatting based on frequency code\n",
    "        date_format_lib={'D':'%m/%d/%Y','H':'%m/%d/%Y: %T'}\n",
    "        freq_format = date_format_lib[freq]\n",
    "\n",
    "\n",
    "        # creat options list and index for SelectionRangeSlider\n",
    "        options = [(date.strftime(date_format_lib[freq]),date) for date in dates]\n",
    "        index = (0, len(options)-1)\n",
    "\n",
    "        #     # Create out function to display outputs (not needed?)\n",
    "        #     out = iw.Output(layout={'border': '1px solid black'})\n",
    "        #     #     @out.capture()\n",
    "\n",
    "        # Instantiate the date_range_slider\n",
    "        date_range_slider = iw.SelectionRangeSlider(\n",
    "            options=options, index=index, description = 'Date Range',\n",
    "            orientation = 'horizontal',layout={'width':'500px','grid_area':'main'},#layout=Layout(grid_area='main'),\n",
    "            readout=True)\n",
    "\n",
    "        # Save the labels for the date_range_slider as separate items\n",
    "        date_list = [date_range_slider.label[0], date_range_slider.label[-1]]\n",
    "        date_label = iw.Label(f'{date_list[0]} -- {date_list[1]}',\n",
    "                             layout=Layout(grid_area='header'))\n",
    "\n",
    "        def updateXAxis(change):\n",
    "            #Update X-axis min/max value here\n",
    "            if change['type'] == 'change' and change['name'] == 'value':\n",
    "                x_start = change['new'][0]\n",
    "                x_end = change['new'][1]\n",
    "        date_range_slider.observe(updateXAxis)\n",
    "    #             x_sc.min = change['new'][0]\n",
    "    #             x_sc.max = change['new'][1]\n",
    "\n",
    "    #     source1, target1 = date_range_slider, date_label\n",
    "    #     dl = iw.dlink((soruce1,'label'),(target1,'value'))\n",
    "\n",
    "    #     ## ADJUST LABEL OUTPUT TO MATCH SLIDER\n",
    "    #     output2 = date_label#widgets.Output()\n",
    "    #     def on_value_change(change):\n",
    "    #         with output2:\n",
    "    #             print(change['new'])\n",
    "    #     header  = date_range_slider\n",
    "    #     main    = date_label      \n",
    "\n",
    "    #     slider_items=[date_range_slider, date_label]\n",
    "    #     output = iw.GridBox(children=[date_range_slider,date_label],\n",
    "    #                        layout=Layout(\n",
    "    #                        grid_template_rows='auto auto',\n",
    "    #                        grid_template_columns='auto auto',\n",
    "    #                        grid_template_areas='''\n",
    "    #                        \"header header\"\n",
    "    #                        \"main main\"\n",
    "    #                        '''))# display='flex','flex_flow'\n",
    "        return date_range_slider\n",
    "\n",
    "    make_date_range_slider(start_date, end_date,'D')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# # Instantiate the date_range_slider\n",
    "# date_range_slider = iw.SelectionRangeSlider(\n",
    "#     options=options, index=index, description = 'Date Range',\n",
    "#     orientation = 'horizontal',layout={'width':'500px'},readout=False)\n",
    "\n",
    "# # Save the labels for the date_range_slider as separate items\n",
    "# # date_list = [date_range_slider.label[0], date_range_slider.label[-1]]\n",
    "# date_label1 = iw.Label(date_range_slider.label[0])\n",
    "# date_label2 = iw.Label(date_range_slider.label[-1])\n",
    "# source_1, target_1 = date_range_slider, iw.Label\n",
    "\n",
    "# def tf_label(label):\n",
    "#     return str(label[0])\n",
    "# dl1 = iw.dlink((source_1,'label',lambda x: tf_label(x)),(target_1,'value'))#,lambda x: str(x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SAVE AND LOAD GROUPED DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# twitter_grouped.to_csv('twitter_data_grouped.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# twitter_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# twitter_grouped.loc['02-21-2017']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stock_df.loc['02-21-2017'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# twitter_grouped.loc['02-21-2017':].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stock_prices = stock_df['price']\n",
    "# stock_prices.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CBH = custom_BH_freq()\n",
    "\n",
    "# grouped_resampled = twitter_grouped.loc['02-21-2017':].asfreq(CBH)\n",
    "# stock_resampled = stock_df.loc['02-21-2017':].asfreq(CBH)\n",
    "# # grouped_resampled = grouped_resampled.loc[stock_df.index[0]:stock_df.index[-1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display_range = stock_resampled.index\n",
    "# display_range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grouped_resampled.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grouped_twitter_valid = grouped_resampled['content_raw'].notnull().index\n",
    "# grouped_twitter_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # display_range = grouped_resampled.index\n",
    "# from ipywidgets import interact\n",
    "# display_range = grouped_resampled['content_raw'].notnull().index\n",
    "\n",
    "\n",
    "# @interact(date=display_range, num_items=range(3,10))\n",
    "# def display_date(date=['02-22-2017'], num_items=10, filter_null=True):\n",
    "    \n",
    "#     display(grouped_resampled.loc[date:].head(num_items).style.set_caption('Twitter'))\n",
    "#     display(stock_resampled.loc[date:].head(num_items).style.set_caption('S&P500'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grouped_resampled['join_date'] = grouped_resampled.index\n",
    "# stock_df['join_date'] =  stock_df.index\n",
    "\n",
    "# print(grouped_resampled.join_date)\n",
    "# print(stock_df.join_date)\n",
    "\n",
    "# # print(len(grouped_resampled),len(twitter_grouped))\n",
    "\n",
    "# # compare_indices = [True for i in grouped_resampled.index if i in stock_df.index]\n",
    "# # np.sum(compare_indices==False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BOOKMARK _ NEW APPROACH. \n",
    "\n",
    "- use `load_stock_price_series()` to get a SERIES of just stock prices (not yet on official Freq)\n",
    "- use twitter_df data (not grouped)\n",
    "- use new function to take each tweet's timestamp, then find the corresponding stock price X amount of time later (then difference)\n",
    "\n",
    "### GOAL NOW:\n",
    "\n",
    "- [ ] write a function that will tweet timestamps and find the next corresponding stock price to use as labels\n",
    "- [ ] **add a rolling forward offset that would move off hours timestamps to 09:30 am next day**\n",
    "\n",
    "`match_stock_price_to_tweets`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grouped_resampled.index, stock_df.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grouped_resampled = grouped_resampled.join(stock_df['price'],how='right')#,left_index=True, right_index=True)\n",
    "# display(grouped_resampled.head())\n",
    "# display(grouped_resampled.tail())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## LOAD STOCK PRICE SERIES\n",
    "# # Make stock price reference series with minute resolution\n",
    "# stock_price = load_stock_price_series()\n",
    "# print(f'Null Values when Freq={stock_price.index.freq}:\\t{stock_price.isna().sum()}')\n",
    "\n",
    "# # Change to Minute-Data\n",
    "# stock_price = stock_price.asfreq('T')\n",
    "# print(f'Null Values when Freq={stock_price.index.freq}:\\t{stock_price.isna().sum()}')\n",
    "\n",
    "# stock_price.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Dropna in stock_Series?\n",
    "# # stock_price.fillna(-999, inplace=True)\n",
    "# stock_price.dropna(inplace=True)\n",
    "# stock_price.index\n",
    "# # stock_price.loc[stock_price > 0].plot()\n",
    "# # plt.figure()\n",
    "# # stock_price.loc[stock_price < 0].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOAD IN STOCK PRICE SERIES\n",
    "# try \n",
    "# display(stock_price.head())\n",
    "# stock_price.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# test_per = twitter_periods.index[0]\n",
    "# test_per\n",
    "\n",
    "# test_index = twitter_df.index[0]\n",
    "# test_index\n",
    "\n",
    "# # get_month_day_year(test_per), get_month_day_year(test_index)\n",
    "# test_index.second\n",
    "\n",
    "# twitter_df['B_day']= [str(x.strftime('%m-%d-%Y')) for x in twitter_periods['time_index'].values]\n",
    "\n",
    "# twitter_df['B_day'][0],twitter_df['day'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CURRENT:\n",
    "code as of morning of 07/08\n",
    "```python\n",
    "\n",
    "# CREATE COLUMNS TO GET NEXT BUSINESS DAY\n",
    "def get_month_day_year(x):\n",
    "    x_date =dt.date(month=x.month, day=x.day, year=x.year)\n",
    "    return x_date.strftime('%m-%d-%Y')\n",
    "\n",
    "def get_time_of_day(x):\n",
    "    x_time = dt.time(hour= x.hour, minute=x.minute, second=x.second)\n",
    "    return x_time.strftime('%T')\n",
    "\n",
    "\n",
    "# Separate day and time for twitter timestamp \n",
    "twitter_df['day'] = twitter_df['date'].apply(lambda x: get_month_day_year(x))\n",
    "# twitter_df['day'] =  pd.to_datetime(twitter_df['day'])\n",
    "\n",
    "twitter_df['time'] =  twitter_df['date'].apply(lambda x: get_time_of_day(x))\n",
    "\n",
    "# Get the corresponding next business day for each row\n",
    "twitter_periods = twitter_df.to_period('B')\n",
    "twitter_periods.reset_index(inplace=True)\n",
    "\n",
    "# def get_bday_btime(x):\n",
    "#     for r\n",
    "twitter_df['B_day']= [str(x.strftime('%m-%d-%Y')) for x in twitter_periods['time_index'].values]\n",
    "twitter_df['B_shifted']=np.where(twitter_df['day']== twitter_df['B_day'],False,True);\n",
    "twitter_df['B_time'] = np.where(twitter_df['day']== twitter_df['B_day'],twitter_df['time'],dt.time(hour=9, minute=30))#:30:00');\n",
    "twitter_df['B_dt_index'] = [ str(twitter_df['B_day'].iloc[x]) +'  '+ str(twitter_df['B_time'].iloc[x])  for x in range(len(twitter_df))]\n",
    "twitter_df['B_dt_index'] = pd.to_datetime(twitter_df['B_dt_index'])#asfreq('T')\n",
    "# twitter_df['B_dt_index'] = twitter_df['B_dt_index'].round('T')\n",
    "\n",
    "\n",
    "\n",
    "twitter_df.head(16)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# type(twitter_df['time'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ts = twitter_df.index[2]\n",
    "# print(ts)\n",
    "# ts.strftime('%H:%M')\n",
    "# ts.round('T')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FULL WORKFLOW FOR TWITTER-STOCK DATA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### def functions for getting business day info (get_B_day_time_index_shift)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_B_day_time_index_shift(test_df, verbose=1):\n",
    "\n",
    "    fmtYMD= '%Y-%m-%d'\n",
    "\n",
    "    test_df['day']= test_df['date'].dt.strftime('%Y-%m-%d')\n",
    "    test_df['time'] = test_df['date'].dt.strftime('%T')\n",
    "    test_df['dayofweek'] = test_df['date'].dt.day_name()\n",
    "\n",
    "    test_df_to_period = test_df[['date','content']]\n",
    "    test_df_to_period = test_df_to_period.to_period('B')\n",
    "    test_df_to_period['B_periods'] = test_df_to_period.index.values\n",
    "    test_df_to_period['B_day'] = test_df_to_period['B_periods'].apply(lambda x: x.strftime(fmtYMD))\n",
    "\n",
    "\n",
    "\n",
    "    test_df['B_day'] = test_df_to_period['B_day'].values\n",
    "    test_df['B_shifted']=np.where(test_df['day']== test_df['B_day'],False,True)\n",
    "    test_df['B_time'] = np.where(test_df['B_shifted'] == True,'09:30:00', test_df['time'])\n",
    "    \n",
    "    test_df['B_dt_index'] = pd.to_datetime(test_df['B_day'] + ' ' + test_df['B_time']) \n",
    "\n",
    "    test_df['time_shift'] = test_df['B_dt_index']-test_df['date'] \n",
    "    \n",
    "    if verbose > 0:\n",
    "        test_df.head(20)\n",
    "    \n",
    "    return test_df\n",
    "\n",
    "def reorder_twitter_df_columns(twitter_df, order=[]):\n",
    "    if len(order)==0:\n",
    "        order=['date','dayofweek','B_dt_index','source','content','content_raw','retweet_count','favorite_count','sentiment_scores','time_shift']\n",
    "    twitter_df_out = twitter_df[order]\n",
    "    twitter_df_out.index = twitter_df.index\n",
    "    return twitter_df_out\n",
    "\n",
    "\n",
    "def match_stock_price_to_tweets(tweet_timestamp,time_after_tweet= 30,time_freq ='T',stock_price=[]):#stock_price_index=stock_date_data):\n",
    "    \n",
    "    import pandas as pd\n",
    "    from datetime import datetime as dt\n",
    "    # output={'pre_tweet_price': price_at_tweet,'post_tweet_price':price_after_tweet,'delta_price':delta_price, 'delta_time':delta_time}\n",
    "    output={}\n",
    "    # convert tweet timestamp to minute accuracy\n",
    "    ts=[]\n",
    "    ts = pd.to_datetime(tweet_timestamp).round(time_freq)\n",
    "    \n",
    "    BH = pd.tseries.offsets.BusinessHour(start='09:30',end='16:30')\n",
    "    BD = pd.tseries.offsets.BusinessDay()\n",
    "    \n",
    "    \n",
    "    # checking if time is within stock_date_data\n",
    "#     def roll_B_day_forward(ts):\n",
    "        \n",
    "    if ts not in stock_price.index:\n",
    "        ts = BH.rollforward(ts)        \n",
    "        \n",
    "        if ts not in stock_price.index:\n",
    "            return np.nan#\"ts2_not_in_index\"\n",
    "\n",
    "    # Get price at tweet time\n",
    "    price_at_tweet = stock_price.loc[ts]\n",
    "    \n",
    "    if np.isnan(price_at_tweet):\n",
    "        output['pre_tweet_price'] = np.nan\n",
    "    else: \n",
    "        output['pre_tweet_price'] = price_at_tweet\n",
    "                    \n",
    "        \n",
    "    # Use timedelta to get desired timepoint following tweet\n",
    "    hour_freqs = 'BH','H','CBH'\n",
    "    day_freqs = 'B','D'\n",
    "\n",
    "    if time_freq=='T':\n",
    "        ofst=pd.offsets.Minute(time_after_tweet)\n",
    "\n",
    "    elif time_freq in hour_freqs:\n",
    "        ofst=pd.offsets.Hour(time_after_tweet)\n",
    "\n",
    "    elif time_freq in day_freqs:\n",
    "        ofst=pd.offsets.Day(time_after_tweet)\n",
    "\n",
    "\n",
    "    # get timestamp to check post-tweet price\n",
    "    post_tweet_ts = ofst(ts)\n",
    "\n",
    "    \n",
    "    if post_tweet_ts not in stock_price.index:\n",
    "#         post_tweet_ts =BD.rollforward(post_tweet_ts)\n",
    "        post_tweet_ts = BH.rollforward(post_tweet_ts)\n",
    "    \n",
    "        if post_tweet_ts not in stock_price.index:\n",
    "            return np.nan\n",
    "\n",
    "\n",
    "    # Get next available stock price\n",
    "    price_after_tweet = stock_price.loc[post_tweet_ts]\n",
    "    if np.isnan(price_after_tweet):\n",
    "        output['post_tweet_price'] = 'NaN in stock_price'\n",
    "    else:\n",
    "        # calculate change in price\n",
    "        delta_price = price_after_tweet - price_at_tweet\n",
    "        delta_time = post_tweet_ts - ts\n",
    "        output['post_tweet_price'] = price_after_tweet\n",
    "        output['delta_time'] = delta_time\n",
    "        output['delta_price'] = delta_price\n",
    "\n",
    "#         output={'pre_tweet_price': price_at_tweet,'post_tweet_price':price_after_tweet,'delta_price':delta_price, 'delta_time':delta_time}\n",
    "\n",
    "    return output\n",
    "    \n",
    "def unpack_match_stocks(stock_dict):\n",
    "    stock_series = pd.Series(stock_dict)\n",
    "    return stock_series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stock_price.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ###   APPLYING FULL WORKFLOW WRITTEN ABOVE IN CODE ###\n",
    "# ## LOAD IN STOCK PRICE\n",
    "# del stock_price\n",
    "# try: stock_price\n",
    "# except NameError: stock_price = None\n",
    "# if stock_price is  None:    \n",
    "#     print('loading stock_price')\n",
    "#     stock_price = load_stock_price_series()\n",
    "# else:\n",
    "#     print('using pre-existing stock_price')\n",
    "        \n",
    "# # Make sure stock_price is loaded as minute data\n",
    "# stock_price = stock_price.asfreq('T')\n",
    "# stock_price.isna().sum()\n",
    "# # stock_price.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stock_price[:10]\n",
    "# stock_times = stock_price.index.strftime('%T')\n",
    "# stock_days = stock_price.index.day_name() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stock_details = pd.concat([stock_price, pd.Series(stock_times.values), pd.Series(stock_days.values)],axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### INVESTINGATING STOCK PRICE MISSING DATAA @NOW-ish"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stock_details = pd.DataFrame({'times':stock_times,'days': stock_days})\n",
    "# stock_details = pd.concat([stock_price.reset_index(),stock_details],axis=1)\n",
    "# stock_details.set_index('index',inplace=True)\n",
    "# display(stock_details.head())\n",
    "# stock_details.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stock_details.loc[stock_details['stock_price'].isna()].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pandas_profiling import ProfileReport\n",
    "# ProfileReport(twitter_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FOR NOW - GOING FORWARD WITH 4000 TWEETS WITH PRICES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###   APPLYING FULL WORKFLOW WRITTEN ABOVE IN CODE ###\n",
    "## LOAD IN STOCK PRICE\n",
    "# del stock_price\n",
    "try: stock_price\n",
    "except NameError: stock_price = None\n",
    "if stock_price is  None:    \n",
    "    print('loading stock_price')\n",
    "    stock_price = load_stock_price_series()\n",
    "else:\n",
    "    print('using pre-existing stock_price')\n",
    "        \n",
    "# Make sure stock_price is loaded as minute data\n",
    "stock_price = stock_price.asfreq('T')\n",
    "stock_price.dropna(inplace=True)\n",
    "\n",
    "## LOAD TWEETS, SELECT THE PROPER DATE RANGE AND COLUMNS\n",
    "twitter_df = load_twitter_df(verbose=0)\n",
    "twitter_df = twitter_df.loc[stock_price.index[0]:stock_price.index[-1]]\n",
    "      \n",
    "      \n",
    "twitter_df = twitter_df[['date','source','content','content_raw','retweet_count','favorite_count','sentiment_scores']]\n",
    "# twitter_df.head()\n",
    "\n",
    "# Get get the business day index to account for tweets during off-hours\n",
    "twitter_df = get_B_day_time_index_shift(twitter_df,verbose=1)\n",
    "\n",
    "# Reorder Columns to desired order\n",
    "twitter_df = reorder_twitter_df_columns(twitter_df,\n",
    "                                        order=['date','dayofweek','B_dt_index','B_day','source','content',\n",
    "                                               'content_raw','retweet_count','favorite_count',\n",
    "                                               'sentiment_scores','time_shift'])\n",
    "\n",
    "\n",
    "# Make temporary B_dt_index var in order to round that column to minute-resolution\n",
    "B_dt_index = twitter_df[['B_dt_index','B_day']]#.asfreq('T')\n",
    "B_dt_index['B_dt_index']= pd.to_datetime(B_dt_index['B_dt_index'])\n",
    "B_dt_index['B_dt_index']= B_dt_index['B_dt_index'].dt.round('T')\n",
    "\n",
    "# Get stock_prices for each twitter timestamp\n",
    "twitter_df['B_dt_minutes'] = B_dt_index['B_dt_index'].copy()\n",
    "twitter_df['stock_price_results'] = twitter_df['B_dt_minutes'].apply(lambda x: match_stock_price_to_tweets(x,stock_price=stock_price))\n",
    "df_to_add = twitter_df['stock_price_results'].apply(lambda x: unpack_match_stocks(x))\n",
    "\n",
    "new_twitter_df = pd.concat([twitter_df,df_to_add], axis=1)\n",
    "\n",
    "\n",
    "twitter_df = new_twitter_df.loc[~new_twitter_df['post_tweet_price'].isna()]\n",
    "# twitter_df.drop(['0'],axis=1,inplace=True)\n",
    "twitter_df['stock_delta_class'] = np.where(twitter_df['delta_price'] > 0,'pos','neg')\n",
    "\n",
    "# display(twitter_df.head())\n",
    "print(twitter_df.isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TROUBLESHOOTING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check_df = twitter_df.loc[twitter_df['stock_price_results'].isna()]#.sum()\n",
    "# BH = pd.tseries.offsets.BusinessHour(start='09:30',end='16:30')\n",
    "# BD = pd.tseries.offsets.BusinessDay()\n",
    "# check_df.head()\n",
    "\n",
    "# check_times = check_df['B_dt_minutes']\n",
    "# ofst = pd.offsets.Minute(30)\n",
    "# np.sum([x in stock_price.index for x in check_times])\n",
    "\n",
    "# # Example\n",
    "# ct = check_times[0]\n",
    "# print(ct)\n",
    "# ct_oft = ofst(ct)\n",
    "# print(ct_oft)\n",
    "# print(ct_oft in stock_price.index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stock_price.loc['2017-01-26 11:00':'2017-01-26 12:00']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # twitter_df['stock_price_results'] has:\n",
    "#     # dict_keys(['pre_tweet_price', 'post_tweet_price', 'delta_price', 'delta_time'])\n",
    "# test_dict = twitter_df['stock_price_results'][0]\n",
    "def unpack_match_stocks(stock_dict):\n",
    "    stock_series = pd.Series(stock_dict)\n",
    "    return stock_series\n",
    "# # display(twitter_df['stock_price_results'].head())\n",
    "# new_df = twitter_df[['stock_price_results','B_dt_minutes']].copy()\n",
    "# new_df2 = twitter_df['stock_price_results'].apply(lambda x: unpack_match_stocks(x))\n",
    "# new_df = pd.concat([new_df,new_df2],axis=1)\n",
    "# display(new_df.head(10))\n",
    "# display(new_df.info())\n",
    "# # new_df.isna().sum() /len(new_df)\n",
    "# new_df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bad_times = new_df['B_dt_minutes']\n",
    "# stock_price.loc[bad_times[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # TROUBLESHOOTING\n",
    "# bad_tweets = twitter_df.loc[new_df['pre_tweet_price']=='NaN in stock_price']\n",
    "# # bad_tweets = bad_tweets[['B_day','dayofweek','B_dt_minutes','time_shift','stock_price_results']]\n",
    "# bad_tweets.drop(['source','content','content_raw','retweet_count','favorite_count','date','B_dt_index'], axis=1, inplace=True)\n",
    "# # print(stock_price.index)\n",
    "# display(bad_tweets.head(20))\n",
    "# print(bad_tweets.info())\n",
    "# print(len(bad_tweets),len(twitter_df))\n",
    "# # print(bad_tweets.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bad_TS = bad_tweets['B_dt_minutes']\n",
    "# BH = pd.tseries.offsets.BusinessHour(start='09:30',end='16:30')\n",
    "\n",
    "# TS = bad_TS[0]\n",
    "# print(TS)\n",
    "# test_ts = BH.rollforward(TS)\n",
    "# test_roll = bad_TS.apply(lambda x: BH.rollforward(x))\n",
    "# # BH, test_ts\n",
    "# test_roll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # TROUBLESHOOTING\n",
    "# bad_tweets = twitter_df.loc[twitter_df['stock_price_results'].isna()]\n",
    "# print(stock_price.index)\n",
    "# bad_tweets.head()\n",
    "# # print(bad_tweets.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## FUNCTION CODE\n",
    "# import pandas as pd\n",
    "# from datetime import datetime as dt\n",
    "# tweet_timestamp = bad_tweets.sample(1).index\n",
    "# print(tweet_timestamp)\n",
    "# # convert tweet timestamp to minute accuracy\n",
    "# ts=[]\n",
    "# ts = pd.to_datetime(tweet_timestamp).round(time_freq)\n",
    "# print('Rounded: ',ts)\n",
    "# CBH =custom_BH_freq()      \n",
    "# BH = pd.tseries.offsets.BusinessHour(start='09:30',end='04:30')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # checking if time is within stock_date_data\n",
    "# if ts not in stock_price.index:\n",
    "    \n",
    "#     ts2 = BH.rollforward(pd.to_datetime(ts))\n",
    "# #     ts2 = CBH.rollforward(ts) \n",
    "#     print(f'ts prior: {ts}\\tts post_roll: {ts2}')\n",
    "# #     if ts2 not in stock_price.index:\n",
    "# #         print('ts2 Not in index:',ts2)\n",
    "# #     else: \n",
    "# #         print('ts2 is in index',ts2)\n",
    "# #         ts=ts2\n",
    "\n",
    "# # bad_tweets['B_dt_index'][1:10].apply(lambda x: match_stock_price_to_tweets(x,stock_price=stock_price))\n",
    "# # twitter_df.loc[twitter_df['stock_price_results'].isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# twitter_df.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# twitter_df.loc'2017-01-26 11:34:00'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NOW:\n",
    "\n",
    "- Use `df_tweets['B_dt_index']` for `match_stock_price_to_tweets`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BOOKMARK- CELL BELOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ['pos' for x in twitter_df['delta_price'] if x>0 else 'neg']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# twitter_df.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tokenize=twitter_df[['date','content','content_raw','B_dt_minutes','pre_tweet_price','post_tweet_price','delta_price']].copy()\n",
    "                        \n",
    "df_tokenize.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SAVE /LOAD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_tokenize.to_csv('twitter_df_with_matched_price_data.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP PROCESSING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate Stopwords List from nltk + punctuation + custom list\n",
    "from nltk import regexp_tokenize\n",
    "pattern = \"([a-zA-Z]+(?:'[a-z]+)?)\"\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "\n",
    "stopwords_list = stopwords.words('english')\n",
    "stopwords_list += list(string.punctuation)\n",
    "stopwords_list += ['http','https','...','','``','co','','','','',\"n't\",\"''\",'u','s',\"'s\",'|','\\\\|','amp',\"i'm\"]\n",
    "stopwords_list += [0,1,2,3,4,5,6,7,8,9]\n",
    "\n",
    "## Adding in stopword removal to the actual dataframe\n",
    "def apply_stopwords(stopwords_list,  text, tokenize=True, pattern = \"([a-zA-Z]+(?:'[a-z]+)?)\"):\n",
    "\n",
    "    if tokenize==True:\n",
    "        from nltk import regexp_tokenize\n",
    "        \n",
    "        text = regexp_tokenize(text,pattern)\n",
    "        \n",
    "    stopped = [x.lower() for x in text if x.lower() not in stopwords_list]\n",
    "    return ' '.join(stopped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save 'hashtags' column containing all hastags\n",
    "import re\n",
    "df=twitter_df.copy()\n",
    "# df['content_raw'] = df['content'].copy()\n",
    "\n",
    "# Add has_RT and starts_RT columns\n",
    "# Creating columns for tweets that `has_RT` or `starts_RT`\n",
    "df['has_RT']=df['content_raw'].str.contains('RT')\n",
    "df['starts_RT']=df['content_raw'].str.contains('^RT')\n",
    "\n",
    "## FIRST REMOVE THE RT HEADERS\n",
    "\n",
    "# Remove `RT @Mentions` FIRST:\n",
    "re_RT = re.compile('RT [@]?\\w*:')\n",
    "\n",
    "raw_col =  'content_raw'\n",
    "check_content_col =raw_col\n",
    "fill_content_col = 'content'\n",
    "\n",
    "df['content_starts_RT'] = df[check_content_col].apply(lambda x: re_RT.findall(x))\n",
    "df[fill_content_col] =  df[check_content_col].apply(lambda x: re_RT.sub(' ',x))\n",
    "\n",
    "\n",
    "## SECOND REMOVE URLS\n",
    "# Remove urls with regex\n",
    "urls = re.compile(r\"(http[s]?://\\w*\\.\\w*/+\\w+)\")\n",
    "\n",
    "check_content_col = 'content'\n",
    "fill_content_col = 'content'\n",
    "\n",
    "# df_full['content_urls'] = df_full[check_content_col].apply(lambda x: urls.findall(x))\n",
    "df[fill_content_col] =  df[check_content_col].apply(lambda x: urls.sub(' ',x))\n",
    "\n",
    "## SAVE THIS MINIMALLY CLEANED CONTENT AS 'content_min_clean'\n",
    "df['content_min_clean'] =  df[fill_content_col]\n",
    "\n",
    "\n",
    "\n",
    "## REMOVE AND SAVE HASHTAGS, MENTIONS\n",
    "# Remove and save Hashtags\n",
    "hashtags = re.compile(r'\\#\\w*')\n",
    "\n",
    "check_content_col = 'content'\n",
    "fill_content_col = 'content'\n",
    "\n",
    "df['content_hashtags'] =  df[check_content_col].apply(lambda x: hashtags.findall(x))\n",
    "df[fill_content_col] =  df[check_content_col].apply(lambda x: hashtags.sub(' ',x))\n",
    "\n",
    "\n",
    "# Remove and save mentions (@)'s\n",
    "mentions = re.compile(r'\\@\\w*')\n",
    "\n",
    "check_content_col = 'content'\n",
    "fill_content_col = 'content'\n",
    "\n",
    "df['content_mentions'] =  df[check_content_col].apply(lambda x: mentions.findall(x))\n",
    "df[fill_content_col] =  df[check_content_col].apply(lambda x: mentions.sub(' ',x))\n",
    "\n",
    "\n",
    "# Creating content_stopped columns and then tokens_stopped column\n",
    "df['content_stopped'] = df['content'].apply(lambda x: apply_stopwords(stopwords_list,x))\n",
    "df['tokens_stopped'] = df['content_stopped'].apply(lambda x: regexp_tokenize(x,pattern))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP NEURAL NETWORK _ MOD 4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mod4functions_JMI as jmi\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "mpl.rcParams['figure.figsize']=(10,8)\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Keras preprocessing\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils.np_utils import to_categorical\n",
    "\n",
    "# Keras neural network basics\n",
    "from keras import models\n",
    "from keras import layers\n",
    "from keras import optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tokenize['stock_delta_class'].value_counts()/len(df_tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tokenize = df\n",
    "df_tokenize.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "' '.join(df_tokenize['content_stopped'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text_data = df_tokenize['content_min_clean']\n",
    "# text_data = df_tokenize['content']\n",
    "text_data = df_tokenize['content']\n",
    "from gensim.models import Word2Vec\n",
    "vector_size = 300\n",
    "\n",
    "wv_keras = Word2Vec(text_data, size=vector_size, window=10, min_count=1, workers=4)\n",
    "wv_keras.train(text_data,total_examples=wv_keras.corpus_count, epochs=10)\n",
    "\n",
    "wv = wv_keras.wv\n",
    "vocab_size = len(wv_keras.wv.vocab)\n",
    "print(f'There are {vocab_size} words in the word2vec vocabulary, with a vector size {vector_size}.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the vectors in a new matrix\n",
    "word_model = wv_keras\n",
    "vector_size = word_model.wv.vectors[1].shape[0]\n",
    "\n",
    "embedding_matrix = np.zeros((len(word_model.wv.vocab) + 1, vector_size))\n",
    "for i, vec in enumerate(word_model.wv.vectors):\n",
    "  embedding_matrix[i] = vec\n",
    "embedding_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get list of texts to be converted to sequences\n",
    "# sentences_train =text_data # df_tokenize['tokens'].values\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "tokenizer = Tokenizer(num_words=len(wv.vocab))\n",
    "tokenizer.fit_on_texts(list(text_data)) #tokenizer.fit_on_texts(text_data)\n",
    "\n",
    "word_index = tokenizer.index_word\n",
    "reverse_index = {v:k for k,v in word_index.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_val_split(X,y,test_size=0.20,val_size=0.1):\n",
    "    \"\"\"Performs 2 successive train_test_splits to produce a training, testing, and validation dataset\"\"\"\n",
    "    from sklearn.model_selection import train_test_split\n",
    "\n",
    "    if val_size==0:\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size)\n",
    "        return X_train, X_test, y_train, y_test\n",
    "    else:\n",
    "\n",
    "        first_split_size = test_size + val_size\n",
    "        second_split_size = val_size/(test_size + val_size)\n",
    "\n",
    "        X_train, X_test_val, y_train, y_test_val = train_test_split(X, y, test_size=first_split_size)\n",
    "\n",
    "        X_test, X_val, y_test, y_val = train_test_split(X_test_val, y_test_val, test_size=second_split_size)\n",
    "\n",
    "        return X_train, X_test, X_val, y_train, y_test, y_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tokenize.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining loss function to use\n",
    "def my_rmse(y_true,y_pred):\n",
    "    \"\"\"RMSE calculation using keras.backend\"\"\"\n",
    "    from keras import backend as kb\n",
    "    sq_err = kb.square(y_pred - y_true)\n",
    "    mse = kb.mean(sq_err,axis=-1)\n",
    "    rmse =kb.sqrt(mse)\n",
    "    return rmse\n",
    "\n",
    "# CREATING CALLBACKS\n",
    "from keras import callbacks\n",
    "filepath = 'twitterBLP_model1_weights.ep{epoch:02d}-acc{acc:.2f}.hdf5'\n",
    "checkpoint = callbacks.ModelCheckpoint(filepath=filepath, monitor='acc',mode='min',\n",
    "                                       save_best_only=True, verbose=1)\n",
    "early_stop = callbacks.EarlyStopping(monitor='acc',mode='min',patience=1,min_delta=.001,verbose=1)\n",
    "callbacks = [checkpoint,early_stop]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# return integer-encoded sentences\n",
    "from keras.preprocessing import text, sequence\n",
    "X = tokenizer.texts_to_sequences(text_data)\n",
    "X = sequence.pad_sequences(X)\n",
    "\n",
    "y = [1 if x=='pos' else 0  for x in df_tokenize['stock_delta_class']]\n",
    "# y\n",
    "     # y = df_tokenize['stock_delta_class'].values\n",
    "# reverse_index\n",
    "X_train, X_test, y_train, y_test = train_test_val_split(X, y, test_size=0.1, val_size=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import models, layers, optimizers, regularizers\n",
    "vocab_size = len(wv_keras.wv.vocab)\n",
    "model2 = models.Sequential()\n",
    "# embedding_layer = wv.get_keras_embedding(train_embeddings=False)\n",
    "\n",
    "model2.add(layers.Embedding(vocab_size+1,\n",
    "                             vector_size,input_length=X_train.shape[1],\n",
    "                             weights=[embedding_matrix],trainable=False)) \n",
    "          \n",
    "model2.add(layers.LSTM(units=200, return_sequences=False, kernel_regularizer=regularizers.l2(.01)))\n",
    "# model1B.add(layers.GlobalMaxPooling1D())\n",
    "model2.add(layers.Dropout(0.2))\n",
    "model2.add(layers.Dense(units=10, activation='relu'))\n",
    "model2.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "model2.compile(loss='binary_crossentropy',optimizer=\"adam\",metrics=['acc'])#,'val_acc'])#, callbacks=callbacks)\n",
    "model2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clock = bs.Clock()\n",
    "clock.tic('starting keras .fit')\n",
    "num_epochs = 1\n",
    "history = model2.fit(X_train, y_train, epochs=num_epochs, verbose=True, validation_split=0.2,\n",
    "                     callbacks=callbacks,batch_size=300)#, validation_data=(X_val))\n",
    "\n",
    "clock.toc(f'completed {num_epochs} epochs')\n",
    "\n",
    "loss, accuracy = model2.evaluate(X_train, y_train, verbose=True)\n",
    "print(f'Training Accuracy:{accuracy}')\n",
    "\n",
    "loss, accuracy = model2.evaluate(X_test, y_test, verbose=True)\n",
    "print(f'Testing Accuracy:{accuracy}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jmi.plot_keras_history(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat_test = model2.predict_classes(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "conf_mat = confusion_matrix(y_test, y_hat_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bs_ds as bs\n",
    "bs.plot_confusion_matrix(conf_mat,classes=['Stock Increase','Stock Decrease'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score, roc_curve, classification_report\n",
    "\n",
    "print(classification_report(y_test,y_hat_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_json = model2.to_json()\n",
    "with open(\"NLPmodel.json\",\"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "\n",
    "model2.save_weights('NLPmodel.h5')\n",
    "print(\"saved model to disk.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merging Dataframes"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "learn-env-ext",
   "language": "python",
   "name": "learn-env-ext"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "307.2px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": "25"
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
