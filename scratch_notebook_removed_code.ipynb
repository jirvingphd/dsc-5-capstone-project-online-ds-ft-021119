{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TEST CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from bs_ds.imports import *\n",
    "# from bs_ds.glassboxes import *\n",
    "# from test_imports import module_menu\n",
    "\n",
    "# module_menu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from test_imports import autoFill #as af\n",
    "\n",
    "# from ipywidgets import HTML, HBox\n",
    "\n",
    "# def open(value):\n",
    "#     show.value=value.new\n",
    "# options = [x for x in dir() if '__' not in x]    \n",
    "# # options = ['Football', 'Basketball', 'Voleyball', 'Basketcase', 'Showcase', 'Footer', 'Header', 'Viewer', 'Footage', 'Showtime', 'Show Business']\n",
    "\n",
    "# autofill = autoFill(options,callback=open)\n",
    "\n",
    "# show = HTML('Result will be displayed here!')\n",
    "\n",
    "# display(HBox([autofill,show]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import ipywidgets as widgets\n",
    "# from IPython.display import display\n",
    "\n",
    "# geo={'USA':['CHI','NYC'],'Russia':['MOW','LED']}\n",
    "# geoWs = {key: widgets.Select(options=geo[key]) for key in geo}\n",
    "\n",
    "# def get_current_state():\n",
    "#     return {'country': i.children[0].value,\n",
    "#             'city': i.children[1].value}\n",
    "\n",
    "# def print_city(**func_kwargs):\n",
    "#     print('func_kwargs', func_kwargs)\n",
    "#     print('i.kwargs', i.kwargs)\n",
    "#     print('get_current_state', get_current_state())\n",
    "\n",
    "# def select_country(country):\n",
    "#     new_i = widgets.interactive(print_city, country=countryW, city=geoWs[country['new']])\n",
    "#     i.children = new_i.children\n",
    "\n",
    "# countryW = widgets.Select(options=list(geo.keys()))\n",
    "# init = countryW.value\n",
    "# cityW = geoWs[init]\n",
    "\n",
    "# countryW.observe(select_country, 'value')\n",
    "\n",
    "# i = widgets.interactive(print_city, country=countryW, city=cityW)\n",
    "\n",
    "# display(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import test_imports as ti\n",
    "# from test_imports import *\n",
    "\n",
    "#import_packages\n",
    "# additional_packages =  [('keras.models','models','Keras models'),\n",
    "#                     ('keras.layers','layers','Keras Layers')]\n",
    "# ti.import_packages(import_list_of_tuples=additional_packages)\n",
    "# ti.import_packages()\n",
    "# print(dir())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model saving/loading/plotting\n",
    "https://machinelearningmastery.com/save-load-keras-deep-learning-models/\n",
    "\n",
    "```python\n",
    "# serialize model to JSON\n",
    "model_json = model.to_json()\n",
    "with open(\"model.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "# serialize weights to HDF5\n",
    "model.save_weights(\"model.h5\")\n",
    "print(\"Saved model to disk\")\n",
    " \n",
    "# later...\n",
    " \n",
    "# load json and create model\n",
    "json_file = open('model.json', 'r')\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "loaded_model = model_from_json(loaded_model_json)\n",
    "# load weights into new model\n",
    "loaded_model.load_weights(\"model.h5\")\n",
    "print(\"Loaded model from disk\")\n",
    " \n",
    "# evaluate loaded model on test data\n",
    "loaded_model.compile(loss='binary_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
    "score = loaded_model.evaluate(X, Y, verbose=0)\n",
    "print(\"%s: %.2f%%\" % (loaded_model.metrics_names[1], score[1]*100))\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\james\\Anaconda3\\envs\\learn-env-ext\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From C:\\Users\\james\\Anaconda3\\envs\\learn-env-ext\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 2169, 300)         112200    \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 200)               400800    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 10)                2010      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 11        \n",
      "=================================================================\n",
      "Total params: 515,021\n",
      "Trainable params: 402,821\n",
      "Non-trainable params: 112,200\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.models import model_from_json\n",
    "json_file = open('NLPmodel.json')\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "\n",
    "loaded_model = model_from_json(loaded_model_json )\n",
    "\n",
    "loaded_model.load_weights('NLPmodel.h5')\n",
    "\n",
    "loaded_model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['acc'])\n",
    "loaded_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Variable 'lstm_1/kernel:0' shape=(300, 800) dtype=float32_ref>,\n",
       " <tf.Variable 'lstm_1/recurrent_kernel:0' shape=(200, 800) dtype=float32_ref>,\n",
       " <tf.Variable 'lstm_1/bias:0' shape=(800,) dtype=float32_ref>,\n",
       " <tf.Variable 'dense_1/kernel:0' shape=(200, 10) dtype=float32_ref>,\n",
       " <tf.Variable 'dense_1/bias:0' shape=(10,) dtype=float32_ref>,\n",
       " <tf.Variable 'dense_2/kernel:0' shape=(10, 1) dtype=float32_ref>,\n",
       " <tf.Variable 'dense_2/bias:0' shape=(1,) dtype=float32_ref>,\n",
       " <tf.Variable 'embedding_1/embeddings:0' shape=(374, 300) dtype=float32_ref>]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loaded_model.weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import plot_model\n",
    "plot_model(loaded_model,show_shapes=True, rankdir='TB',to_file='nlp_model.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from IPython.display import SVG\n",
    "# from keras.utils import model_to_dot\n",
    "\n",
    "# SVG(model_to_dot(loaded_model).create(prog='dot', format='svg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## NEW EVAL METHOD USING PREDICTIONS AS PART OF SLIDING WINDOW\n",
    "    # # GETTING THE FIRST EVAL PATCH AND RESHAPING TO MATCH INPUT\n",
    "    # first_eval_batch = train_data[-n_input:]\n",
    "    # current_batch = first_eval_batch.reshape((1, n_input, n_features))\n",
    "    # test_predictions=[]\n",
    "    # # LOOP THROUGH ALL TEST DATA\n",
    "    # for i in range(len(test_data)):\n",
    "\n",
    "    #     # get prediction 1 time stamp ahead ([0] is for grabbing just the number instead of [array])\n",
    "    #     current_pred = model.predict(current_batch)[0] \n",
    "\n",
    "    #     # store prediction\n",
    "    #     test_predictions.append(current_pred) \n",
    "\n",
    "    #     # UPDATE BATCH TO INCLUDE CURRENT PREDICTION AND DROP OLDEST VALUE FROM ARRAY\n",
    "    #     current_batch = np.append(current_batch[:,1:,:],[[current_pred]],axis=1) # axis===ndim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CODE THAT IS BEING REPLACE BY TIMESERIESGENERATOR\n",
    "```python\n",
    "# BIN STOCK_DF BY INTO X_WINDOW # TIME SEQUENCES\n",
    "# Create binned columns with x_window # of sequential values, label column with next value in sequence\n",
    "stock_df_binned = make_df_timeseries_bins_by_column(stock_df, x_window=x_window, verbose=0)#.iloc[:,11:])\n",
    "# display(stock_df_binned.head(2).style.set_caption('Binned for Neural Network'))\n",
    "\n",
    "\n",
    "# Split the training and test data by number of days\n",
    "df_train_bins, df_test_bins = train_test_split_by_last_days(stock_df_binned,periods_per_day=periods_per_day,\n",
    "                                                            num_test_days=num_test_days,num_train_days=num_train_days,\n",
    "                                                            plot=True)\n",
    "```\n",
    "\n",
    "\n",
    "- And then no need to create X_train, y_train, etc\n",
    "\n",
    "```python\n",
    "# Creating X,y and index for training data and test data\n",
    "train_index =df_train_bins.index\n",
    "test_index = df_test_bins.index\n",
    "\n",
    "X_train = df_train_bins['price_bins'].values\n",
    "y_train = df_train_bins['price_labels'].values\n",
    "\n",
    "X_test = df_test_bins['price_bins'].values\n",
    "y_test = df_test_bins['price_labels'].values\n",
    "\n",
    "\n",
    "# Converting all X and y into 2D arrays\n",
    "X_train_stack =  np.vstack(X_train)\n",
    "X_train_in = np.reshape(X_train_stack,(X_train_stack.shape[0],X_train_stack.shape[1],1))\n",
    "\n",
    "X_test_stack =  np.vstack(X_test)\n",
    "X_test_in = np.reshape(X_test_stack,(X_test_stack.shape[0],X_test_stack.shape[1],1))\n",
    "\n",
    "\n",
    "# Set True to display array details\n",
    "show_array_details = False\n",
    "\n",
    "# Display details of array data, if desired\n",
    "if show_array_details==True:\n",
    "    var_dict = {'X_train':X_train_in, 'y_train':y_train,'X_test':X_test_in,'y_test':y_test}\n",
    "    for name, arr in var_dict.items():\n",
    "        print_array_inf(arr,name)\n",
    "```\n",
    "\n",
    "- Then model.fit replaced with fit_generator\n",
    "\n",
    "```python\n",
    "history = model.fit(X_train_in, y_train, epochs=3, verbose=2, validation_split=(0.1))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### USING ROLLBACK - PANDAS GUIDE\n",
    "- from the https://pandas.pydata.org/pandas-docs/stable/user_guide/timeseries.html#dateoffset-objects\n",
    "\n",
    "```python\n",
    "In [148]: ts = pd.Timestamp('2018-01-06 00:00:00')\n",
    "\n",
    "In [149]: ts.day_name()\n",
    "Out[149]: 'Saturday'\n",
    "\n",
    "# BusinessHour's valid offset dates are Monday through Friday\n",
    "In [150]: offset = pd.offsets.BusinessHour(start='09:00')\n",
    "\n",
    "# Bring the date to the closest offset date (Monday)\n",
    "In [151]: offset.rollforward(ts)\n",
    "Out[151]: Timestamp('2018-01-08 09:00:00')\n",
    "\n",
    "# Date is brought to the closest offset date first and then the hour is added\n",
    "In [152]: ts + offset\n",
    "Out[152]: Timestamp('2018-01-08 10:00:00')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### add AIC and BIC\n",
    "\n",
    "#### NOTES ON Evaluating Forecast Accuracy\n",
    "- **Mean Absolute Error, Mean Squared Error, Root Mean Squared Error**\n",
    "    - A forecast method that minimizes the **MAE** will lead to **forecasts of the median*\n",
    "    - while minimizing the **RMSE will lead to forecasts of the mean.**\n",
    "    \n",
    "- **AIC / BIC**\n",
    "    - Akaike Information Criterion (AIC)\n",
    "    - Bayesian Information Criterion (BIC)\n",
    "    \n",
    "The AIC evaluates a collection of models and estimates the quality of each model relative to the others. Penalties are provided for the number of parameters used in an effort to thwart overfitting. The lower the AIC and BIC, the better the model should be at forecasting.\n",
    "\n",
    "These functions are available as\n",
    "\n",
    "    from from statsmodels.tools.eval_measures import aic, bic\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## METHOD OF CONSTRUCTING DATA FROM GENERATOR / TESTING THIEL'S U"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# REVERTING BACK TO ORIGINAL METHOD NOW THAT I UNDERSTAND I NEED TO SHIFT TRUE VALUES -1\n",
    "# plot_preds_with_thiels_u(mode, test_generator,)\n",
    "# GET PREDICTIONS FROM TEST_GENERATOR\n",
    "preds_from_gen = pd.Series(model.predict_generator(test_generator).ravel(), \n",
    "                             index=test_data_index[n_input:], name='preds_from_gen')\n",
    "\n",
    "# EXTRACT TARGET DATA (TRUE VALUES USED))\n",
    "true_data = [ i[1][0].tolist()[0] for i in test_generator]\n",
    "true_from_gen = pd.Series(data=true_data,index=test_data_index[n_input:], name='true_from_gen') # get the target values\n",
    "df_U = pd.concat([true_from_gen,preds_from_gen],axis=1)\n",
    "\n",
    "display(df_U.head())\n",
    "df_U.plot()\n",
    "plt.legend()\n",
    "\n",
    "U =thiels_U(df_U['true_from_gen'].values,df_U['preds_from_gen'].values)\n",
    "plt.title('True Price vs Predicted Price')\n",
    "print(\"Thiel's U = \",U)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## NEW EVAL METHOD USING PREDICTIONS AS PART OF SLIDING WINDOW\n",
    "    # # GETTING THE FIRST EVAL PATCH AND RESHAPING TO MATCH INPUT\n",
    "    # first_eval_batch = train_data[-n_input:]\n",
    "    # current_batch = first_eval_batch.reshape((1, n_input, n_features))\n",
    "    # test_predictions=[]\n",
    "    # # LOOP THROUGH ALL TEST DATA\n",
    "    # for i in range(len(test_data)):\n",
    "\n",
    "    #     # get prediction 1 time stamp ahead ([0] is for grabbing just the number instead of [array])\n",
    "    #     current_pred = model.predict(current_batch)[0] \n",
    "\n",
    "    #     # store prediction\n",
    "    #     test_predictions.append(current_pred) \n",
    "\n",
    "    #     # UPDATE BATCH TO INCLUDE CURRENT PREDICTION AND DROP OLDEST VALUE FROM ARRAY\n",
    "    #     current_batch = np.append(current_batch[:,1:,:],[[current_pred]],axis=1) # axis===ndim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "learn-env-ext",
   "language": "python",
   "name": "learn-env-ext"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
