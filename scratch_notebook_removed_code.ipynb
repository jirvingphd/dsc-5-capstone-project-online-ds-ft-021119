{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cufflinks as cf\n",
    "from ipywidgets import interact\n",
    "\n",
    "@interact\n",
    "def scatter_plot(x=list(df.select_dtypes('number').columns), \n",
    "                 y=list(df.select_dtypes('number').columns)[1:],\n",
    "                 theme=list(cf.themes.THEMES.keys()), \n",
    "                 colorscale=list(cf.colors._scales_names.keys())):\n",
    "    \n",
    "    df.iplot(kind='scatter', x=x, y=y, mode='markers', \n",
    "             xTitle=x.title(), yTitle=y.title(), \n",
    "             text='title',\n",
    "             title=f'{y.title()} vs {x.title()}',\n",
    "            theme=theme, colorscale=colorscale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "58d7f8acf2c94d22ada9b7995708aaa3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Box(children=(HTML(value='Not hooked'),), layout=Layout(overflow_y='scroll'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "### https://github.com/jupyter-widgets/ipywidgets/blob/master/docs/source/examples/Variable%20Inspector.ipynb\n",
    "import ipywidgets as widgets # Loads the Widget framework.\n",
    "from IPython.core.magics.namespace import NamespaceMagics # Used to query namespace.\n",
    "\n",
    "# For this example, hide these names, just to avoid polluting the namespace further\n",
    "get_ipython().user_ns_hidden['widgets'] = widgets\n",
    "get_ipython().user_ns_hidden['NamespaceMagics'] = NamespaceMagics\n",
    "class VariableInspectorWindow(object):\n",
    "    instance = None\n",
    "    \n",
    "    def __init__(self, ipython):\n",
    "        \"\"\"Public constructor.\"\"\"\n",
    "        if VariableInspectorWindow.instance is not None:\n",
    "            raise Exception(\"\"\"Only one instance of the Variable Inspector can exist at a \n",
    "                time.  Call close() on the active instance before creating a new instance.\n",
    "                If you have lost the handle to the active instance, you can re-obtain it\n",
    "                via `VariableInspectorWindow.instance`.\"\"\")\n",
    "        \n",
    "        VariableInspectorWindow.instance = self\n",
    "        self.closed = False\n",
    "        self.namespace = NamespaceMagics()\n",
    "        self.namespace.shell = ipython.kernel.shell\n",
    "        \n",
    "        self._box = widgets.Box()\n",
    "        self._box.layout.overflow_y = 'scroll'\n",
    "        self._table = widgets.HTML(value = 'Not hooked')\n",
    "        self._box.children = [self._table]\n",
    "        \n",
    "        self._ipython = ipython\n",
    "        self._ipython.events.register('post_run_cell', self._fill)\n",
    "        \n",
    "    def close(self):\n",
    "        \"\"\"Close and remove hooks.\"\"\"\n",
    "        if not self.closed:\n",
    "            self._ipython.events.unregister('post_run_cell', self._fill)\n",
    "            self._box.close()\n",
    "            self.closed = True\n",
    "            VariableInspectorWindow.instance = None\n",
    "\n",
    "    def _fill(self):\n",
    "        \"\"\"Fill self with variable information.\"\"\"\n",
    "        values = self.namespace.who_ls()\n",
    "        self._table.value = '<div class=\"rendered_html jp-RenderedHTMLCommon\"><table><thead><tr><th>Name</th><th>Type</th><th>Value</th></tr></thead><tr><td>' + \\\n",
    "            '</td></tr><tr><td>'.join(['{0}</td><td>{1}</td><td>{2}'.format(v, type(eval(v)).__name__, str(eval(v))) for v in values]) + \\\n",
    "            '</td></tr></table></div>'\n",
    "\n",
    "    def _ipython_display_(self):\n",
    "        \"\"\"Called when display() or pyout is used to display the Variable \n",
    "        Inspector.\"\"\"\n",
    "        self._box._ipython_display_()\n",
    "inspector = VariableInspectorWindow(get_ipython())\n",
    "inspector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://towardsdatascience.com/interactive-controls-for-jupyter-notebooks-f5c94829aee6\n",
    "import cufflinks as cf\n",
    "\n",
    "@interact\n",
    "def scatter_plot(x=list(df.select_dtypes('number').columns), \n",
    "                 y=list(df.select_dtypes('number').columns)[1:],\n",
    "                 theme=list(cf.themes.THEMES.keys()), \n",
    "                 colorscale=list(cf.colors._scales_names.keys())):\n",
    "    \n",
    "    df.iplot(kind='scatter', x=x, y=y, mode='markers', \n",
    "             xTitle=x.title(), yTitle=y.title(), \n",
    "             text='title',\n",
    "             title=f'{y.title()} vs {x.title()}',\n",
    "            theme=theme, colorscale=colorscale)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TEST CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## FIGURING OUT HOW TO DISPLAY EQN AND TABLE SIDE BY SIDE\n",
    "res_html = res_df.to_html()\n",
    "from pprint import pprint\n",
    "eqn=\" $$U = \\\\sqrt{\\\\frac{ \\\\sum_{t=1 }^{n-1}\\\\left(\\\\frac{\\\\bar{Y}_{t+1} - Y_{t+1}}{Y_t}\\\\right)^2}{\\\\sum_{t=1 }^{n-1}\\\\left(\\\\frac{Y_{t+1} - Y_{t}}{Y_t}\\\\right)^2}}$$\"\n",
    "\n",
    "# url=\"['Explanation'](https://docs.oracle.com/cd/E57185_01/CBREG/ch06s02s03s04.html)\"\n",
    "markdown_explanation =\"|Thiel's U Value | Interpretation |\\n\\\n",
    "| --- | --- |\\n\\\n",
    "| <1 | Forecasting is better than guessing| \\n\\\n",
    "| 1 | Forecasting is about as good as guessing| \\n\\\n",
    "|>1 | Forecasting is worse than guessing| \\n\"\n",
    "\n",
    "\n",
    "pprint(res_html)\n",
    "html_thiels = f\"<body>\\n<p>{eqn}</p>\n",
    "\n",
    "</body>\n",
    "</html>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from bs_ds.imports import *\n",
    "# from bs_ds.glassboxes import *\n",
    "# from test_imports import module_menu\n",
    "\n",
    "# module_menu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from test_imports import autoFill #as af\n",
    "\n",
    "# from ipywidgets import HTML, HBox\n",
    "\n",
    "# def open(value):\n",
    "#     show.value=value.new\n",
    "# options = [x for x in dir() if '__' not in x]    \n",
    "# # options = ['Football', 'Basketball', 'Voleyball', 'Basketcase', 'Showcase', 'Footer', 'Header', 'Viewer', 'Footage', 'Showtime', 'Show Business']\n",
    "\n",
    "# autofill = autoFill(options,callback=open)\n",
    "\n",
    "# show = HTML('Result will be displayed here!')\n",
    "\n",
    "# display(HBox([autofill,show]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import ipywidgets as widgets\n",
    "# from IPython.display import display\n",
    "\n",
    "# geo={'USA':['CHI','NYC'],'Russia':['MOW','LED']}\n",
    "# geoWs = {key: widgets.Select(options=geo[key]) for key in geo}\n",
    "\n",
    "# def get_current_state():\n",
    "#     return {'country': i.children[0].value,\n",
    "#             'city': i.children[1].value}\n",
    "\n",
    "# def print_city(**func_kwargs):\n",
    "#     print('func_kwargs', func_kwargs)\n",
    "#     print('i.kwargs', i.kwargs)\n",
    "#     print('get_current_state', get_current_state())\n",
    "\n",
    "# def select_country(country):\n",
    "#     new_i = widgets.interactive(print_city, country=countryW, city=geoWs[country['new']])\n",
    "#     i.children = new_i.children\n",
    "\n",
    "# countryW = widgets.Select(options=list(geo.keys()))\n",
    "# init = countryW.value\n",
    "# cityW = geoWs[init]\n",
    "\n",
    "# countryW.observe(select_country, 'value')\n",
    "\n",
    "# i = widgets.interactive(print_city, country=countryW, city=cityW)\n",
    "\n",
    "# display(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import test_imports as ti\n",
    "# from test_imports import *\n",
    "\n",
    "#import_packages\n",
    "# additional_packages =  [('keras.models','models','Keras models'),\n",
    "#                     ('keras.layers','layers','Keras Layers')]\n",
    "# ti.import_packages(import_list_of_tuples=additional_packages)\n",
    "# ti.import_packages()\n",
    "# print(dir())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model saving/loading/plotting\n",
    "https://machinelearningmastery.com/save-load-keras-deep-learning-models/\n",
    "\n",
    "```python\n",
    "# serialize model to JSON\n",
    "model_json = model.to_json()\n",
    "with open(\"model.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "# serialize weights to HDF5\n",
    "model.save_weights(\"model.h5\")\n",
    "print(\"Saved model to disk\")\n",
    " \n",
    "# later...\n",
    " \n",
    "# load json and create model\n",
    "json_file = open('model.json', 'r')\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "loaded_model = model_from_json(loaded_model_json)\n",
    "# load weights into new model\n",
    "loaded_model.load_weights(\"model.h5\")\n",
    "print(\"Loaded model from disk\")\n",
    " \n",
    "# evaluate loaded model on test data\n",
    "loaded_model.compile(loss='binary_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
    "score = loaded_model.evaluate(X, Y, verbose=0)\n",
    "print(\"%s: %.2f%%\" % (loaded_model.metrics_names[1], score[1]*100))\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\james\\Anaconda3\\envs\\learn-env-ext\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From C:\\Users\\james\\Anaconda3\\envs\\learn-env-ext\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 2169, 300)         112200    \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 200)               400800    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 10)                2010      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 11        \n",
      "=================================================================\n",
      "Total params: 515,021\n",
      "Trainable params: 402,821\n",
      "Non-trainable params: 112,200\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.models import model_from_json\n",
    "json_file = open('NLPmodel.json')\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "\n",
    "loaded_model = model_from_json(loaded_model_json )\n",
    "\n",
    "loaded_model.load_weights('NLPmodel.h5')\n",
    "\n",
    "loaded_model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['acc'])\n",
    "loaded_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Variable 'lstm_1/kernel:0' shape=(300, 800) dtype=float32_ref>,\n",
       " <tf.Variable 'lstm_1/recurrent_kernel:0' shape=(200, 800) dtype=float32_ref>,\n",
       " <tf.Variable 'lstm_1/bias:0' shape=(800,) dtype=float32_ref>,\n",
       " <tf.Variable 'dense_1/kernel:0' shape=(200, 10) dtype=float32_ref>,\n",
       " <tf.Variable 'dense_1/bias:0' shape=(10,) dtype=float32_ref>,\n",
       " <tf.Variable 'dense_2/kernel:0' shape=(10, 1) dtype=float32_ref>,\n",
       " <tf.Variable 'dense_2/bias:0' shape=(1,) dtype=float32_ref>,\n",
       " <tf.Variable 'embedding_1/embeddings:0' shape=(374, 300) dtype=float32_ref>]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loaded_model.weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import plot_model\n",
    "plot_model(loaded_model,show_shapes=True, rankdir='TB',to_file='nlp_model.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from IPython.display import SVG\n",
    "# from keras.utils import model_to_dot\n",
    "\n",
    "# SVG(model_to_dot(loaded_model).create(prog='dot', format='svg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## NEW EVAL METHOD USING PREDICTIONS AS PART OF SLIDING WINDOW\n",
    "    # # GETTING THE FIRST EVAL PATCH AND RESHAPING TO MATCH INPUT\n",
    "    # first_eval_batch = train_data[-n_input:]\n",
    "    # current_batch = first_eval_batch.reshape((1, n_input, n_features))\n",
    "    # test_predictions=[]\n",
    "    # # LOOP THROUGH ALL TEST DATA\n",
    "    # for i in range(len(test_data)):\n",
    "\n",
    "    #     # get prediction 1 time stamp ahead ([0] is for grabbing just the number instead of [array])\n",
    "    #     current_pred = model.predict(current_batch)[0] \n",
    "\n",
    "    #     # store prediction\n",
    "    #     test_predictions.append(current_pred) \n",
    "\n",
    "    #     # UPDATE BATCH TO INCLUDE CURRENT PREDICTION AND DROP OLDEST VALUE FROM ARRAY\n",
    "    #     current_batch = np.append(current_batch[:,1:,:],[[current_pred]],axis=1) # axis===ndim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CODE THAT IS BEING REPLACE BY TIMESERIESGENERATOR\n",
    "```python\n",
    "# BIN STOCK_DF BY INTO X_WINDOW # TIME SEQUENCES\n",
    "# Create binned columns with x_window # of sequential values, label column with next value in sequence\n",
    "stock_df_binned = make_df_timeseries_bins_by_column(stock_df, x_window=x_window, verbose=0)#.iloc[:,11:])\n",
    "# display(stock_df_binned.head(2).style.set_caption('Binned for Neural Network'))\n",
    "\n",
    "\n",
    "# Split the training and test data by number of days\n",
    "df_train_bins, df_test_bins = train_test_split_by_last_days(stock_df_binned,periods_per_day=periods_per_day,\n",
    "                                                            num_test_days=num_test_days,num_train_days=num_train_days,\n",
    "                                                            plot=True)\n",
    "```\n",
    "\n",
    "\n",
    "- And then no need to create X_train, y_train, etc\n",
    "\n",
    "```python\n",
    "# Creating X,y and index for training data and test data\n",
    "train_index =df_train_bins.index\n",
    "test_index = df_test_bins.index\n",
    "\n",
    "X_train = df_train_bins['price_bins'].values\n",
    "y_train = df_train_bins['price_labels'].values\n",
    "\n",
    "X_test = df_test_bins['price_bins'].values\n",
    "y_test = df_test_bins['price_labels'].values\n",
    "\n",
    "\n",
    "# Converting all X and y into 2D arrays\n",
    "X_train_stack =  np.vstack(X_train)\n",
    "X_train_in = np.reshape(X_train_stack,(X_train_stack.shape[0],X_train_stack.shape[1],1))\n",
    "\n",
    "X_test_stack =  np.vstack(X_test)\n",
    "X_test_in = np.reshape(X_test_stack,(X_test_stack.shape[0],X_test_stack.shape[1],1))\n",
    "\n",
    "\n",
    "# Set True to display array details\n",
    "show_array_details = False\n",
    "\n",
    "# Display details of array data, if desired\n",
    "if show_array_details==True:\n",
    "    var_dict = {'X_train':X_train_in, 'y_train':y_train,'X_test':X_test_in,'y_test':y_test}\n",
    "    for name, arr in var_dict.items():\n",
    "        print_array_inf(arr,name)\n",
    "```\n",
    "\n",
    "- Then model.fit replaced with fit_generator\n",
    "\n",
    "```python\n",
    "history = model.fit(X_train_in, y_train, epochs=3, verbose=2, validation_split=(0.1))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### USING ROLLBACK - PANDAS GUIDE\n",
    "- from the https://pandas.pydata.org/pandas-docs/stable/user_guide/timeseries.html#dateoffset-objects\n",
    "\n",
    "```python\n",
    "In [148]: ts = pd.Timestamp('2018-01-06 00:00:00')\n",
    "\n",
    "In [149]: ts.day_name()\n",
    "Out[149]: 'Saturday'\n",
    "\n",
    "# BusinessHour's valid offset dates are Monday through Friday\n",
    "In [150]: offset = pd.offsets.BusinessHour(start='09:00')\n",
    "\n",
    "# Bring the date to the closest offset date (Monday)\n",
    "In [151]: offset.rollforward(ts)\n",
    "Out[151]: Timestamp('2018-01-08 09:00:00')\n",
    "\n",
    "# Date is brought to the closest offset date first and then the hour is added\n",
    "In [152]: ts + offset\n",
    "Out[152]: Timestamp('2018-01-08 10:00:00')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### add AIC and BIC\n",
    "\n",
    "#### NOTES ON Evaluating Forecast Accuracy\n",
    "- **Mean Absolute Error, Mean Squared Error, Root Mean Squared Error**\n",
    "    - A forecast method that minimizes the **MAE** will lead to **forecasts of the median*\n",
    "    - while minimizing the **RMSE will lead to forecasts of the mean.**\n",
    "    \n",
    "- **AIC / BIC**\n",
    "    - Akaike Information Criterion (AIC)\n",
    "    - Bayesian Information Criterion (BIC)\n",
    "    \n",
    "The AIC evaluates a collection of models and estimates the quality of each model relative to the others. Penalties are provided for the number of parameters used in an effort to thwart overfitting. The lower the AIC and BIC, the better the model should be at forecasting.\n",
    "\n",
    "These functions are available as\n",
    "\n",
    "    from from statsmodels.tools.eval_measures import aic, bic\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## METHOD OF CONSTRUCTING DATA FROM GENERATOR / TESTING THIEL'S U"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# REVERTING BACK TO ORIGINAL METHOD NOW THAT I UNDERSTAND I NEED TO SHIFT TRUE VALUES -1\n",
    "# plot_preds_with_thiels_u(mode, test_generator,)\n",
    "# GET PREDICTIONS FROM TEST_GENERATOR\n",
    "preds_from_gen = pd.Series(model.predict_generator(test_generator).ravel(), \n",
    "                             index=test_data_index[n_input:], name='preds_from_gen')\n",
    "\n",
    "# EXTRACT TARGET DATA (TRUE VALUES USED))\n",
    "true_data = [ i[1][0].tolist()[0] for i in test_generator]\n",
    "true_from_gen = pd.Series(data=true_data,index=test_data_index[n_input:], name='true_from_gen') # get the target values\n",
    "df_U = pd.concat([true_from_gen,preds_from_gen],axis=1)\n",
    "\n",
    "display(df_U.head())\n",
    "df_U.plot()\n",
    "plt.legend()\n",
    "\n",
    "U =thiels_U(df_U['true_from_gen'].values,df_U['preds_from_gen'].values)\n",
    "plt.title('True Price vs Predicted Price')\n",
    "print(\"Thiel's U = \",U)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## NEW EVAL METHOD USING PREDICTIONS AS PART OF SLIDING WINDOW\n",
    "    # # GETTING THE FIRST EVAL PATCH AND RESHAPING TO MATCH INPUT\n",
    "    # first_eval_batch = train_data[-n_input:]\n",
    "    # current_batch = first_eval_batch.reshape((1, n_input, n_features))\n",
    "    # test_predictions=[]\n",
    "    # # LOOP THROUGH ALL TEST DATA\n",
    "    # for i in range(len(test_data)):\n",
    "\n",
    "    #     # get prediction 1 time stamp ahead ([0] is for grabbing just the number instead of [array])\n",
    "    #     current_pred = model.predict(current_batch)[0] \n",
    "\n",
    "    #     # store prediction\n",
    "    #     test_predictions.append(current_pred) \n",
    "\n",
    "    #     # UPDATE BATCH TO INCLUDE CURRENT PREDICTION AND DROP OLDEST VALUE FROM ARRAY\n",
    "    #     current_batch = np.append(current_batch[:,1:,:],[[current_pred]],axis=1) # axis===ndim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NOTES ON SHAPE OF EVAL BATCH\n",
    "- Need to match the 3 dimension shape of the input from the generator\n",
    "\n",
    "```python\n",
    "first_eval_batch = scaled_train[-n_input:] # ? he had 12, and 12=his n_input\n",
    "current_batch = first_eval_batch.reshape((1, n_input, n_features))\n",
    "```\n",
    "\n",
    "###  NOTES ON NUMBER FORMATTING\n",
    "- https://pbpython.com/styling-pandas.html\n",
    "- Reference table for formatting: https://mkaz.blog/code/python-string-format-cookbook/\n",
    "\n",
    "\n",
    "```python\n",
    "fmt_price = '${0:,.0f}'\n",
    "format_dict = {'Predictions': '{:0.2f}',  'Test_values': '{:0.2f}', \n",
    "'True Test Price':fmt_price,'True Predictions Price':fmt_price }\n",
    "df_show.style.format(format_dict).hide_index()\n",
    "```\n",
    "\n",
    "## NOTES ON FORECASTING INTO FUTURE\n",
    "\n",
    "- Loop to get next prediction based on current eval_batch\n",
    "- Append prediction to next eval batch and drop the oldest value in the current eval_batch\n",
    "- To predict into the TRUE FUTURE, extend the `for i in range(len(test)):` line\n",
    "    - replace range(len(test)) with however far you want to predict.\n",
    "    - Data will get _much noisier_ farther out into TRUE future.\n",
    "\n",
    "```python\n",
    "test_predictions = []\n",
    "\n",
    "# GETTING THE FIRST EVAL PATCH AND RESHAPING TO MATCH INPUT\n",
    "first_eval_batch = scaled_train[-n_input:]\n",
    "current_batch = first_eval_batch.reshape((1, n_input, n_features))\n",
    "\n",
    "# LOOP THROUGH ALL TEST DATA\n",
    "for i in range(len(test)):\n",
    "    \n",
    "    # get prediction 1 time stamp ahead ([0] is for grabbing just the number instead of [array])\n",
    "    current_pred = model.predict(current_batch)[0] \n",
    "    \n",
    "    # store prediction\n",
    "    test_predictions.append(current_pred) \n",
    "    \n",
    "    # UPDATE BATCH TO INCLUDE CURRENT PREDICTION AND DROP OLDEST VALUE FROM ARRAY\n",
    "    current_batch = np.append(current_batch[:,1:,:],[[current_pred]],axis=1) # axis===ndim\n",
    "\n",
    "print(test_predictions)\n",
    "\n",
    "display(scaled_test)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predictions = []\n",
    "\n",
    "# SHAPE OF \n",
    "first_eval_batch = scaled_train[-n_input:]\n",
    "current_batch = first_eval_batch.reshape((1, n_input, n_features))\n",
    "\n",
    "for i in range(len(test)):\n",
    "    \n",
    "    # get prediction 1 time stamp ahead ([0] is for grabbing just the number instead of [array])\n",
    "    current_pred = model.predict(current_batch)[0]\n",
    "    \n",
    "    # store prediction\n",
    "    test_predictions.append(current_pred) \n",
    "    \n",
    "    # update batch to now include prediction and drop first value\n",
    "    current_batch = np.append(current_batch[:,1:,:],[[current_pred]],axis=1)\n",
    "\n",
    "df_show=pd.DataFrame({'Predictions':[x[0] for x in test_predictions],'Test Values':scaled_test.ravel()})    \n",
    "# print(test_predictions)\n",
    "# display(scaled_test)\n",
    "display(df_show)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_eval_batches(model,scaled_train_data, scaled_test_data, n_input,n_features):\n",
    "    test_predictions = []\n",
    "    first_eval_batch=[]\n",
    "    # take the last window size of data from training data \n",
    "    first_eval_batch = scaled_train_data[-n_input:]\n",
    "    current_batch = first_eval_batch.reshape((1, n_input, n_features))\n",
    "    \n",
    "    for i in range(len(test_data)):\n",
    "        print(i)\n",
    "        # get prediction 1 time stamp ahead ([0] is for grabbing just the number instead of [array])\n",
    "        current_pred = model.predict(current_batch)[0]\n",
    "        \n",
    "        # store prediction\n",
    "        test_predictions.append(current_pred) \n",
    "\n",
    "        # update batch to now include prediction and drop first value\n",
    "        current_batch = np.append(current_batch[:,1:,:],[[current_pred]],axis=1)\n",
    "    return test_predictions\n",
    "\n",
    "new_preds = test_eval_batches(model,train_data,test_data,n_input,n_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model1.predict()\n",
    "scaled_train_data = df_train['price']\n",
    "scaled_test_data = df_test['price']\n",
    "n_input= model_params['input_params']['n_input']\n",
    "n_features = model_params['input_params']['n_features']\n",
    "\n",
    "def get_preds_from_preds(model1,scaled_train_data, scaled_test_data, model_params=None, \n",
    "                      first_eval_from_train_data=True, return_df=True,suffix=None):\n",
    "    test_predictions = []\n",
    "    first_eval_batch=[]\n",
    "    index_tracker = []\n",
    "    if model_params is not None:\n",
    "#         train_data_index = model_params['input_params']['train_data_index']\n",
    "#         test_data_index = model_params['input_params']['test_data_index']\n",
    "        n_input =  model_params['data_params']['x_window']\n",
    "\n",
    "\n",
    "    preds_out = [['i','index','pred_price']]\n",
    "    \n",
    "    # making series into arrays and saving index\n",
    "    train_data_index = scaled_train_data.index#[-n_input:]\n",
    "    scaled_train_data = scaled_train_data.values#[-n_input:].values\n",
    "\n",
    "    # making series into arrays and saving index\n",
    "    test_data_index = scaled_test_data.index\n",
    "    scaled_test_data = scaled_test_data.values\n",
    "    \n",
    "    \n",
    "    if first_eval_from_train_data:\n",
    "        loop_length = range(len(scaled_test_data))\n",
    "        \n",
    "        # take the last window size of data from training data \n",
    "        first_eval_batch = scaled_train_data[-n_input:]\n",
    "        first_batch_idx = train_data_index[-n_input:]\n",
    "\n",
    "    else:\n",
    "        loop_length = range(n_input,len(scaled_test_data))\n",
    "        first_eval_batch = scaled_test_data[:n_input]\n",
    "      \n",
    "    \n",
    "    current_batch = first_eval_batch.reshape((1, n_input, n_features))\n",
    "\n",
    "    for i in loop_length:\n",
    "\n",
    "        # get prediction 1 time stamp ahead ([0] is for grabbing just the number instead of [array])\n",
    "        current_pred = model.predict(current_batch)[0]\n",
    "        preds_out.append([i,test_data_index[i],current_pred[0]])\n",
    "        \n",
    "        \n",
    "        # store prediction\n",
    "        test_predictions.append(current_pred) \n",
    "\n",
    "        # update batch to now include prediction and drop first value\n",
    "        current_batch = np.append(current_batch[:,1:,:],[[current_pred]],axis=1)\n",
    "        \n",
    "    if return_df:\n",
    "        res_df = bs.list2df(preds_out)\n",
    "        res_df['index'] = pd.to_datetime(res_df['index'])\n",
    "        res_df.set_index('index',inplace=True)\n",
    "        \n",
    "        if suffix is not None:\n",
    "            \n",
    "            colnames = [name+suffix for name in res_df.columns]\n",
    "        else:\n",
    "            colnames = res_df.columns\n",
    "            \n",
    "        res_df.columns=colnames\n",
    "        return res_df #test_predictions, checks\n",
    "    \n",
    "    else:\n",
    "        print('reutrning nothing')\n",
    "#         return test_predictions\n",
    "    \n",
    "    \n",
    "    \n",
    "def get_preds_from_gen(model,test_generator,true_test_data,model_params,return_combined=False):\n",
    "        \n",
    "        if model_params is not None:\n",
    "            train_data_index = model_params['input_params']['train_data_index']\n",
    "            test_data_index = model_params['input_params']['test_data_index']\n",
    "            x_window =  model_params['data_params']['x_window']\n",
    "\n",
    "        \n",
    "        gen_index = true_test_data.index[test_generator.start_index:test_generator.end_index+1]\n",
    "        gen_true_targets = test_generator.targets[test_generator.start_index:test_generator.end_index+1]\n",
    "        gen_preds = model.predict_generator(test_generator)\n",
    "        gen_preds = gen_preds.ravel()\n",
    "        gen_true_targets = gen_true_targets.ravel()\n",
    "        \n",
    "        gen_pred_df = pd.DataFrame({'index':gen_index,'gen_true_target':gen_true_targets,'gen_preds':gen_preds})\n",
    "        gen_pred_df['index'] = pd.to_datetime(gen_pred_df['index'])\n",
    "        gen_pred_df.set_index('index',inplace=True)\n",
    "        return gen_pred_df\n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "gen_df = get_preds_from_gen(model1, test_generator,df_test['price'],model_params)    \n",
    "func_df = get_preds_from_preds(model1,df_train['price'],df_test['price'],model_params,\n",
    "                               suffix='_func',first_eval_from_train_data=False)+\n",
    "bs.display_side_by_side(gen_df,func_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ##### SUPER IMPORTANT CELL\n",
    "# scaled_train_data = df_train['price']\n",
    "# scaled_test_data = df_test['price']\n",
    "\n",
    "# get_model_preds_from_preds= ji.get_model_preds_from_preds\n",
    "# def get_preds_from_preds(model1,scaled_train_data, scaled_test_data,\n",
    "#                          model_params=None, n_input=None, n_features=None, \n",
    "#                          build_preds_from_train=True, return_df=True,suffix=None):\n",
    "    \n",
    "#     \"\"\" Gets predictions from model using using its own predictions as the subsequent input.\n",
    "#     Must provide a model_params dictionary with 'input_params' OR must define ('n_input','n_features').\n",
    "    \n",
    "#     * IF build_preds_from_train is True:\n",
    "#         1. starting true time series for predictions is the last rows [-n_input:] from training data.\n",
    "#         2. output predicitons will be the same length as the input scaled_test_data\n",
    "    \n",
    "#     * IF build_preds_from_train is False:\n",
    "#         1. starting true time series for predictions is the first rows [:n_input] from test data.\n",
    "#         2. output predicitons will be shorter by n_input # of rows\n",
    "#         \"\"\"\n",
    "#     test_predictions = []\n",
    "#     first_eval_batch=[]\n",
    "#     index_tracker = []\n",
    "    \n",
    "#     if model_params is not None:\n",
    "#         n_input= model_params['input_params']['n_input']\n",
    "#         n_features = model_params['input_params']['n_features']\n",
    "\n",
    "#     if model_params is None:\n",
    "#         if n_input is None or n_features is None:\n",
    "#             raise Exception('Must provide model params or define n_input and n_features')\n",
    "        \n",
    "\n",
    "#     preds_out = [['i','index','pred']]\n",
    "    \n",
    "#     # SAVING COPY OF INPUT TEST DATA\n",
    "#     scaled_test_series = scaled_test_data.copy() \n",
    "\n",
    "#     ## SAVING TRAIN AND TEST DATA INDICES AND VALUES\n",
    "#     train_data_index = scaled_train_data.index\n",
    "#     scaled_train_data = scaled_train_data.values\n",
    "#     test_data_index = scaled_test_data.index\n",
    "#     scaled_test_data = scaled_test_data.values\n",
    "    \n",
    "    \n",
    "#     ## PREPARE THE FIRST EVAL BATCH TIMESERIES FROM TRAIN OR TEST DATA\n",
    "#     # Change parameters depending on if from train or test data\n",
    "#     if build_preds_from_train:\n",
    "        \n",
    "#         # If using trianing data loop through full test data\n",
    "#         loop_length = range(len(scaled_test_data))\n",
    "        \n",
    "#         # take the last window size of data from training data \n",
    "#         first_eval_batch = scaled_train_data[-n_input:]\n",
    "#         first_batch_idx = train_data_index[-n_input:]\n",
    "        \n",
    "#         # set the true index to test_data_index\n",
    "#         true_index_out = test_data_index\n",
    "        \n",
    "#     # Set the loop to be from n_input # of rows into test_data\n",
    "#     else:\n",
    "#         loop_length = range(n_input,len(scaled_test_data))\n",
    "#         first_eval_batch = scaled_test_data[:n_input]\n",
    "#         true_index_out =  test_data_index[n_input:]\n",
    "      \n",
    "    \n",
    "#     # reshape first batch of data for model.predict \n",
    "#     current_batch = first_eval_batch.reshape((1, n_input, n_features))\n",
    "\n",
    "    \n",
    "#     ## LOOP THROUGH REMAINING TIMEBINS USING CURRENT PREDICITONS AS NEW DATA FOR NEXT\n",
    "#     for i in loop_length:\n",
    "\n",
    "#         # get prediction 1 time stamp ahead ([0] is for grabbing just the number instead of [array])\n",
    "#         current_pred = model.predict(current_batch)[0]        \n",
    "#         # store prediction\n",
    "#         test_predictions.append(current_pred) \n",
    "\n",
    "#         # update batch to now include prediction and drop first value\n",
    "#         current_batch = np.append(current_batch[:,1:,:],[[current_pred]],axis=1)\n",
    "        \n",
    "#         ## Append the data to the output df list\n",
    "#         preds_out.append([i,test_data_index[i],current_pred[0]])\n",
    "\n",
    "\n",
    "#     ## If returning a dataframe,prepare and rename with suffix\n",
    "#     if return_df:\n",
    "#         res_df = bs.list2df(preds_out)\n",
    "#         res_df['index'] = pd.to_datetime(res_df['index'])\n",
    "#         res_df.set_index('index',inplace=True)\n",
    "        \n",
    "#         # adding true price\n",
    "#         res_df['true'] = scaled_test_series.loc[true_index_out]\n",
    "#         res_df=res_df[['i','true','pred']]\n",
    "        \n",
    "#         if suffix is not None:\n",
    "#             colnames = [name+suffix for name in res_df.columns]\n",
    "#         else:\n",
    "#             colnames = res_df.columns\n",
    "            \n",
    "#         res_df.columns=colnames\n",
    "#         return res_df #test_predictions, checks\n",
    "    \n",
    "#     # Else just return array of predictions\n",
    "#     else:\n",
    "#         return test_predictions\n",
    "\n",
    "    \n",
    "    \n",
    "# def get_model_preds_from_gen(model,test_generator,true_test_data,model_params=None,\n",
    "#                        n_input=None, n_features=None, suffix=None, return_df=False):\n",
    "#         \"\"\"\n",
    "#         Gets prediction from model using the generator's timeseries using model.predict_generator()\n",
    "#         Must provide a model_params dictionary with 'input_params' OR must define ('n_input','n_features').\n",
    "\n",
    "#         \"\"\"\n",
    "#         if model_params is not None:\n",
    "#             n_input= model_params['input_params']['n_input']\n",
    "#             n_features = model_params['input_params']['n_features']\n",
    "\n",
    "#         if model_params is None:\n",
    "#             if n_input is None or n_features is None:\n",
    "#                 raise Exception('Must provide model params or define n_input and n_features')\n",
    "\n",
    "#         # GET TRUE VALUES AND DATETIME INDEX FROM GENERATOR\n",
    "        \n",
    "#         # Get true time index from the generator's start_index and end_index \n",
    "#         gen_index = true_test_data.index[test_generator.start_index:test_generator.end_index+1]\n",
    "#         gen_true_targets = test_generator.targets[test_generator.start_index:test_generator.end_index+1]\n",
    "        \n",
    "        \n",
    "#         # Generate predictions from the test_generator\n",
    "#         gen_preds = model.predict_generator(test_generator)\n",
    "#         gen_preds_flat = gen_preds.ravel()\n",
    "        \n",
    "#         gen_true_targets = gen_true_targets.ravel()\n",
    "        \n",
    "        \n",
    "#         # RETURN OUTPUT AS DATAFRAME OR ARRAY OF PREDS\n",
    "#         if return_df:\n",
    "#             # Combine the outputs\n",
    "#             gen_pred_df = pd.DataFrame({'index':gen_index,'true':gen_true_targets,'pred':gen_preds_flat})\n",
    "#             gen_pred_df['index'] = pd.to_datetime(gen_pred_df['index'])\n",
    "#             gen_pred_df.set_index('index',inplace=True)\n",
    "\n",
    "#             if suffix is not None:\n",
    "#                 colnames = [name+suffix for name in gen_pred_df.columns]\n",
    "#             else:\n",
    "#                 colnames = gen_pred_df.columns\n",
    "#             return gen_pred_df\n",
    "#         else:\n",
    "#             return gen_preds\n",
    "\n",
    "\n",
    "# def compare_model_pred_methods(model, true_train_series,true_test_series, test_generator=None,\n",
    "#                                model_params=None, n_input=None, n_features=None, from_gen=True,\n",
    "#                                from_train_series = True, from_test_series=True, \n",
    "#                                iplot=True, plot_with_train_data=True,return_df=True, inverse_tf=True):\n",
    "#     \"\"\" Gets predictions for training data from the 3 options: \n",
    "#     1) from generator  --  len(output) = (len(true_test_series)-n_input)\n",
    "#     2) from predictions on test data  --  len(output) = (len(true_test_series)-n_input)\n",
    "#     3) from predictions on train data -- len(true_test_series)\n",
    "#     \"\"\"\n",
    "#         if model_params is not None:\n",
    "#             n_input= model_params['input_params']['n_input']\n",
    "#             n_features = model_params['input_params']['n_features']\n",
    "\n",
    "#         if model_params is None:\n",
    "#             if n_input is None or n_features is None:\n",
    "#                 raise Exception('Must provide model params or define n_input and n_features')\n",
    "                \n",
    "#         if from_gen is True and test_generator is None:\n",
    "#             raise Exception('If from_gen=True, must provide generator.')\n",
    "\n",
    "                \n",
    "#         ### GET the 3 DIFERENT TYPES OF PREDICTIONS    \n",
    "#         df_list = []\n",
    "#         if from_gen:\n",
    "#             gen_df = ji.get_model_preds_from_gen(model, test_generator,true_test_series,\n",
    "#                                                  model_params=model_params, n_input=n_input, n_features=n_features, \n",
    "#                                                 suffix='_from_gen',return_df=True)    \n",
    "#             df_list.append(gen_df)\n",
    "\n",
    "#         if from_test_series:\n",
    "#             func_df_from_test = ji.get_model_preds_from_preds(model, true_train_series,true_test_series,\n",
    "#                                                           model_params=model_params, n_input=n_input, \n",
    "#                                                           n_features=n_features, suffix='_from_test',\n",
    "#                                                           build_preds_from_train=False, return_df=True)\n",
    "#             df_list.append(func_df_from_test)\n",
    "\n",
    "#         if from_train_series:\n",
    "#             func_df_from_train  = ji.get_model_preds_from_preds(model,true_train_series,true_test_series,\n",
    "#                                                                 model_params=model_params, n_input=n_input,\n",
    "#                                                                 suffix='_from_train', build_preds_from_train=True,\n",
    "#                                                             return_df=True)\n",
    "#             df_list.append(func_df_from_train)\n",
    "\n",
    "#         # bs.display_side_by_side(func_df,func_df_from_train)\n",
    "\n",
    "#         df_all_preds = pd.concat([df for df in df_list],axis=1)\n",
    "#         df_all_preds = bs.drop_cols(df_all_preds,['i_'])\n",
    "\n",
    "\n",
    "#         if inverse_tf:\n",
    "#             df_out = ji.transform_cols_from_library(df_all_preds,single_scaler=model_params['scaler_library']['price'],inverse=True)\n",
    "#         else:\n",
    "#             df_out = df_all_preds\n",
    "\n",
    "#         if iplot:\n",
    "#             if plot_with_train_data is False:\n",
    "#                 ji.plotly_time_series(df_out)\n",
    "#             else:\n",
    "#                 true_train_price = ji.inverse_transform_series(true_train_series,model_params['scaler_library']['price'])\n",
    "#                 df_plot = pd.concat([true_train_price,df_out],axis=1)\n",
    "#                 ji.plotly_time_series(df_plot)\n",
    "\n",
    "#         if return_df:\n",
    "#             return df_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ✔BOOKMARK: NOW MUST FIGURE OUT HOW TO GET THE EVALUATE_REGRESSION FOR THE DF_MODEL WITH MULTIPLE PREDS\n",
    "## ALSO - CODE FROM TESTING GENERATOR PREDICTIOND\n",
    "- ## USED NEW `get_evaluate_regression_dict()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# from keras.preprocessing.sequence import TimeseriesGenerator\n",
    "# batch_size=1\n",
    "# n_input = model_params['data_params']['x_window']\n",
    "# print(n_input)\n",
    "# # batch_size = model_params['data_params']['batch_size']\n",
    "# # RESHAPING TRAINING AND TEST DATA \n",
    "\n",
    "# test_data_series = df_test['price']\n",
    "# print(test_data_series.shape)\n",
    "# test_data = test_data_series.values.reshape(-1,1)\n",
    "# print(test_data.shape)\n",
    "# test_data_index =test_data_series.index\n",
    "# len(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# true_test_data = df_test['price']\n",
    "# true_train_data = df_train['price']\n",
    "\n",
    "# test_gen_index = true_test_data.index[test_generator.start_index:test_generator.end_index+1]\n",
    "# test_gen_preds = model1.predict_generator(test_generator)\n",
    "# test_gen_true_targets = test_generator.targets[test_generator.start_index:test_generator.end_index+1]\n",
    "\n",
    "# train_gen_index = true_train_data.index[train_generator.start_index:train_generator.end_index+1]\n",
    "# train_gen_preds = model1.predict_generator(train_generator)\n",
    "# train_gen_true_targets = train_generator.targets[train_generator.start_index:train_generator.end_index+1]\n",
    "\n",
    "\n",
    "# print('---'*3,'TRAIN DATA','---'*3)\n",
    "# print('gen_index: ', train_gen_index.shape)\n",
    "# print('gen_preds: ', train_gen_preds.shape)\n",
    "# print('gen_true_targets: ', train_gen_true_targets.shape)\n",
    "\n",
    "\n",
    "# print('---'*3,'TEST DATA','---'*3)\n",
    "# print('gen_index: ', test_gen_index.shape)\n",
    "# print('gen_preds: ', test_gen_preds.shape)\n",
    "# print('gen_true_targets: ', test_gen_true_targets.shape)\n",
    "\n",
    "\n",
    "#     # Generate predictions from the test_generator\n",
    "# test_gen_preds = model1.predict_generator(test_generator)\n",
    "# test_gen_preds_flat = test_gen_preds.reshape() #ravel()\n",
    "# test_gen_true_targets = test_gen_true_targets.ravel()\n",
    "\n",
    "# print('---'*3,'TEST DATA RAVEL','---'*3)\n",
    "# print('gen_index: ', test_gen_index.shape)\n",
    "# print('gen_preds_flat: ', test_gen_preds_flat.shape)\n",
    "# print('gen_preds: ', test_gen_preds.shape)\n",
    "# print('gen_true_targets: ', test_gen_true_targets.shape)\n",
    "# # plt.plot(test_gen_preds)\n",
    "\n",
    "# test_gen_preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TWITTER TS PROCESSING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## PREVIEWING EFFECT OF ROLLING OFFSETS\n",
    "def test_ofst_rolling(i=0):\n",
    "    \"\"\"Tool to help visualize the effect of offsets when deterimining a time interval range.\"\"\"\n",
    "    ofst = pd.offsets.CustomBusinessHour(start='09:30',end='16:30',)\n",
    "    print('TS: \\t\\t',twitter_df.index[i])\n",
    "    print('TS.floor(H): \\t',twitter_df.index[i].floor('H'))\n",
    "    print('TS rollback: \\t\\t',ofst.rollback(twitter_df.index[i]))\n",
    "    print('TS rollforward: \\t',ofst.rollforward(twitter_df.index[i]))\n",
    "    print('\\n')\n",
    "    print('TS floor(H)-rollback: \\t',ofst.rollback(twitter_df.index[i].floor('H')))\n",
    "    print('TS floor(H)-rollforward: ',ofst.rollforward(twitter_df.index[i].floor('H')))\n",
    "\n",
    "test_ofst_rolling(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# REMOVED FOR MEET WITH BRANDON"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  ~~KERAS MODEL 2B~~\n",
    "Blog Post Code\n",
    "- NEW BLOG POST - LSTM TIME SERIES FORECASTING\n",
    "    - https://machinelearningmastery.com/how-to-develop-lstm-models-for-time-series-forecasting/\n",
    "- https://www.analyticsvidhya.com/blog/2018/10/predicting-stock-price-machine-learningnd-deep-learning-techniques-python/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "from keras.layers import TimeDistributed, Conv1D, MaxPool1D, Flatten, LSTM, Dense, MaxPooling1D\n",
    "model3 = Sequential()\n",
    "# input_shape =  (X_train_in.shape[0],X_train_in.shape[1],1)\n",
    "input_shape=(n_input, n_features,1)\n",
    "\n",
    "model3.add(TimeDistributed(Conv1D(filters=64, kernel_size=1, activation='relu'), input_shape=input_shape)) #(None, n_steps, n_features)))\n",
    "model3.add(TimeDistributed(MaxPooling1D(pool_size=2)))\n",
    "model3.add(TimeDistributed(Flatten()))\n",
    "model3.add(LSTM(50, activation='relu'))\n",
    "model3.add(Dense(1))\n",
    "model3.compile(optimizer='adam', loss=my_rmse)\n",
    "model3.summary()\n",
    "# fit model\n",
    "\n",
    "clock.tic('')\n",
    "history3 = model3.fit_generator( train_generator,epochs=3,verbose=2,callbacks=callbacks,workers=3)\n",
    "\n",
    "clock.toc('')\n",
    "evaluate_model_plot_history(model3, train_generator, test_generator)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-#\n",
    "\\# ✔ BOOKMARK - RESUMING ANALYSIS WITH NEW FUNCTIONS\n",
    "- Check Evernote log: https://www.evernote.com/l/AApgV1EaL9lEYpujWnwHq62W9QyEKkH4hwA/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "learn-env-ext",
   "language": "python",
   "name": "learn-env-ext"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
