{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functions_combined_BEST import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# import pandas as pd \n",
    "# import numpy as np\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# import warnings\n",
    "# warnings.filterwarnings('ignore')\n",
    "\n",
    "# def quick_table(tuples, col_names=None, caption =None,display_df=True):\n",
    "#     \"\"\"Accepts a bigram output tuple of tuples and makes captioned table.\"\"\"\n",
    "#     import pandas as pd\n",
    "#     from IPython.display import display\n",
    "#     if col_names == None:\n",
    "    \n",
    "#         df = pd.DataFrame.from_records(tuples)\n",
    "        \n",
    "#     else:\n",
    "        \n",
    "#         df = pd.DataFrame.from_records(tuples,columns=col_names)\n",
    "#         dfs = df.style.set_caption(caption)\n",
    "        \n",
    "#         if display_df == True:\n",
    "#             display(dfs)\n",
    "            \n",
    "#     return df\n",
    "\n",
    "# def compare_word_cloud(text1,label1,text2,label2):\n",
    "#     \"\"\"Compares the wordclouds from 2 sets of texts\"\"\"\n",
    "#     from wordcloud import WordCloud\n",
    "#     import matplotlib.pyplot as plt\n",
    "\n",
    "#     wordcloud1 = WordCloud(max_font_size=80, max_words=200, background_color='white').generate(' '.join(text1))\n",
    "#     wordcloud2 = WordCloud(max_font_size=80, max_words=200, background_color='white').generate(' '.join(text2))\n",
    "\n",
    "\n",
    "#     fig,ax = plt.subplots(nrows=1,ncols=2,figsize=(20,15))\n",
    "#     ax[0].imshow(wordcloud1, interpolation='bilinear')\n",
    "#     ax[0].set_aspect(1.5)\n",
    "#     ax[0].axis(\"off\")\n",
    "#     ax[0].set_title(label1, fontsize=20)\n",
    "\n",
    "#     ax[1].imshow(wordcloud2, interpolation='bilinear')\n",
    "#     ax[1].set_aspect(1.5)\n",
    "#     ax[1].axis(\"off\")\n",
    "#     ax[1].set_title(label2, fontsize=20)\n",
    "\n",
    "#     fig.tight_layout()\n",
    "#     return fig,ax\n",
    "\n",
    "# # Define get_tags_ats to accept a list of text entries and return all found tags and ats as 2 series/lists\n",
    "# def get_tags_ats(text_to_search,exp_tag = r'(#\\w*)',exp_at = r'(@\\w*)', output='series',show_counts=False):\n",
    "#     \"\"\"Accepts a list of text entries to search, and a regex for tags, and a regex for @'s.\n",
    "#     Joins all entries in the list of text and then re.findsall() for both expressions.\n",
    "#     Returns a series of found_tags and a series of found_ats.'\"\"\"\n",
    "#     import re\n",
    "#     import pandas as pd\n",
    "#     # Create a single long joined-list of strings\n",
    "#     text_to_search_combined = ' '.join(text_to_search)\n",
    "        \n",
    "#     # print(len(text_to_search_combined), len(text_to_search_list))\n",
    "#     found_tags = re.findall(exp_tag, text_to_search_combined)\n",
    "#     found_ats = re.findall(exp_at, text_to_search_combined)\n",
    "    \n",
    "#     if output.lower() == 'series':\n",
    "#         found_tags = pd.Series(found_tags, name='tags')\n",
    "#         found_ats = pd.Series(found_ats, name='ats')\n",
    "        \n",
    "#         if show_counts==True:\n",
    "#             print(f'\\t{found_tags.name}:\\n{found_tags.value_counts()} \\n\\n\\t{found_ats.name}:\\n{found_ats.value_counts()}')\n",
    "                \n",
    "#     if (output.lower() != 'series') & (show_counts==True):\n",
    "#         raise Exception('output must be set to \"series\" in order to show_counts')\n",
    "                       \n",
    "#     return found_tags, found_ats\n",
    "\n",
    "\n",
    "# def clean_text(series,is_tokens=False,return_tokens=False, urls=True, hashtags=True, mentions=True, remove_stopwords=True, verbose=False):\n",
    "#     \"\"\"Accepts a series/df['column'] and tokenizes, removes urls, hasthtags, and @s using regex before tokenizing and removing stopwrods\"\"\"\n",
    "#     import pandas as pd\n",
    "#     import re, nltk\n",
    "#     from nltk.corpus import stopwords\n",
    "    \n",
    "#     series_cleaned=series.copy()\n",
    "    \n",
    "#     # Remove URLS\n",
    "#     if urls==True:\n",
    "#         urls = re.compile(r\"(http[s]?://\\w*\\.\\w*/+\\w+)\")\n",
    "#         series_cleaned = series_cleaned.apply(lambda x: urls.sub(' ', x))\n",
    "            \n",
    "#         if verbose==True:\n",
    "#             print('URLs removed...')\n",
    "            \n",
    "#     # Remove hashtags\n",
    "#     if hashtags==True:\n",
    "#         hashtags = re.compile(r'(\\#\\w*)')\n",
    "#         series_cleaned = series_cleaned.apply(lambda x: hashtags.sub(' ', x))\n",
    "        \n",
    "#         if verbose==True:\n",
    "#             print('Hashtags removed...')\n",
    "    \n",
    "#     # Remove mentions\n",
    "#     if mentions==True:\n",
    "#         mentions = re.compile(r'(\\@\\w*)')\n",
    "#         series_cleaned = series_cleaned.apply(lambda x: mentions.sub(' ',x))\n",
    "\n",
    "#         if verbose==True:\n",
    "#             print('Mentions removed...')\n",
    "    \n",
    "    \n",
    "#     # Regexp_tokenize stopped words (to keep contractions)\n",
    "#     if is_tokens==False:\n",
    "#         pattern = \"([a-zA-Z]+(?:'[a-z]+)?)\"\n",
    "#         series_cleaned = series_cleaned.apply(lambda x: nltk.regexp_tokenize(x,pattern))\n",
    "#         if verbose==True:\n",
    "#             print('Text regexp_tokenized...\\n')\n",
    "    \n",
    "    \n",
    "#     # Filter Out Stopwords\n",
    "#     stopwords_list = []\n",
    "#     from nltk.corpus import stopwords\n",
    "#     import string\n",
    "    \n",
    "#     # Generate Stopwords List\n",
    "#     stopwords_list = stopwords.words('english')\n",
    "#     stopwords_list += list(string.punctuation)\n",
    "#     stopwords_list += ['http','https','...','``','co','“','’','‘','”',\n",
    "#                        'rt',\"n't\",\"''\",\"RT\",'u','s',\"'s\",'?']#,'@','#']\n",
    "#     stopwords_list += [0,1,2,3,4,5,6,7,8,9]\n",
    "#     stopwords_list +=['RT','rt',';']\n",
    "     \n",
    "#     if remove_stopwords==True:\n",
    "#         series_cleaned = series_cleaned.apply(lambda x: [w.lower() for w in x if w.lower() not in stopwords_list])\n",
    "#         # for s in range(len(series_cleaned)):\n",
    "#         #     text =[]\n",
    "#         #     text_stopped = []\n",
    "#         #     text = series_cleaned[s]\n",
    "#         #     text_stopped = [x.lower() for x in text if x.lower() not in stopwords_list]\n",
    "#         #     series_cleaned[s]= text_stopped\n",
    "        \n",
    "#         if verbose==True:\n",
    "#             print('Stopwords removed...')\n",
    "       \n",
    "#     if return_tokens==False:\n",
    "#         series_cleaned = series_cleaned.apply(lambda x: ' '.join(x))\n",
    "    \n",
    "#     print('\\n')\n",
    "#     return series_cleaned\n",
    "\n",
    "# def train_test_val_split(X,y,test_size=0.20,val_size=0.1):\n",
    "#     \"\"\"Performs 2 successive train_test_splits to produce a training, testing, and validation dataset\"\"\"\n",
    "#     from sklearn.model_selection import train_test_split\n",
    "\n",
    "#     first_split_size = test_size + val_size\n",
    "#     second_split_size = val_size/(test_size + val_size)\n",
    "\n",
    "#     X_train, X_test_val, y_train, y_test_val = train_test_split(X, y, test_size=first_split_size)\n",
    "\n",
    "#     X_test, X_val, y_test, y_val = train_test_split(X_test_val, y_test_val, test_size=second_split_size)\n",
    "\n",
    "#     return X_train, X_test, X_val, y_train, y_test, y_val\n",
    "\n",
    "\n",
    "# def plot_keras_history(history):\n",
    "#     \"\"\"Plots the history['acc','val','val_acc','val_loss']\"\"\"\n",
    "#     import matplotlib.pyplot as plt\n",
    "#     acc = history.history['acc']\n",
    "#     loss = history.history['loss']\n",
    "#     val_acc = history.history['val_acc']\n",
    "#     val_loss = history.history['val_loss']\n",
    "#     x = range(1,len(acc)+1)\n",
    "    \n",
    "#     fig,ax = plt.subplots(nrows=2, ncols=1, figsize=(6,8))\n",
    "#     ax[0].plot(x, acc,'b',label='Training Acc')\n",
    "#     ax[0].plot(x, val_acc,'r',label='Validation Acc')\n",
    "#     ax[0].legend()\n",
    "#     ax[1].plot(x, loss,'b',label='Training Loss')\n",
    "#     ax[1].plot(x, val_loss, 'r', label='Validation Loss')\n",
    "#     ax[1].legend()\n",
    "#     plt.show()\n",
    "#     return fig, ax\n",
    "\n",
    "\n",
    "# def plot_auc_roc_curve(y_test, y_test_pred):\n",
    "#     \"\"\" Takes y_test and y_test_pred from a ML model and plots the AUC-ROC curve.\"\"\"\n",
    "#     from sklearn.metrics import confusion_matrix\n",
    "#     from sklearn.metrics import roc_auc_score, roc_curve\n",
    "#     import matplotlib.pyplot as plt\n",
    "#     auc = roc_auc_score(y_test, y_test_pred[:,1])\n",
    "\n",
    "#     FPr, TPr, _  = roc_curve(y_test, y_test_pred[:,1])\n",
    "#     plt.plot(FPr, TPr,label=f\"AUC for CatboostClassifier:\\n{round(auc,2)}\" )\n",
    "\n",
    "#     plt.plot([0, 1], [0, 1],  lw=2,linestyle='--')\n",
    "#     plt.xlim([-0.01, 1.0])\n",
    "#     plt.ylim([0.0, 1.05])\n",
    "\n",
    "#     plt.xlabel('False Positive Rate')\n",
    "#     plt.ylabel('True Positive Rate')\n",
    "#     plt.title('Receiver operating characteristic (ROC) Curve')\n",
    "#     plt.legend(loc=\"lower right\")\n",
    "#     plt.show()\n",
    "\n",
    "\n",
    "\n",
    "# def display_random_tweets(df_tokenize,n=5 ,display_cols=['content','text_for_vectors','tokens'], group_labels=[],verbose=True):\n",
    "#     \"\"\"Takes df_tokenize['text_for_vectors']\"\"\"\n",
    "#     import numpy as np\n",
    "#     import pandas as pd \n",
    "#     from IPython.display import display\n",
    "#     if len(group_labels)==0:\n",
    "\n",
    "#         group_labels = display_cols\n",
    "\n",
    "    \n",
    "#     random_tweets={}\n",
    "#     # Randomly pick n indices to display from specified col\n",
    "#     idx = np.random.choice(range(len(df_tokenize)), n)\n",
    "    \n",
    "#     for i in range(len(display_cols)):\n",
    "        \n",
    "#         group_name = str(group_labels[i])\n",
    "#         random_tweets[group_name] ={}\n",
    "\n",
    "#         # Select column data\n",
    "#         df_col = df_tokenize[display_cols[i]]\n",
    "        \n",
    "\n",
    "#         tweet_group = {}\n",
    "#         tweet_group['index'] = idx\n",
    "        \n",
    "#         chosen_tweets = df_col[idx]\n",
    "#         tweet_group['text'] = chosen_tweets\n",
    "\n",
    "#         # print(chosen_tweets)\n",
    "#         if verbose>0:\n",
    "#             with pd.option_context('max_colwidth',300):\n",
    "#                 df_display = pd.DataFrame.from_dict(tweet_group)\n",
    "#                 display(df_display.style.set_caption(f'Group: {group_name}'))\n",
    "\n",
    "\n",
    "#         random_tweets[group_name] = tweet_group\n",
    "        \n",
    "#         # if verbose>0:\n",
    "              \n",
    "#         #     for group,data in random_tweets.items():\n",
    "#         #         print(f'\\n\\nRandom Tweet for {group:>.{300}}:\\n{\"---\"*20}')\n",
    "\n",
    "#         #         df = random_tweets[group]\n",
    "#         #         display(df)\n",
    "#     if verbose==0:\n",
    "#         return random_tweets\n",
    "#     else:\n",
    "#         return\n",
    "\n",
    "\n",
    "# def reload(mod):\n",
    "#     \"\"\"Reloads the module from file.\"\"\"\n",
    "#     from importlib import reload\n",
    "#     import sys\n",
    "#     print(f'Reloading...')\n",
    "#     return  reload(mod)\n",
    "\n",
    "\n",
    "# def process_df_full(df_full, raw_col='content_raw', fill_content_col='content',force=False):\n",
    "#     \"\"\"Accepts df_full, which contains the raw tweets to process, the raw_col name, the column to fill.\n",
    "#     If force=False, returns error if the fill_content_col already exists.\n",
    "#     Processing Workflow:1) Create has_RT, starts_RT columns. 2) Creates [fill_content_col,`content_min_clean`] cols after removing 'RT @mention:' and urls.\n",
    "#     3) Removes hashtags from fill_content_col and saves hashtags in new col. 4) Removes mentions from fill_content_col and saves to new column.\"\"\"\n",
    "#     import re\n",
    "#     import pandas as pd\n",
    "    \n",
    "#     if force==False:\n",
    "#         if fill_content_col in df_full.columns:\n",
    "#             raise Exception(f'{fill_content_col} already exists. To overwrite, set force=True.')\n",
    "\n",
    "\n",
    "#     # # create 'content_raw' column from 'content'\n",
    "#     # df_full[fill_content_col] = df_full['content'].copy()\n",
    "\n",
    "\n",
    "#     # Add has_RT and starts_RT columns\n",
    "#     # Creating columns for tweets that `has_RT` or `starts_RT`\n",
    "#     df_full['has_RT']=df_full[raw_col].str.contains('RT')\n",
    "#     df_full['starts_RT']=df_full[raw_col].str.contains('^RT')\n",
    "\n",
    "\n",
    "#     ## FIRST REMOVE THE RT HEADERS\n",
    "\n",
    "#     # Remove `RT @Mentions` FIRST:\n",
    "#     re_RT = re.compile(r'RT [@]?\\w*:')\n",
    "\n",
    "#     # raw_col =  'content_raw'\n",
    "#     check_content_col =raw_col\n",
    "#     fill_content_col = fill_content_col\n",
    "\n",
    "#     df_full['content_starts_RT'] = df_full[check_content_col].apply(lambda x: re_RT.findall(x))\n",
    "#     df_full[fill_content_col] =  df_full[check_content_col].apply(lambda x: re_RT.sub(' ',x))\n",
    "\n",
    "\n",
    "#     ## SECOND REMOVE URLS\n",
    "#     # Remove urls with regex\n",
    "#     urls = re.compile(r\"(http[s]?://\\w*\\.\\w*/+\\w+)\")\n",
    "\n",
    "#     check_content_col = fill_content_col\n",
    "#     fill_content_col = fill_content_col\n",
    "\n",
    "#     # df_full['content_urls'] = df_full[check_content_col].apply(lambda x: urls.findall(x))\n",
    "#     df_full[fill_content_col] =  df_full[check_content_col].apply(lambda x: urls.sub(' ',x))\n",
    "\n",
    "#     ## SAVE THIS MINIMALLY CLEANED CONTENT AS 'content_min_clean'\n",
    "#     df_full['content_min_clean'] =  df_full[fill_content_col]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#     ## REMOVE AND SAVE HASHTAGS, MENTIONS\n",
    "#     # Remove and save Hashtags\n",
    "#     hashtags = re.compile(r'\\#\\w*')\n",
    "\n",
    "#     check_content_col = fill_content_col\n",
    "#     fill_content_col = fill_content_col\n",
    "\n",
    "#     df_full['content_hashtags'] =  df_full[check_content_col].apply(lambda x: hashtags.findall(x))\n",
    "#     df_full[fill_content_col] =  df_full[check_content_col].apply(lambda x: hashtags.sub(' ',x))\n",
    "\n",
    "\n",
    "#     # Remove and save mentions (@)'s\n",
    "#     mentions = re.compile(r'\\@\\w*')\n",
    "\n",
    "#     check_content_col = fill_content_col\n",
    "#     fill_content_col = fill_content_col\n",
    "\n",
    "#     df_full['content_mentions'] =  df_full[check_content_col].apply(lambda x: mentions.findall(x))\n",
    "#     df_full[fill_content_col] =  df_full[check_content_col].apply(lambda x: mentions.sub(' ',x))\n",
    "\n",
    "#     return df_full\n",
    "\n",
    "\n",
    "\n",
    "# def load_orig_dataset(root_dir = 'russian-troll-tweets/', ext='.csv'):\n",
    "#     \"\"\"Accepts a root_dir, finds all files that end with ext and loads into a dataframe.\"\"\"\n",
    "#     import os\n",
    "#     import pandas as pd\n",
    "#     # root_dir = 'russian-troll-tweets/'\n",
    "#     # os.listdir('russian-troll-tweets/')\n",
    "#     filelist = [os.path.join(root_dir,file) for file in os.listdir(root_dir) if file.endswith(ext)]\n",
    "#     print(f'Loading {len(filelist)} files into dataframe...')\n",
    "#         # Vertically concatenate \n",
    "#     df = pd.DataFrame()\n",
    "#     for file in filelist:\n",
    "#         df_new = pd.read_csv(file)\n",
    "#         df = pd.concat([df,df_new], axis=0)\n",
    "#     # df.info()\n",
    "\n",
    "#     return df\n",
    "\n",
    "# def ask_user_to_save(df, filename=None,encoding=None, ask=True, skip_saving=False, overwrite=False):\n",
    "#     \"\"\"Asks user to save df as filename. If no filename specified: filename ='saved_df.csv'\n",
    "#     Set ask=False to save without asking.\"\"\"\n",
    "#     import os, warnings\n",
    "\n",
    "#     if type(df)=='string':\n",
    "#         raise Exception('First input must be the dataframe to be saved.')\n",
    "#     if skip_saving==True:\n",
    "#         return print(f'Since skip_loading=True, no file was save.')\n",
    "\n",
    "#     if filename==None:\n",
    "#         filename='saved_df.csv'\n",
    "    \n",
    "#     if ask==True:\n",
    "#         ans = input('Would you like to save the df to a .csv?(y/n):')\n",
    "#     else:\n",
    "#         ans = 'y'\n",
    "    \n",
    "#     # If ans to save =='y'\n",
    "#     if ans.lower()=='y':\n",
    "\n",
    "#         # Check if the file already exists\n",
    "#         if filename in os.listdir():\n",
    "        \n",
    "#             if overwrite==False:\n",
    "#                 # raise Exception(f\"{filename} already exists.\")\n",
    "#                 return warnings.warn(f\"{filename} already exists.\")\n",
    "\n",
    "#             if overwrite==True:\n",
    "#                 warnings.warn(f\"Overwriting {filename}.\")\n",
    "\n",
    "#         df.to_csv(filename)\n",
    "#         print(f'{filename} successfully saved.')\n",
    "#     else:\n",
    "#         print('Ok. No file was saved. ')\n",
    "        \n",
    "\n",
    "# def ask_user_to_load(filename, load_as_global = True ,ask=True, skip_loading=False, index_col=0, encoding=None):\n",
    "#     \"\"\"Asks user to save df as filename. If no filename specified: filename ='saved_df.csv'\n",
    "#     Set ask=False to save without asking.\"\"\"\n",
    "#     import os\n",
    "#     import pandas as pd\n",
    "\n",
    "    \n",
    "#     if skip_loading==True:\n",
    "#         return print(f'Since skip_loading=True, no file was loaded.')\n",
    "        \n",
    "\n",
    "#     if ask==True:\n",
    "#         ans = input('Would you like to load {filename} to a datafrane?(y/n):')\n",
    "\n",
    "#     else:\n",
    "#         ans = 'y'\n",
    "        \n",
    "\n",
    "#     # If ans to load =='y'\n",
    "#     if ans.lower()=='y':\n",
    "        \n",
    "#         if load_as_global == True:\n",
    "#             global df_\n",
    "#             df_ = pd.read_csv(filename, encoding=encoding,index_col=index_col) \n",
    "#             print(f'{filename} loaded as global variable: \"df_\"')\n",
    "#             pass\n",
    "#         else:\n",
    "#             df_ = pd.read_csv(filename, encoding=encoding,index_col=index_col) \n",
    "#             return df_\n",
    "#     else:\n",
    "#         return print('Ok. No file was loaded.')\n",
    "        \n",
    "\n",
    "# def run_all_checkpoint(skip=False):\n",
    "#     ans = input('Continue running all?(y/n):')\n",
    "#     if ans.lower()=='y':\n",
    "#         return print('OK. Continuing to run...')\n",
    "#     else:\n",
    "#         raise Exception('User requested to stop running.')\n",
    "\n",
    "\n",
    "# ## TO CHECK FOR STRINGS IN BOTH DATASETS:\n",
    "# def check_dfs_for_exp_list(df_controls, df_trolls, list_of_exp_to_check):\n",
    "#     df_resample = df_trolls\n",
    "#     for exp in list_of_exp_to_check:\n",
    "#     #     exp = '[Pp]eggy'\n",
    "#         print(f'For {exp}:')\n",
    "#         print(f\"\\tControl tweets: {len(df_controls.loc[df_controls['content_min_clean'].str.contains(exp)])}\")\n",
    "#         print(f\"\\tTroll tweets: {len(df_resample.loc[df_resample['content_min_clean'].str.contains(exp)])}\\n\")\n",
    "              \n",
    "# # list_of_exp_to_check = ['[Pp]eggy','[Mm]exico','nasty','impeachment','[mM]ueller']\n",
    "# # check_dfs_for_exp_list(df_controls, df_resample, list_of_exp_to_check=list_of_exp_to_check)\n",
    "\n",
    "\n",
    "# def get_group_texts_tokens(df_small, groupby_col='troll_tweet', group_dict={0:'controls',1:'trolls'}, column='content_stopped'):\n",
    "#     from nltk import regexp_tokenize\n",
    "#     pattern = \"([a-zA-Z]+(?:'[a-z]+)?)\"\n",
    "#     text_dict = {}\n",
    "#     for k,v in group_dict.items():\n",
    "#         group_text_temp = df_small.groupby(groupby_col).get_group(k)[column]\n",
    "#         group_text_temp = ' '.join(group_text_temp)\n",
    "#         group_tokens = regexp_tokenize(group_text_temp, pattern)\n",
    "#         text_dict[v] = {}\n",
    "#         text_dict[v]['tokens'] = group_tokens\n",
    "#         text_dict[v]['text'] =  ' '.join(group_tokens)\n",
    "            \n",
    "#     print(f\"{text_dict.keys()}:['tokens']|['text']\")\n",
    "#     return text_dict\n",
    "\n",
    "\n",
    "\n",
    "# def check_df_groups_for_exp(df_full, list_of_exp_to_check, check_col='content_min_clean', groupby_col='troll_tweet', group_dict={0:'Control',1:'Troll'}):      \n",
    "#     \"\"\"Checks `check_col` column of input dataframe for expressions in list_of_exp_to_check and \n",
    "#     counts the # present for each group, defined by the groupby_col and groupdict. \n",
    "#     Returns a dataframe of counts.\"\"\"\n",
    "    \n",
    "#     list_of_results = []      \n",
    "\n",
    "#     header_list= ['Term']\n",
    "#     [header_list.append(x) for x in group_dict.values()]\n",
    "#     list_of_results.append(header_list)\n",
    "    \n",
    "#     for exp in list_of_exp_to_check:\n",
    "#         curr_exp_list = [exp]\n",
    "        \n",
    "#         for k,v in group_dict.items():\n",
    "#             df_group = df_full.groupby(groupby_col).get_group(k)\n",
    "#             curr_group_count = len(df_group.loc[df_group[check_col].str.contains(exp)])\n",
    "#             curr_exp_list.append(curr_group_count)\n",
    "        \n",
    "#         list_of_results.append(curr_exp_list)\n",
    "        \n",
    "#     df_results = bs.list2df(list_of_results, index_col='Term')\n",
    "#     return df_results\n",
    "\n",
    "\n",
    "# ###########################################################################\n",
    "\n",
    "# def plot_fit_cloud(troll_cloud,contr_cloud,label1='Troll',label2='Control'):\n",
    "#     import matplotlib.pyplot as plt\n",
    "#     fig,ax = plt.subplots(nrows=1,ncols=2,figsize=(18,18))\n",
    "\n",
    "#     ax[0].imshow(troll_cloud, interpolation='gaussian')\n",
    "#     # ax[0].set_aspect(1.5)\n",
    "#     ax[0].axis(\"off\")\n",
    "#     ax[0].set_title(label1, fontsize=40)\n",
    "\n",
    "#     ax[1].imshow(contr_cloud, interpolation='bilinear',)\n",
    "#     # ax[1].set_aspect(1.5)\n",
    "#     ax[1].axis(\"off\")\n",
    "#     ax[1].set_title(label2, fontsize=40)\n",
    "#     plt.tight_layout()\n",
    "#     return fig, ax\n",
    "              \n",
    "              \n",
    "              \n",
    "\n",
    "# ############### TIMESERIES TESTS AND VISUALS ###############\n",
    "\n",
    "# def plot_time_series(stocks_df, freq=None, fill_method='ffill',figsize=(12,4)):\n",
    "    \n",
    "#     df = stocks_df.copy()\n",
    "#     df.fillna(method=fill_method, inplace=True)\n",
    "#     df.dropna(inplace=True)\n",
    "    \n",
    "#     if (df.index.freq==None) & (freq == None):\n",
    "#         xlabels=f'Time'\n",
    "    \n",
    "#     elif (df.index.freq==None) & (freq != None):\n",
    "#         df = df.asfreq(freq)\n",
    "#         df.fillna(method=fill_method, inplace=True)\n",
    "#         df.dropna(inplace=True)\n",
    "#         xlabels=f'Time - Frequency = {freq}'\n",
    "\n",
    "#     else:\n",
    "#         xlabels=f'Time - Frequency = {df.index.freq}'\n",
    "        \n",
    "#     ylabels=\"Price\"\n",
    "\n",
    "#     raw_plot = df.plot(figsize=figsize)\n",
    "#     raw_plot.set_title('Stock Bid Closing Price ')\n",
    "#     raw_plot.set_ylabel(ylabels)\n",
    "#     raw_plot.set_xlabel(xlabels)\n",
    "    \n",
    "    \n",
    "# #################### TIMEINDEX FUNCTIONS #####################\n",
    "# def get_day_window_size_from_freq(dataset):#, freq='CBH'):\n",
    "    \n",
    "#     if dataset.index.freq == custom_BH_freq():\n",
    "#         return 7\n",
    "    \n",
    "#     if dataset.index.freq=='T':\n",
    "#         day_window_size = 1440\n",
    "#     elif dataset.index.freq=='BH':\n",
    "#         day_window_size = 8\n",
    "#     elif dataset.index.freq=='CBH':\n",
    "#         day_window_size = 7\n",
    "#     elif dataset.index.freq=='B':\n",
    "#         day_window_size=1\n",
    "#     elif dataset.index.freq=='D':\n",
    "#         day_window_size=1\n",
    "        \n",
    "#     else:\n",
    "#         raise Exception('dataset freq=None')\n",
    "        \n",
    "#     return day_window_size\n",
    "    \n",
    "\n",
    "# def custom_BH_freq():\n",
    "#     import pandas as pd\n",
    "#     CBH = pd.tseries.offsets.CustomBusinessHour(start='09:30',end='16:30')\n",
    "#     return CBH\n",
    "    \n",
    "    \n",
    "# def  set_timeindex_freq(ive_df, col_to_fill=None, freq='CBH',fill_method='ffill',\n",
    "#                         verbose=3): #set_tz=True,\n",
    "    \n",
    "#     import pandas as pd\n",
    "#     import numpy as np\n",
    "    \n",
    "    \n",
    "#     if verbose>1:\n",
    "#         # print(f\"{'Index When:':>{10}}\\t{'Freq:':>{20}}\\t{'Index Start:':>{40}}\\t{'Index End:':>{40}}\")\n",
    "#         print(f\"{'Index When:'}\\t{'Freq:'}\\t{'Index Start'}\\t\\t{'Index End:'}\")\n",
    "#         print(f\"Pre-Change\\t{ive_df.index.freq}\\t{ive_df.index[0]}\\t{ive_df.index[-1]}\")\n",
    "        \n",
    "    \n",
    "#     if freq=='CBH':\n",
    "#         freq=custom_BH_freq()\n",
    "# #         start_idx = \n",
    "        \n",
    "#     # Change frequency to freq\n",
    "#     ive_df = ive_df.asfreq(freq,)#'min')\n",
    "    \n",
    "#     #     # Set timezone\n",
    "#     #     if set_tz==True:\n",
    "#     #         ive_df.tz_localize()\n",
    "#     #         ive_df.index = ive_df.index.tz_convert('America/New_York')\n",
    "    \n",
    "#     # Report Success / Details\n",
    "#     if verbose>1:\n",
    "#         print(f\"Post-Change\\t{ive_df.index.freq}\\t{ive_df.index[0]}\\t{ive_df.index[-1]}\")\n",
    "\n",
    "\n",
    "#     ## FILL AND TRACK TIMEPOINTS WITH MISSING DATA    \n",
    "    \n",
    "#     # Helper Function for adding column to track the datapoints that were filled\n",
    "#     def check_null_times(x):\n",
    "#         import numpy as np\n",
    "#         if np.isnan(x):\n",
    "#             return True\n",
    "#         else:\n",
    "#             return False\n",
    "\n",
    "#     ## CREATE A COLUMN TO TRACK ROWS TO BE FILLED\n",
    "#     # If col_to_fill provided, use that column to create/judge ive_df['filled_timebin'] \n",
    "#     if col_to_fill!=None:\n",
    "#         ive_df['filled_timebin'] = ive_df[col_to_fill].apply(lambda x: check_null_times(x))#True if ive_df.isna().any()\n",
    "        \n",
    "#     # if not provided, use all columns and sum results\n",
    "#     elif col_to_fill == None:\n",
    "#         # Prefill fol with 0's\n",
    "#         ive_df['filled_timebin']=0\n",
    "        \n",
    "#         # loop through all columns and add results of check_null_times from each loop\n",
    "#     for col in ive_df.columns:\n",
    "#         if ive_df[col].dtypes=='float64':\n",
    "#             #ive_df['filled_timebin'] = ive_df[target_col].apply(lambda x: check_null_times(x))#True if ive_df.isna().any()\n",
    "#             curr_filled_timebin_col = ive_df[col].apply(lambda x: check_null_times(x))#True if ive_df.isna().any() \n",
    "\n",
    "#             # add results\n",
    "#             ive_df['filled_timebin'] +=  curr_filled_timebin_col\n",
    "            \n",
    "#     ive_df['filled_timebin'] = ive_df['filled_timebin'] >0\n",
    "            \n",
    "#     ## FILL IN NULL VALUES\n",
    "#     ive_df.fillna(method=fill_method, inplace=True)\n",
    "\n",
    "#     # Report # filled\n",
    "#     if verbose>0:\n",
    "#         check_fill = ive_df.loc[ive_df['filled_timebin']>0]\n",
    "#         print(f'\\nFilled {len(check_fill==True)}# of rows using method {fill_method}')\n",
    "    \n",
    "#     # Report any remaning null values\n",
    "#     if verbose>0:\n",
    "#         res = ive_df.isna().sum()\n",
    "#         if res.any():\n",
    "#             print(f'Cols with Nulls:')\n",
    "#             print(res[res>0])\n",
    "#         else:\n",
    "#             print('No Remaining Null Values')   \n",
    "            \n",
    "#     # display header\n",
    "#     if verbose>2:\n",
    "#         display(ive_df.head())\n",
    "    \n",
    "#     return ive_df\n",
    "\n",
    "\n",
    "# # Helper Function for adding column to track the datapoints that were filled\n",
    "# def check_null_times(x):\n",
    "#     import numpy as np\n",
    "#     if np.isnan(x):\n",
    "#         return True\n",
    "#     else:\n",
    "#         return False\n",
    "              \n",
    "# ######## SEASONAL DECOMPOSITION    \n",
    "# def plot_decomposition(TS, decomposition, figsize=(12,8),window_used=None):\n",
    "#     \"\"\" Plot the original data and output decomposed components\"\"\"\n",
    "    \n",
    "#     # Gather the trend, seasonality and noise of decomposed object\n",
    "#     trend = decomposition.trend\n",
    "#     seasonal = decomposition.seasonal\n",
    "#     residual = decomposition.resid\n",
    "\n",
    "#     fontdict_axlabels = {'fontsize':12}#,'fontweight':'bold'}\n",
    "    \n",
    "#     # Plot gathered statistics\n",
    "#     fig, ax = plt.subplots(nrows=4, ncols=1,figsize=figsize)\n",
    "    \n",
    "#     ylabel = 'Original'\n",
    "#     ax[0].plot(np.log(TS), color=\"blue\")\n",
    "#     ax[0].set_ylabel(ylabel, fontdict=fontdict_axlabels)\n",
    "    \n",
    "#     ylabel = label='Trend'\n",
    "#     ax[1].plot(trend, color=\"blue\")\n",
    "#     ax[1].set_ylabel(ylabel, fontdict=fontdict_axlabels)\n",
    "    \n",
    "#     ylabel='Seasonality'\n",
    "#     ax[2].plot(seasonal, color=\"blue\")\n",
    "#     ax[2].set_ylabel(ylabel, fontdict=fontdict_axlabels)\n",
    "    \n",
    "#     ylabel='Residuals'\n",
    "#     ax[3].plot(residual, color=\"blue\")\n",
    "#     ax[3].set_ylabel(ylabel, fontdict=fontdict_axlabels)\n",
    "#     ax[3].set_xlabel('Time', fontdict=fontdict_axlabels)\n",
    "    \n",
    "#     # Add title with window \n",
    "#     if window_used == None:\n",
    "#         plt.suptitle('Seasonal Decomposition', y=1.02)\n",
    "#     else:\n",
    "#         plt.suptitle(f'Seasonal Decomposition - Window={window_used}', y=1.02)\n",
    "    \n",
    "#     # Adjust aesthetics\n",
    "#     plt.tight_layout()\n",
    "    \n",
    "#     return ax\n",
    "    \n",
    "    \n",
    "# def seasonal_decompose_and_plot(ive_df,col='BidClose',freq='H',\n",
    "#                           fill_method='ffill',window=144,\n",
    "#                          model='multiplicative', two_sided=False,\n",
    "#                                plot_components=True):##WIP:\n",
    "#     \"\"\"Perform seasonal_decompose from statsmodels.tsa.seasonal.\n",
    "#     Plot Output Decomposed Components\"\"\"\n",
    "#     import pandas as pd\n",
    "#     import numpy as np\n",
    "#     from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "\n",
    "\n",
    "#     # TS = ive_df['BidClose'].asfreq('BH')\n",
    "#     TS = pd.DataFrame(ive_df[col])\n",
    "#     TS = TS.asfreq(freq)\n",
    "#     TS[TS==0]=np.nan\n",
    "#     TS.fillna(method='ffill',inplace=True)\n",
    "\n",
    "#     # Perform decomposition\n",
    "#     decomposition = seasonal_decompose(np.log(TS),freq=window, model=model, two_sided=two_sided)\n",
    "    \n",
    "#     if plot_components==True:\n",
    "#         ax = plot_decomposition(TS, decomposition, window_used=window)\n",
    "    \n",
    "#     return decomposition\n",
    "\n",
    "\n",
    "\n",
    "# def stationarity_check(df, col='BidClose',freq=None, day_window_size='infer',\n",
    "#                        days_in_rolling_window=5):\n",
    "#     \"\"\"From learn.co lesson: use ADFuller Test for Stationary and Plot\"\"\"\n",
    "    \n",
    "#     import matplotlib.pyplot as plt\n",
    "#     TS = df[col].copy()\n",
    "    \n",
    "#     if freq==None:\n",
    "#         freq=df.index.freq\n",
    "        \n",
    "#     else:        \n",
    "#         TS = TS.asfreq(freq)\n",
    "#         TS.fillna(method='ffill',inplace=True)\n",
    "#         TS.dropna(inplace=True)\n",
    "    \n",
    "    \n",
    "#     # Import adfuller\n",
    "#     from statsmodels.tsa.stattools import adfuller\n",
    "#     import pandas as pd\n",
    "#     import numpy as np\n",
    "    \n",
    "#     # Calculate rolling statistics\n",
    "#     if day_window_size=='infer':\n",
    "#         day_window_size = get_day_window_size_from_freq(TS)\n",
    "        \n",
    "#     window = day_window_size*days_in_rolling_window\n",
    "#     rolmean = TS.rolling(window = window, center = False).mean()\n",
    "#     rolstd = TS.rolling(window = window, center = False).std()\n",
    "    \n",
    "#     # Perform the Dickey Fuller Test\n",
    "#     dftest = adfuller(TS) # change the passengers column as required \n",
    "    \n",
    "#     #Plot rolling statistics:\n",
    "#     fig, ax = plt.subplots(nrows=2, ncols=1, figsize=(8,4))\n",
    "#     ax[0].set_title('Rolling Mean & Standard Deviation')\n",
    "\n",
    "#     ax[0].plot(TS, color='blue',label='Original')\n",
    "#     ax[0].plot(rolmean, color='red', label='Rolling Mean',alpha =0.6)\n",
    "#     ax[1].plot(rolstd, color='black', label = 'Rolling Std')\n",
    "#     ax[0].legend()\n",
    "#     ax[1].legend()\n",
    "# #     plt.show(block=False)\n",
    "#     plt.tight_layout()\n",
    "    \n",
    "#     # Print Dickey-Fuller test results\n",
    "#     print ('Results of Dickey-Fuller Test:')\n",
    "#     print('\\tIf p<.05 then timeseries IS stationary.')\n",
    "\n",
    "#     dfoutput = pd.Series(dftest[0:4], index=['Test Statistic','p-value','#Lags Used','Number of Observations Used'])\n",
    "#     for key,value in dftest[4].items():\n",
    "#         dfoutput['Critical Value (%s)'%key] = value\n",
    "#     print (dfoutput)\n",
    "    \n",
    "#     return None\n",
    "              \n",
    "              \n",
    "# ##################### DATASET LOADING FUNCTIONS #####################   \n",
    "# def load_raw_stock_data_from_txt(filename='IVE_bidask1min.txt', \n",
    "#                                folderpath='data/',\n",
    "#                                start_index = '2017-01-23',\n",
    "#                                  clean=False,fill_method='ffill',\n",
    "#                                  freq='CBH',verbose=2):\n",
    "#     import pandas as pd\n",
    "    \n",
    "#     # Load in the text file and set headers\n",
    "#     fullfilename= folderpath+filename\n",
    "#     headers = ['Date','Time','BidOpen','BidHigh','BidLow','BidClose','AskOpen','AskHigh','AskLow','AskClose']\n",
    "#     stock_df = pd.read_csv(fullfilename, names=headers,parse_dates=True)\n",
    "    \n",
    "#     # Create datetime index\n",
    "#     date_time_index = stock_df['Date']+' '+stock_df['Time']\n",
    "#     date_time_index = pd.to_datetime(date_time_index)\n",
    "#     stock_df.index=date_time_index\n",
    "    \n",
    "#     # Select only the days after start_index\n",
    "#     stock_df = stock_df[start_index:]\n",
    "    \n",
    "#     # Remove 0's from BidClose\n",
    "#     if clean==True:\n",
    "        \n",
    "#         stock_df.loc[stock_df['BidClose']==0] = np.nan\n",
    "#         stock_df['BidClose'].fillna(method=fill_method, inplace=True)\n",
    "        \n",
    "#         if verbose>0:\n",
    "#             print(f\"Number of 0 values:\\n{len(stock_df.loc[stock_df['BidClose']==0])}\")\n",
    "#             print(f\"Filling 0 values using method = {fill_method}\")\n",
    "            \n",
    "\n",
    "\n",
    "                  \n",
    "#     # call set_timeindex_freq to specify proper frequency\n",
    "#     if freq!=None:\n",
    "#         # Set the time index .\n",
    "#         stock_df = set_timeindex_freq(stock_df, freq=freq, fill_method = fill_method, verbose=verbose)\n",
    "                  \n",
    "#     # Display feedback\n",
    "#     if verbose>0:\n",
    "#         display(stock_df.head())\n",
    "#     if verbose>1:\n",
    "#         print(stock_df.index)\n",
    "\n",
    "#     return stock_df\n",
    "\n",
    "\n",
    "\n",
    "# # def load_stock_df_from_csv(filename='ive_sp500_min_data_match_twitter_ts.csv',\n",
    "# #                            folderpath='data/',\n",
    "# #                           start_index = '2017-01-23', clean=False,freq='CBH',\n",
    "# #                            fill_method='ffill',verbose=2):\n",
    "# #     import os\n",
    "# #     import pandas as pd\n",
    "\n",
    "# #     #         check_for_google_drive()\n",
    "        \n",
    "# #     # Check if user provided folderpath to append to filename\n",
    "# #     if len(folderpath)>0:\n",
    "# #         fullfilename = folderpath+filename\n",
    "# #     else:\n",
    "# #         fullfilename=filename\n",
    "        \n",
    "# #     # load in csv by fullfilename\n",
    "# #     stock_df = pd.read_csv(fullfilename,index_col=0, parse_dates=True)\n",
    "# #     stock_df = stock_df[start_index:]\n",
    "# # #     stock_df = set_timeindex_freq(stock_df,['BidClose'],freq=freq, fill_method=fill_method)\n",
    "    \n",
    "# #     if clean==True:\n",
    "        \n",
    "# #         if verbose>0:\n",
    "# #             print(f\"Number of 0 values:\\n{len(stock_df.loc[stock_df['BidClose']==0])}\")\n",
    "# #             print(f\"Filling 0 values using method = {fill_method}\")\n",
    "            \n",
    "# #         stock_df.loc[stock_df['BidClose']==0] = np.nan\n",
    "# #         stock_df['BidClose'].fillna(method=fill_method, inplace=True)\n",
    "        \n",
    "# #     if freq!=None:\n",
    "# #         # Set the time index .\n",
    "# #         stock_df = set_timeindex_freq(stock_df, freq=freq, fill_method = fill_method, verbose=verbose)\n",
    "        \n",
    "\n",
    "# #     # Display info depending on verbose level\n",
    "# #     if verbose>0:\n",
    "# #         display(stock_df.head())\n",
    "    \n",
    "# #     if verbose>1:\n",
    "# #         print(stock_df.index)\n",
    "        \n",
    "# #     return stock_df   \n",
    "                  \n",
    "# def get_technical_indicators(dataset,make_price_from='BidClose'):\n",
    "    \n",
    "\n",
    "#     dataset['price'] = dataset[make_price_from].copy()\n",
    "#     if dataset.index.freq == custom_BH_freq():\n",
    "#         days = get_day_window_size_from_freq(dataset)#,freq='CBH')\n",
    "#     else:\n",
    "#         days = get_day_window_size_from_freq(dataset)\n",
    "        \n",
    "#     # Create 7 and 21 days Moving Average\n",
    "#     dataset['ma7'] = dataset['price'].rolling(window=7*days).mean()\n",
    "#     dataset['ma21'] = dataset['price'].rolling(window=21*days).mean()\n",
    "    \n",
    "#     # Create MACD\n",
    "#     dataset['26ema'] = dataset['price'].ewm(span=26*days).mean()\n",
    "# #     dataset['12ema'] = pd.ewma(dataset['price'], span=12)\n",
    "#     dataset['12ema'] = dataset['price'].ewm(span=12*days).mean()\n",
    "\n",
    "#     dataset['MACD'] = (dataset['12ema']-dataset['26ema'])\n",
    "\n",
    "#     # Create Bollinger Bands\n",
    "# #     dataset['20sd'] = pd.stats.moments.rolling_std(dataset['price'],20)\n",
    "#     dataset['20sd'] = dataset['price'].rolling(20*days).std()\n",
    "#     dataset['upper_band'] = dataset['ma21'] + (dataset['20sd']*2)\n",
    "#     dataset['lower_band'] = dataset['ma21'] - (dataset['20sd']*2)\n",
    "    \n",
    "#     # Create Exponential moving average\n",
    "#     dataset['ema'] = dataset['price'].ewm(com=0.5).mean()\n",
    "    \n",
    "#     # Create Momentum\n",
    "#     dataset['momentum'] = dataset['price']-days*1\n",
    "    \n",
    "#     return dataset\n",
    "\n",
    "                  \n",
    "# def plot_technical_indicators(dataset, last_days=90):\n",
    "   \n",
    "#     days = get_day_window_size_from_freq(dataset)\n",
    "    \n",
    "#     fig, ax = plt.subplots(nrows=2, ncols=1,figsize=(10, 6), dpi=100)\n",
    "# #     shape_0 = dataset.shape[0]\n",
    "# #     xmacd_ = shape_0-(days*last_days)\n",
    "    \n",
    "#     dataset = dataset.iloc[-(days*last_days):, :]\n",
    "#     x_ = range(3, dataset.shape[0])\n",
    "#     x_ =list(dataset.index)\n",
    "    \n",
    "#     # Plot first subplot\n",
    "#     ax[0].plot(dataset['ma7'],label='MA 7', color='g',linestyle='--')\n",
    "#     ax[0].plot(dataset['price'],label='Closing Price', color='b')\n",
    "#     ax[0].plot(dataset['ma21'],label='MA 21', color='r',linestyle='--')\n",
    "#     ax[0].plot(dataset['upper_band'],label='Upper Band', color='c')\n",
    "#     ax[0].plot(dataset['lower_band'],label='Lower Band', color='c')\n",
    "#     ax[0].fill_between(x_, dataset['lower_band'], dataset['upper_band'], alpha=0.35)\n",
    "#     ax[0].set_title('Technical indicators for Goldman Sachs - last {} days.'.format(last_days))\n",
    "#     ax[0].set_ylabel('USD')\n",
    "#     ax[0].legend()\n",
    "\n",
    "# #     shape_0 = dataset.shape[0]\n",
    "# #     xmacd_ = shape_0-(days*last_days)\n",
    "# #     # Plot second subplot\n",
    "# #     ax[1].set_title('MACD')\n",
    "# #     ax[1].plot(dataset['MACD'],label='MACD', linestyle='-.')\n",
    "# #     ax[1].hlines(15, xmacd_, shape_0, colors='g', linestyles='--')\n",
    "# #     ax[1].hlines(-15, xmacd_, shape_0, colors='g', linestyles='--')\n",
    "# #     ax[1].plot(dataset['momentum'],label='Momentum', color='b',linestyle='-')\n",
    "\n",
    "# #     ax[1].legend()\n",
    "#     plt.delaxes(ax[1])\n",
    "#     plt.show()\n",
    "    \n",
    "# # plot_technical_indicators(stock_df)#.filter(regex='Bid'))  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# def train_test_split_by_last_days(stock_df, periods_per_day=7,num_test_days = 90, num_train_days=180, plot=True):\n",
    "#     \"\"\"Takes the last num_test_days of the time index to use as testing data, and take shte num_Trian_days prior to that date\n",
    "#     as the training data.\"\"\"\n",
    "#     # DETERMINING DAY TO USE TO SPLIT DATA INTO TRAIN AND TEST\n",
    "#     day_freq = periods_per_day\n",
    "#     start_train_day = stock_df.index[-1] - num_train_days*day_freq\n",
    "#     last_day = stock_df.index[-1] - num_test_days*day_freq\n",
    "#     print(f'Dates for Train Data:{start_train_day} : {last_day}.')\n",
    "#     print(f'Dates for Test Data:{last_day} : {stock_df.index[-1]}.')\n",
    "#     train_data = stock_df.loc[start_train_day:last_day]#,'price']\n",
    "#     test_data = stock_df.loc[last_day:]#,'price']\n",
    "#     print(f'Data split on index:{last_day}.')\n",
    "#     print(f'train_data.shape:{train_data.shape}')\n",
    "#     print(f'test_data.shape:{test_data.shape}')\n",
    "\n",
    "\n",
    "#     if plot==True:\n",
    "#         if 'price' in stock_df.columns:\n",
    "#             plot_col ='price'\n",
    "#         elif 'price_labels' in stock_df.columns:\n",
    "#             plot_col = 'price_labels'\n",
    "        \n",
    "#         train_data[plot_col].plot(label='Training')\n",
    "#         test_data[plot_col].plot(label='Test')\n",
    "#         plt.title('Training and Test Data for S&P500')\n",
    "#         plt.ylabel('Price')\n",
    "#         plt.xlabel('Trading Date/Hour')\n",
    "#         plt.legend();\n",
    "    \n",
    "#     return train_data, test_data\n",
    "\n",
    "# # train_data, test_data = train_test_split_by_last_days(stock_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# # display(stock_df.head())\n",
    "# # df_stock_sca, scaler_library = make_scaler_library(stock_df,transform=True)\n",
    "# # display(df_stock_sca.head())\n",
    "\n",
    "# # df_stock_inv = transform_cols_from_library(df_stock_sca,scaler_library,inverse=True)\n",
    "# # display(df_stock_inv.head())\n",
    "\n",
    "# def make_X_y_timeseries_data(data,x_window = 35, verbose=2,as_array=True):\n",
    "#     \"\"\"Takes a Series and creates X_train, y_train where each element of X contains x_window # of timepoints\n",
    "#     and y_train contains one single timepoint following X_train's timepoint\"\"\"\n",
    "#     import numpy as np\n",
    "#     import pandas as pd\n",
    "                          \n",
    "    \n",
    "#     if any(data.isna()):\n",
    "#         raise Exception('Function does not accept null values')\n",
    "        \n",
    "#     # Display info on data\n",
    "#     if verbose>0:\n",
    "#         print(f'Input Range: {np.min(data)} - {np.max(data)}')\n",
    "#         print(f'Input Shape: {np.shape(data)}\\n')\n",
    "\n",
    "#     time_index_in = data.index\n",
    "#     time_index = data.index[x_window:]\n",
    "    \n",
    "#     X_train, y_train = [], []\n",
    "#     check_time_index = []\n",
    "# #     headers = data.columns\n",
    "#     # for every possible x_window:\n",
    "#     for i in range(x_window, data.shape[0]):\n",
    "#         check_time_index.append([data.index[i-x_window], data.index[i]])\n",
    "#         # Append a list of the past x_window # of timepoints\n",
    "#         X_train.append(data.iloc[i-x_window:i])#.values)\n",
    "        \n",
    "#         # Append the next single timepoint's data\n",
    "#         y_train.append(data.iloc[i])#.values)\n",
    "    \n",
    "#     if as_array == True:\n",
    "#         # Make X_train, y_train into arrays\n",
    "#         X_train, y_train = np.array(X_train), np.array(y_train)\n",
    "    \n",
    "\n",
    "#     if verbose>0:\n",
    "#         print(f'\\nOutput Shape - X: {X_train.shape}')\n",
    "#         print(f'Output Shape - y: {y_train.shape}')\n",
    "#         print(f'\\nTimeindex Shape: {np.shape(time_index)}\\n\\tRange: {time_index[0]}-{time_index[-1]}')\n",
    "#         print(f'\\tFrequency:',time_index.freq)\n",
    "# #     print(time_index)\n",
    "# #     print(check_time_index)\n",
    "#     return X_train, y_train, time_index\n",
    "\n",
    "\n",
    "# def make_df_timeseries_bins_by_column(df, x_window = 35, verbose=2,one_or_two_dfs = 1): #target_col='price',\n",
    "#     \"\"\" Function will take each column from the dataframe and create a train_data dataset  (with X and Y data), with\n",
    "#     each row in X containing x_window number of observations and y containing the next following observation\"\"\"\n",
    "\n",
    "#     col_data  = {}\n",
    "#     time_index_for_df = []\n",
    "#     for col in df.columns:\n",
    "        \n",
    "#         col_data[col] = {}\n",
    "#         col_bins, col_labels, col_idx =  make_X_y_timeseries_data(df[col], verbose=0, as_array=True)#,axis=0)\n",
    "# #         print(f'col_bins dtype={type(col_bins)}')\n",
    "# #         print(f'col_labels dtype={type(col_labels)}')\n",
    "        \n",
    "#         ## ALTERNATIVE IS TO PLACE DF COLUMNS CREATION ABOVE HERE\n",
    "#         col_data[col]['bins']=col_bins\n",
    "#         col_data[col]['labels'] = col_labels\n",
    "# #         col_data[col]['index'] = col_idx\n",
    "#         time_index_for_df = col_idx\n",
    "    \n",
    "#     # Convert the dictionaries into a dataframe\n",
    "#     df_timeseries_bins = pd.DataFrame(index=time_index_for_df)\n",
    "# #     df_timeseries_bins.index=time_index_for_df\n",
    "# #     print(time_index_for_df)\n",
    "#     # for each original column\n",
    "#     for colname,data_dict in col_data.items():\n",
    "        \n",
    "#         #for column's new data bins,labels\n",
    "#         for data_col, X in col_data[colname].items():\n",
    "            \n",
    "#             # new column title\n",
    "#             new_colname = colname+'_'+data_col\n",
    "# #             print(new_colname)\n",
    "#             make_col = []\n",
    "#             if data_col=='labels':\n",
    "#                 df_timeseries_bins[new_colname] = col_data[colname][data_col]\n",
    "#             else:\n",
    "#                 # turn array of lists into list of arrays\n",
    "#                 for x in range(X.shape[0]):\n",
    "#                     x_data = np.array(X[x])\n",
    "# #                     x_data = X[x]\n",
    "#                     make_col.append(x_data)\n",
    "#                 # fill in column's data\n",
    "#                 df_timeseries_bins[new_colname] = make_col\n",
    "                \n",
    "# #     print(df_timeseries_bins.index)\n",
    "# #     print(time_index_for_df)\n",
    "        \n",
    "    \n",
    "#     if one_or_two_dfs==1:\n",
    "#         return df_timeseries_bins\n",
    "    \n",
    "#     elif one_or_two_dfs==2:\n",
    "#         df_bins = df_timeseries_bins.filter(regex=('bins'))\n",
    "#         df_labels = df_timeseries_bins.filter(regex=('labels'))\n",
    "        \n",
    "#     return df_bins, df_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# def make_scaler_library(df,transform=False,columns=[]):\n",
    "#     from sklearn.preprocessing import MinMaxScaler\n",
    "#     scaler_dict = {}\n",
    "#     scaler_dict['index'] = df.index\n",
    "#     if len(columns)==0:\n",
    "#         user_cols = []\n",
    "#         columns = df.columns\n",
    "#     for col in columns:\n",
    "#         user_cols=columns\n",
    "#         scaler = MinMaxScaler()\n",
    "#         scaler.fit(df[col].values.reshape(-1,1))\n",
    "#         scaler_dict[col] = scaler \n",
    "        \n",
    "#     if transform==False:\n",
    "#         return scaler_dict\n",
    "    \n",
    "#     elif transform==True:\n",
    "#         df_out = transform_cols_from_library(df, scaler_dict,columns=user_cols)\n",
    "#         return df_out, scaler_dict\n",
    "    \n",
    "# def transform_cols_from_library(df,scaler_library,inverse=False,columns=[]):\n",
    "    \n",
    "#     df_out = df.copy()\n",
    "    \n",
    "#     if len(columns)==0:\n",
    "#         columns = df.columns\n",
    "#     for col in columns:\n",
    "#         scaler = scaler_library[col]\n",
    "#         if inverse==False:\n",
    "#             scaled_col = scaler.transform(df[col].values.reshape(-1,1))\n",
    "#         elif inverse==True:\n",
    "#             scaled_col = scaler.inverse_transform(df[col].values.reshape(-1,1))\n",
    "#         df_out[col] = scaled_col.ravel()\n",
    "#     return df_out\n",
    "\n",
    "# def inverse_transform_series(df, scaler_library,columns=[] ,scaler_colname='price'):\n",
    "#     train_price_index = df.index\n",
    "# #     train_price = scaler_library['price'].inverse_transform(df_train_bins['price_labels'].values.reshape(-1,1))\n",
    "#     scaler = scaler_library[scaler_colname]\n",
    "    \n",
    "#     if len(columns)==0:\n",
    "#         columns = df.columns\n",
    "        \n",
    "#     for col in columns:\n",
    "#         if 'bins' in col:\n",
    "#             print('skipping binned column: ',col)\n",
    "#         else:\n",
    "#             price_to_change = df[col].values\n",
    "#             price_tf = scaler.inverse_transform(price_to_change.reshape(-1,1))\n",
    "#             df[col]  = pd.Series(price_tf.ravel(), index=train_price_index)\n",
    "#     return df\n",
    "\n",
    "\n",
    "# df_train_bins_plot = inverse_transform_series(df_train_bins, scaler_library)\n",
    "# # plot_me.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TWITTER DATA: Trying PipeLine NLP + Param Grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------------------------------------------------------------------\n",
      "--------- \tHELP:\t ------------------------------------------------------------------------------------------\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Help on function load_twitter_df in module functions_combined_BEST:\n",
      "\n",
      "load_twitter_df(overwrite=True, set_index='time_index', verbose=2, replace_na='')\n",
      "\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "--------- \tSOURCE:\t ------------------------------------------------------------------------------------------\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "def load_twitter_df(overwrite=True,set_index='time_index',verbose=2,replace_na=''):\n",
      "    import pandas as pd\n",
      "    from IPython.display import display\n",
      "    try: twitter_df\n",
      "    except NameError: twitter_df = None\n",
      "    if twitter_df is not None:\n",
      "        print('twitter_df already exists.')\n",
      "        if overwrite==True:\n",
      "            print('Overwrite=True. deleting original...')\n",
      "            del(twitter_df)\n",
      "            \n",
      "    if twitter_df is None:\n",
      "        print('loading twitter_df')\n",
      "        \n",
      "        twitter_df = pd.read_csv('data/trump_twitter_archive_df.csv', encoding='utf-8', parse_dates=True)\n",
      "        twitter_df.drop('Unnamed: 0',axis=1,inplace=True)\n",
      "\n",
      "        twitter_df['date']  = pd.to_datetime(twitter_df['date'])\n",
      "        twitter_df['time_index'] = twitter_df['date'].copy()\n",
      "        twitter_df.set_index(set_index,inplace=True,drop=True)\n",
      "\n",
      "\n",
      "        # Fill in missing values before merging with stock data\n",
      "        twitter_df.fillna(replace_na, inplace=True)\n",
      "        twitter_df.sort_index(ascending=True, inplace=True)\n",
      "\n",
      "        # RECASTING A COUPLE COLUMNS\n",
      "        twitter_df['is_retweet'] = twitter_df['is_retweet'].astype('bool')\n",
      "        twitter_df['id_str'] = twitter_df['id_str'].astype('str')\n",
      "        twitter_df['sentiment_class'] = twitter_df['sentiment_class'].astype('category')\n",
      "\n",
      "#         twitter_df.reset_index(inplace=True)\n",
      "        # Check header and daterange of index\n",
      "    if verbose>0:\n",
      "        display(twitter_df.head(2))\n",
      "        print(twitter_df.index[[0,-1]])\n",
      "    return twitter_df\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from functions_combined_BEST import *\n",
    "ihelp(load_twitter_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source</th>\n",
       "      <th>content</th>\n",
       "      <th>date</th>\n",
       "      <th>retweet_count</th>\n",
       "      <th>favorite_count</th>\n",
       "      <th>is_retweet</th>\n",
       "      <th>id_str</th>\n",
       "      <th>has_RT</th>\n",
       "      <th>starts_RT</th>\n",
       "      <th>content_starts_RT</th>\n",
       "      <th>final_content</th>\n",
       "      <th>content_min_clean</th>\n",
       "      <th>content_hashtags</th>\n",
       "      <th>hashtag_strings</th>\n",
       "      <th>content_mentions</th>\n",
       "      <th>mention_strings</th>\n",
       "      <th>final_content_stop</th>\n",
       "      <th>final_content_stop_tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Twitter for iPhone</td>\n",
       "      <td>Since Election Day 2016 Stocks up almost 50% S...</td>\n",
       "      <td>2019-06-20 00:12:31</td>\n",
       "      <td>16742</td>\n",
       "      <td>64362</td>\n",
       "      <td>False</td>\n",
       "      <td>1141499029727121408</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>[]</td>\n",
       "      <td>Since Election Day 2016 Stocks up almost 50% S...</td>\n",
       "      <td>Since Election Day 2016 Stocks up almost 50% S...</td>\n",
       "      <td>[]</td>\n",
       "      <td></td>\n",
       "      <td>[@LouDobbs, @TuckerCarlson, @seanhannity, @Ing...</td>\n",
       "      <td>@LouDobbs @TuckerCarlson @seanhannity @Ingraha...</td>\n",
       "      <td>since election day stocks almost stocks gained...</td>\n",
       "      <td>[since, election, day, stocks, almost, stocks,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Twitter for iPhone</td>\n",
       "      <td>Congratulations to President Lopez Obrador — M...</td>\n",
       "      <td>2019-06-19 23:01:59</td>\n",
       "      <td>20490</td>\n",
       "      <td>87423</td>\n",
       "      <td>False</td>\n",
       "      <td>1141481280653209600</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>[]</td>\n",
       "      <td>Congratulations to President Lopez Obrador — M...</td>\n",
       "      <td>Congratulations to President Lopez Obrador — M...</td>\n",
       "      <td>[]</td>\n",
       "      <td></td>\n",
       "      <td>[]</td>\n",
       "      <td></td>\n",
       "      <td>congratulations president lopez obrador mexico...</td>\n",
       "      <td>[congratulations, president, lopez, obrador, m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Twitter for iPhone</td>\n",
       "      <td>Just returning from Orlando and Doral (Miami) ...</td>\n",
       "      <td>2019-06-19 20:39:28</td>\n",
       "      <td>12693</td>\n",
       "      <td>68118</td>\n",
       "      <td>False</td>\n",
       "      <td>1141445414824136704</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>[]</td>\n",
       "      <td>Just returning from Orlando and Doral (Miami) ...</td>\n",
       "      <td>Just returning from Orlando and Doral (Miami) ...</td>\n",
       "      <td>[]</td>\n",
       "      <td></td>\n",
       "      <td>[]</td>\n",
       "      <td></td>\n",
       "      <td>returning orlando doral miami florida heading ...</td>\n",
       "      <td>[returning, orlando, doral, miami, florida, he...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Twitter for iPhone</td>\n",
       "      <td>I will be interviewed LIVE tonight by @seanhan...</td>\n",
       "      <td>2019-06-19 20:29:24</td>\n",
       "      <td>10586</td>\n",
       "      <td>48786</td>\n",
       "      <td>False</td>\n",
       "      <td>1141442879518322689</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>[]</td>\n",
       "      <td>I will be interviewed LIVE tonight by   on    ...</td>\n",
       "      <td>I will be interviewed LIVE tonight by @seanhan...</td>\n",
       "      <td>[]</td>\n",
       "      <td></td>\n",
       "      <td>[@seanhannity, @FoxNews]</td>\n",
       "      <td>@seanhannity @FoxNews</td>\n",
       "      <td>interviewed live tonight p enjoy</td>\n",
       "      <td>[interviewed, live, tonight, p, enjoy]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Twitter for iPhone</td>\n",
       "      <td>....asking Hillary Clinton why she deleted and...</td>\n",
       "      <td>2019-06-19 19:48:04</td>\n",
       "      <td>21712</td>\n",
       "      <td>93137</td>\n",
       "      <td>False</td>\n",
       "      <td>1141432478491717632</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>[]</td>\n",
       "      <td>....asking Hillary Clinton why she deleted and...</td>\n",
       "      <td>....asking Hillary Clinton why she deleted and...</td>\n",
       "      <td>[]</td>\n",
       "      <td></td>\n",
       "      <td>[]</td>\n",
       "      <td></td>\n",
       "      <td>asking hillary clinton deleted acid washed ema...</td>\n",
       "      <td>[asking, hillary, clinton, deleted, acid, wash...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               source                                            content  \\\n",
       "0  Twitter for iPhone  Since Election Day 2016 Stocks up almost 50% S...   \n",
       "1  Twitter for iPhone  Congratulations to President Lopez Obrador — M...   \n",
       "2  Twitter for iPhone  Just returning from Orlando and Doral (Miami) ...   \n",
       "3  Twitter for iPhone  I will be interviewed LIVE tonight by @seanhan...   \n",
       "4  Twitter for iPhone  ....asking Hillary Clinton why she deleted and...   \n",
       "\n",
       "                 date  retweet_count  favorite_count is_retweet  \\\n",
       "0 2019-06-20 00:12:31          16742           64362      False   \n",
       "1 2019-06-19 23:01:59          20490           87423      False   \n",
       "2 2019-06-19 20:39:28          12693           68118      False   \n",
       "3 2019-06-19 20:29:24          10586           48786      False   \n",
       "4 2019-06-19 19:48:04          21712           93137      False   \n",
       "\n",
       "                id_str  has_RT  starts_RT content_starts_RT  \\\n",
       "0  1141499029727121408   False      False                []   \n",
       "1  1141481280653209600   False      False                []   \n",
       "2  1141445414824136704   False      False                []   \n",
       "3  1141442879518322689   False      False                []   \n",
       "4  1141432478491717632   False      False                []   \n",
       "\n",
       "                                       final_content  \\\n",
       "0  Since Election Day 2016 Stocks up almost 50% S...   \n",
       "1  Congratulations to President Lopez Obrador — M...   \n",
       "2  Just returning from Orlando and Doral (Miami) ...   \n",
       "3  I will be interviewed LIVE tonight by   on    ...   \n",
       "4  ....asking Hillary Clinton why she deleted and...   \n",
       "\n",
       "                                   content_min_clean content_hashtags  \\\n",
       "0  Since Election Day 2016 Stocks up almost 50% S...               []   \n",
       "1  Congratulations to President Lopez Obrador — M...               []   \n",
       "2  Just returning from Orlando and Doral (Miami) ...               []   \n",
       "3  I will be interviewed LIVE tonight by @seanhan...               []   \n",
       "4  ....asking Hillary Clinton why she deleted and...               []   \n",
       "\n",
       "  hashtag_strings                                   content_mentions  \\\n",
       "0                  [@LouDobbs, @TuckerCarlson, @seanhannity, @Ing...   \n",
       "1                                                                 []   \n",
       "2                                                                 []   \n",
       "3                                           [@seanhannity, @FoxNews]   \n",
       "4                                                                 []   \n",
       "\n",
       "                                     mention_strings  \\\n",
       "0  @LouDobbs @TuckerCarlson @seanhannity @Ingraha...   \n",
       "1                                                      \n",
       "2                                                      \n",
       "3                              @seanhannity @FoxNews   \n",
       "4                                                      \n",
       "\n",
       "                                  final_content_stop  \\\n",
       "0  since election day stocks almost stocks gained...   \n",
       "1  congratulations president lopez obrador mexico...   \n",
       "2  returning orlando doral miami florida heading ...   \n",
       "3                   interviewed live tonight p enjoy   \n",
       "4  asking hillary clinton deleted acid washed ema...   \n",
       "\n",
       "                           final_content_stop_tokens  \n",
       "0  [since, election, day, stocks, almost, stocks,...  \n",
       "1  [congratulations, president, lopez, obrador, m...  \n",
       "2  [returning, orlando, doral, miami, florida, he...  \n",
       "3             [interviewed, live, tonight, p, enjoy]  \n",
       "4  [asking, hillary, clinton, deleted, acid, wash...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "# from functions_combined_BEST import *\n",
    "def make_my_data():\n",
    "    \"\"\"Performs all text processing done in other notebook as part of NLP\"\"\"\n",
    "    from functions_combined_BEST import full_twitter_df_processing, load_raw_twitter_file\n",
    "    df = load_raw_twitter_file()\n",
    "#     file = '../trump_tweets_01202017_06202019.csv'\n",
    "#     df = pd.read_csv(file, encoding='utf-8')\n",
    "#     df.rename(axis=1,mapper={'text':'content','created_at':'date'},inplace=True)\n",
    "#     df['date']=pd.to_datetime(df['date'])\n",
    "# #     df.head()\n",
    "\n",
    "    df = full_twitter_df_processing(df,'content','final_content',force=True)\n",
    "    return df\n",
    "    \n",
    "df = make_my_data()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# http://queirozf.com/entries/scikit-learn-pipeline-examples\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "X_train, y_train, X_test, y_test = make_my_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PREPARING THE STOCK DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "\n",
    "stock_df = pd.read_csv('data/stock_df_with_tech_indicators.csv', index_col=0, parse_dates=True)\n",
    "stock_df = set_timeindex_freq(stock_df,verbose=0)\n",
    "stock_df = stock_df.iloc[:,11:]\n",
    "\n",
    "# DIsply input stock data\n",
    "display(stock_df.head().style.set_caption('Raw Data'))\n",
    "plot_time_series(stock_df['price'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# scale the data using MinMaxScaler\n",
    "stock_df_orig = stock_df.copy()\n",
    "stock_df, scaler_library = make_scaler_library(stock_df_orig,transform=True)\n",
    "display(stock_df.head(2).style.set_caption('MinMaxScaled'))\n",
    "\n",
    "\n",
    "# Create binned columns with x_window # of sequential values, label column with next value in sequence\n",
    "stock_df_binned = make_df_timeseries_bins_by_column(stock_df, x_window=35, verbose=0)#.iloc[:,11:])\n",
    "# display(stock_df_binned.head(2).style.set_caption('Binned for Neural Network'))\n",
    "\n",
    "# Split the training and test data by number of days\n",
    "df_train_bins, df_test_bins = train_test_split_by_last_days(stock_df_binned,periods_per_day=7,\n",
    "                                                            num_test_days=90,num_train_days=180,\n",
    "                                                            plot=False)\n",
    "display(df_train_bins.head(2).style.set_caption('Training Data Binned for Neural Network'))\n",
    "# display(df_test_bins.head(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating X,y and index for training data and test data\n",
    "index_train =df_train_bins.index\n",
    "X_train = df_train_bins['price_bins'].values\n",
    "y_train = df_train_bins['price_labels'].values\n",
    "\n",
    "index_test = df_test_bins.index\n",
    "X_test = df_test_bins['price_bins'].values\n",
    "y_test = df_test_bins['price_labels'].values\n",
    "\n",
    "\n",
    "# Converting all X and y into 2D arrays\n",
    "X_train_stack =  np.vstack(X_train)\n",
    "X_train_in = np.reshape(X_train_stack,(X_train_stack.shape[0],X_train_stack.shape[1],1))\n",
    "\n",
    "X_test_stack =  np.vstack(X_test)\n",
    "X_test_in = np.reshape(X_test_stack,(X_test_stack.shape[0],X_test_stack.shape[1],1))\n",
    "\n",
    "# Displaying shapes of input\n",
    "print(X_train_stack.shape, y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import models, layers\n",
    "model = models.Sequential()\n",
    "\n",
    "model.add(layers.LSTM(units=50, return_sequences=True, input_shape = (X_train_in.shape[1],1)))\n",
    "model.add(layers.LSTM(units=50))\n",
    "model.add(layers.Dense(1))\n",
    "\n",
    "model.compile(loss='mean_squared_error', optimizer='adam',metrics=['acc'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(X_train_in,y_train, epochs=10, verbose=1)\n",
    "\n",
    "# Get predictions and combine with true price\n",
    "predictions = model.predict(X_test_in)\n",
    "predictions = pd.Series(predictions.ravel(),name='predicted_price',index=index_test)\n",
    "true_price =  pd.Series(y_test,name='true_price',index=index_test)\n",
    "\n",
    "df_predictions = pd.concat([predictions,true_price],axis=1)#, columns=['predicted_price','true_price'], index=index_test)\n",
    "display(df_predictions.head())\n",
    "\n",
    "# Plot outcome\n",
    "mpl.rcParams['figure.figsize']=(12,4)\n",
    "plt.plot(df_train_bins['price_labels'])\n",
    "plt.plot(df_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_predictions = pd.concat([df_predictions, pd.Series(y_test,name='price',index=index_test)],axis=1)\n",
    "# display(df_predictions.head())\n",
    "# df_predictions.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_train_bins_plot = inverse_transform_series(df_train_bins, scaler_library)\n",
    "\n",
    "# price_predictions = inverse_transform_series(df_predictions, scaler_library)\n",
    "# fig,ax = plt.subplots()\n",
    "# df_train_bins_plot['price_labels'].plot(ax=ax)\n",
    "# price_predictions.plot(ax=ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display(df_predictions.head())\n",
    "# df_predictions.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "learn-env-ext",
   "language": "python",
   "name": "learn-env-ext"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": "25"
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
