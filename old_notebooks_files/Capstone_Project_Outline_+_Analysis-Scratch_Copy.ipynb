{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# processed_datafile = '_stock_df_with_technical_indicators.csv'\n",
    "# print(f'NOTE: current processed filename ready for modeling is: \"{processed_datafile}\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs_ds.imports import *\n",
    "import functions_combined_BEST as ji\n",
    "\n",
    "\"\"\"NOTE: current processed filename ready for modeling is: processed_datafile\"\"\"\n",
    "processed_datafile = '_stock_df_with_technical_indicators.csv'\n",
    "\n",
    "## LOAD PROCESSED CSV WITH TECH INDICATORS\n",
    "def load_processed_stock_data(processed_data_filename = '_stock_df_with_technical_indicators.csv', force_from_raw=False):\n",
    "    import functions_combined_BEST as ji\n",
    "    import os\n",
    "    import pandas as pd\n",
    "    \n",
    "    # check if csv file already exists\n",
    "    current_files = os.listdir()\n",
    "\n",
    "    # Run all processing on raw data if file not found.\n",
    "    if (force_from_raw is True) or (processed_data_filename not in current_files):\n",
    "        \n",
    "        print(f'File not found. Processing raw data using custom ji functions...')\n",
    "        print('1) ji.load_raw_stock_data_from_text\\n2) ji.get_technical_indicators,dropping na from column \"ma21\"')\n",
    "\n",
    "        stock_df = ji.load_raw_stock_data_from_txt(\n",
    "            filename=\"IVE_bidask1min.txt\", \n",
    "            start_index='2016-12-01',\n",
    "            clean=True, fill_or_drop_null='drop', \n",
    "            freq='CBH',verbose=1)\n",
    "\n",
    "        ## CALCULATE TECHNICAL INDICATORS FOR STOCK MARKET\n",
    "        stock_df = ji.get_technical_indicators(stock_df, make_price_from='BidClose')\n",
    "\n",
    "        ## Clean up stock_df \n",
    "        # Remove beginning null values for moving averages\n",
    "        na_idx = stock_df.loc[stock_df['ma21'].isna() == True].index # was 'upper_band'\n",
    "        stock_df = stock_df.loc[na_idx[-1]+1*na_idx.freq:]\n",
    "\n",
    "\n",
    "    # load processed_data_filename if found\n",
    "    elif processed_data_filename in current_files:\n",
    "\n",
    "        print(f'File found. Loading {processed_data_filename}')\n",
    "        \n",
    "        stock_df=pd.read_csv(processed_data_filename, index_col=0, parse_dates=True)\n",
    "        stock_df['date_time_index'] = stock_df.index.to_series()\n",
    "        stock_df.index.freq=ji.custom_BH_freq()\n",
    "        \n",
    "    print(stock_df.index[[0,-1]])\n",
    "    display(stock_df.head(3))\n",
    "\n",
    "    return stock_df        \n",
    "\n",
    "# LOAD IN FULL STOCK DATASET \n",
    "full_df = load_processed_stock_data()\n",
    "\n",
    "# SELECT DESIRED COLUMNS OF \n",
    "stock_df = full_df[[\n",
    "    'date_time_index','price','ma7','ma21','26ema','12ema','MACD',\n",
    "    '20sd','upper_band','lower_band','ema','momentum','filled_timebin'\n",
    "]].copy()\n",
    "\n",
    "\n",
    "# def load_everything(file_to_load = None):\n",
    "#     import pandas as pd\n",
    "# #     from bs_ds.imports import \n",
    "# #     from bs_ds import ihelp\n",
    "# #     from functions_combined_BEST import global_imports\n",
    "    \n",
    "# #     import_packages()\n",
    "# #     global_imports('functions_combined_BEST','ji')\n",
    "# #     global_imports('IPython.display','display',asfunction=True)\n",
    "# #     global_imports('IPython.display')\n",
    "# #     import functions_combined_BEST as ji\n",
    "\n",
    "    \n",
    "#     import warnings\n",
    "#     warnings.filterwarnings('ignore')\n",
    "#     pd.set_option('display.max_columns',None)\n",
    "    \n",
    "#     if file_to_load is not None:\n",
    "#         stock_df = check_for_csv_and_load(processed_datafile=file_to_load)\n",
    "#     else:\n",
    "#         stock_df = check_for_csv_and_load()\n",
    "#     return stock_df\n",
    "# bs.html_on()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stock_df.index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TEST CODE\n",
    "- https://ipython-books.github.io/33-mastering-widgets-in-the-jupyter-notebook/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## BEST PROGRESS ON GRID BUTTONS FOR PLOTTING COLUMSNhttps://github.com/jupyter-widgets/ipywidgets/issues/763\n",
    "# df_model1.to_csv('df_model1_for_widget_testing.csv')\n",
    "df_model1= pd.read_csv('df_model1_for_widget_testing.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing Coloring of Separate Columns by Critical value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "        \n",
    "df_model = df_model1\n",
    "true_colname='True Test Price'\n",
    "pred_colname='Predicted Test Price'\n",
    "shift_list=np.arange(-4,4) #[-4,-3,-2,-1,0,1,2,3,4]\n",
    "\n",
    "\n",
    "# GET EVALUATION METRICS FROM PREDICTIONS\n",
    "true_test_series = df_model[true_colname].dropna()\n",
    "pred_test_series = df_model[pred_colname].dropna()\n",
    "\n",
    "# Comparing Shifted Timebins\n",
    "    # res_df = compare_eval_metrics_for_shifts(true_test_series, pred_test_series,shift_list=np.arange(-4,4,1))\n",
    "\n",
    "results=[['Bins Shifted','Metric','Value']]\n",
    "combined_results = pd.DataFrame(columns=results[0])\n",
    "\n",
    "\n",
    "for shift in shift_list:\n",
    "\n",
    "    df_shift=pd.DataFrame()\n",
    "    df_shift[pred_colname] = df_model[pred_colname].shift(shift)\n",
    "    df_shift[true_colname] = df_model[true_colname]\n",
    "    df_shift.dropna(inplace=True)      \n",
    "\n",
    "    shift_results = evaluate_regression(df_shift[true_colname],df_shift[pred_colname]).reset_index()\n",
    "    shift_results.insert(0,'Bins Shifted',shift)\n",
    "\n",
    "    combined_results = pd.concat([combined_results,shift_results], axis=0)\n",
    "\n",
    "pivot = combined_results.pivot(index='Bins Shifted', columns='Metric',values='Value')\n",
    "pivot.columns.rename(None, inplace=True)\n",
    "display(pivot)\n",
    "# combined_results.unstack(0)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing making interactive plot to select shifts to plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import ipywidgets as widgets\n",
    "# # interact(compare_time_shifted_model,**{'df_model':df_model,'shift_list':shift_list_in})\n",
    "# # compare_time_shifted_model(df_model1, shift_list=shift_list)\n",
    "# %matplotlib notebook\n",
    "# def f(x):\n",
    "#     return x\n",
    "\n",
    "# true_data=df_model1['True Test Price'].rename('true')\n",
    "# pred_data=df_model1['Predicted Test Price'].rename('pred')\n",
    "\n",
    "# shift_list_in=list(np.arange(-4,5,1))\n",
    "# shift_dict = dict(zip(shift_list_in,[True for x in shift_list_in]))\n",
    "# #                   print)\n",
    "# df_data = pd.concat([true_data,pred_data],axis=1)#,names=['true','pred'])\n",
    "\n",
    "# @interact(x=shift_dict)\n",
    "# def plot_shift_list(shift_list=x ):\n",
    "# #     print(pred_data)\n",
    "# #     return \n",
    "    \n",
    "#     df_data['true'].plot()\n",
    "#     df_data['pred'].shift(shift).plot()#ax=fig)#shift(shift)\n",
    "    \n",
    "#     return plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a dataframe with a new  column for each shift \n",
    "shift_list = np.arange(-4,5,1)\n",
    "# start df_shifts with true and predicted values\n",
    "df_shifts = pd.DataFrame(df_model1['True Test Price'].rename('true'))\n",
    "df_shifts['pred'] = df_model1['Predicted Test Price'] #.rename('pred')\n",
    "\n",
    "# shift the predicted data\n",
    "for shift in  shift_list_in:\n",
    "    # add df column\n",
    "    df_shifts['pred_'+str(shift)] = df_shifts['pred'].shift(shift)\n",
    "    \n",
    "   \n",
    "def on_click_(change):\n",
    "    return\n",
    "\n",
    "def make_col_list_from_bottons(df_shifts):\n",
    "    ## MAKE A LIST OF BUTTONS FROM DATAFRAME COLUMNS    \n",
    "    list_of_buttons=[widgets.ToggleButton(value=True, description=column,icon='check') for column in df_shifts.columns]# ()# for x in shift_list]\n",
    "    \n",
    "    # Group and display buttons\n",
    "    grid_display=widgets.HBox([b for b in list_of_buttons])\n",
    "    grid_display = [button.observe(on_click_,'value') for button in grid_display.children]\n",
    "#     display(grid_display)\n",
    "    return grid_display\n",
    "display(grid_display)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[x.value for x in grid_display.children]#.values\n",
    "grid_display.children"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_shift_list(plot_list=grid_display):\n",
    " \n",
    "        \n",
    "    for column in plot_list.children:\n",
    "        column.observe(on_click)\n",
    "    @interact(column)\n",
    "    def plot_on_click(column):\n",
    "            if column.value is True:\n",
    "                df_shifts[column.description].plot()\n",
    "\n",
    "     \n",
    "\n",
    "\n",
    "# plot_shift_list()\n",
    "#         plot_cols= ['pred_'+button.description for button in buttons]\n",
    "#     print(plot_cols)\n",
    "#     df_data['true'].plot()\n",
    "#     df_data[plot_cols].plot()\n",
    "#     df_data['pred'].shift(shift).plot()#ax=fig)#shift(shift)\n",
    "    \n",
    "#     return plt.show()\n",
    "# interactive(plot_shift_list, grid_display)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new approach, make df_data a df with every shift as a diffferent column, then plot that\n",
    "%matplotlib inline\n",
    "true_data=df_model1['True Test Price'].rename('true')\n",
    "pred_data=df_model1['Predicted Test Price'].rename('pred')\n",
    "\n",
    "shift_list_in=list(np.arange(-4,5,1))\n",
    "shift_dict = dict(zip(shift_list_in,[True for x in shift_list_in]))\n",
    "\n",
    "df_data = pd.concat([true_data,pred_data],axis=1)#,names=['true','pred'])\n",
    "df_shifts = pd.DataFrame()\n",
    "df_shifts['true'] = df_data['true'].copy()\n",
    "@interact()\n",
    "def plot_shift_checklist():\n",
    "    for shift in  shift_list_in:\n",
    "        df_shifts['preds shifted '+str(shift)] = df_shifts['true'].shift(shift)\n",
    "    df_shifts.plot(figsize=(10,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def global_imports(modulename,shortname = None, asfunction = False):\n",
    "#     \"\"\"from stackoverflow: https://stackoverflow.com/questions/11990556/how-to-make-global-imports-from-a-function,\n",
    "#     https://stackoverflow.com/a/46878490\"\"\"\n",
    "#     from importlib import import_module\n",
    "\n",
    "#     if shortname is None:\n",
    "#         shortname = modulename\n",
    "\n",
    "#     if asfunction is False:\n",
    "#         globals()[shortname] = import_module(modulename) #__import__(modulename)\n",
    "#     else:\n",
    "#         globals()[shortname] = eval(modulename + \".\" + shortname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from bs_ds.imports import *\n",
    "# from bs_ds import ihelp\n",
    "\n",
    "# import functions_combined_BEST as ji\n",
    "\n",
    "# from IPython.display import display\n",
    "# import warnings\n",
    "# warnings.filterwarnings('ignore')\n",
    "# pd.set_option('display.max_columns',None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # import bs_ds as bs\n",
    "# # import functions_combined_BEST as ji\n",
    "# # from bs_ds import ihelp\n",
    "# # from bs_ds.imports import *\n",
    "\n",
    "# # import pandas as pd \n",
    "# # import numpy as np\n",
    "# # import matplotlib.pyplot as plt\n",
    "\n",
    "# from IPython.display import display\n",
    "# import warnings\n",
    "# warnings.filterwarnings('ignore')\n",
    "# pd.set_option('display.max_columns',None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def welcome_message():\n",
    "#     from IPython.display import Markdown as md\n",
    "#     logo = \"<img src='https://raw.githubusercontent.com/jirvingphd/bs_ds/master/docs/bs_ds_logo.png' width=75> \\n\"\n",
    "#     msg = f\"- For convenient loading of standard modules use:\\n```python\\nfrom bs_ds.imports import *\\n```\"\n",
    "#     msg2 = f\"- For a Dropdow Menu of Available Functions' Help and Source Code:\\n```python\\nbs_bs.module_menu()\\n```\"\n",
    "#     return md(logo+msg+'\\n'+msg2)\n",
    "# welcome_message()\n",
    "\n",
    "# # logo = '<img src=\"bs-ds.readthedocs.io/en/latest/_images/bs_ds_logo.png\", height=100>\\n' \n",
    "# # import IPython\n",
    "# # IPython.display.Markdown(logo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# # code from using v 3.6.1 of plotly.\n",
    "# import plotly\n",
    "# import plotly.offline as py\n",
    "# import plotly.tools as tls\n",
    "# import plotly.graph_objs as go\n",
    "\n",
    "# py.init_notebook_mode(connected=True)\n",
    "\n",
    "# %matplotlib inline\n",
    "# # import cufflinks as cf\n",
    "# # cf.set_config_file(offline=True, world_readable=True, theme='pearl')\n",
    "\n",
    "\n",
    "# # import folium\n",
    "# # import altair as alt\n",
    "# # import missingno as msg\n",
    "# # import sys\n",
    "# # import warnings\n",
    "\n",
    "\n",
    "# from ipywidgets import interact, interactive, fixed\n",
    "# import ipywidgets as widgets\n",
    "# from IPython.display import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# # stock_df = ji.load_raw_stock_data_from_txt(filename=\"IVE_bidask1min.txt\", start_index='2016-12-01',\n",
    "# #                                         clean=True,fill_or_drop_null='drop', freq='CBH',verbose=1)\n",
    "# # preparing 'df' for code below\n",
    "# # date_time_index = stock_df.index.to_series()\n",
    "# # df = stock_df[['Date','BidClose']]\n",
    "# # df = pd.concat([date_time_index, df],axis=1)\n",
    "# # df.rename({'BidClose':'price'} , axis='columns',inplace=True)\n",
    "# # df.to_csv('test_stock_df.csv')\n",
    "\n",
    "# stock_df = pd.read_csv('test_stock_df.csv',index_col=0)\n",
    "# stock_df.columns=['date_time','Date','price']\n",
    "# stock_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# ## EXAMPLE PLOTLY\n",
    "# # Load data\n",
    "# df = pd.read_csv(\n",
    "#     \"https://raw.githubusercontent.com/plotly/datasets/master/finance-charts-apple.csv\")\n",
    "# df.columns = [col.replace(\"AAPL.\", \"\") for col in df.columns]\n",
    "\n",
    "# # Create figure\n",
    "# fig = go.Figure()\n",
    "\n",
    "# fig.add_trace(\n",
    "#     go.Scatter(x=list(df.Date), y=list(df.High)))\n",
    "\n",
    "# # Set title\n",
    "# fig.update_layout(\n",
    "#     title_text=\"Time series with range slider and selectors\"\n",
    "# )\n",
    "\n",
    "# # Add range slider\n",
    "# fig.update_layout(\n",
    "#     xaxis=go.layout.XAxis(\n",
    "#         rangeselector=dict(\n",
    "#             buttons=list([\n",
    "#                 dict(count=1,\n",
    "#                      label=\"1m\",\n",
    "#                      step=\"month\",\n",
    "#                      stepmode=\"backward\"),\n",
    "#                 dict(count=6,\n",
    "#                      label=\"6m\",\n",
    "#                      step=\"month\",\n",
    "#                      stepmode=\"backward\"),\n",
    "#                 dict(count=1,\n",
    "#                      label=\"YTD\",\n",
    "#                      step=\"year\",\n",
    "#                      stepmode=\"todate\"),\n",
    "#                 dict(count=1,\n",
    "#                      label=\"1y\",\n",
    "#                      step=\"year\",\n",
    "#                      stepmode=\"backward\"),\n",
    "#                 dict(step=\"all\")\n",
    "#             ])\n",
    "#         ),\n",
    "#         rangeslider=dict(\n",
    "#             visible=True\n",
    "#         ),\n",
    "#         type=\"date\"\n",
    "#     )\n",
    "# )\n",
    "\n",
    "# fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# stock_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ji.reload(ji)\n",
    "# fig = ji.iplot_time_series(stock_df);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ji.iplot_time_series(stock_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# # Plotly attempt\n",
    "# try: \n",
    "#     del fig\n",
    "# except:\n",
    "#     pass\n",
    "# # import plotly.graph_objects as go\n",
    "# import plotly\n",
    "# import plotly.graph_objs as go\n",
    "\n",
    "# import pandas as pd\n",
    "\n",
    "# # Load data\n",
    "# df = pd.read_csv(\n",
    "#     \"https://raw.githubusercontent.com/plotly/datasets/master/finance-charts-apple.csv\")\n",
    "# df.columns = [col.replace(\"AAPL.\", \"\") for col in df.columns]\n",
    "\n",
    "# # Create figure\n",
    "# fig = go.Figure()\n",
    "\n",
    "\n",
    "# fig.add_trace(\n",
    "#     go.Scatter(x=list(df.Date), y=list(df.High)))\n",
    "\n",
    "# # Set title\n",
    "# # fig.update({'title_text':\"Time series with range slider and selectors\"})\n",
    "#     # title_text=\"Time series with range slider and selectors\"\n",
    "# # )\n",
    "\n",
    "# # Add range slider\n",
    "# fig.update(\n",
    "#     xaxis=go.layout.XAxis(\n",
    "#         rangeselector=dict(\n",
    "#             buttons=list([\n",
    "#                 dict(count=1,\n",
    "#                      label=\"1m\",\n",
    "#                      step=\"month\",\n",
    "#                      stepmode=\"backward\"),\n",
    "#                 dict(count=6,\n",
    "#                      label=\"6m\",\n",
    "#                      step=\"month\",\n",
    "#                      stepmode=\"backward\"),\n",
    "#                 dict(count=1,\n",
    "#                      label=\"YTD\",\n",
    "#                      step=\"year\",\n",
    "#                      stepmode=\"todate\"),\n",
    "#                 dict(count=1,\n",
    "#                      label=\"1y\",\n",
    "#                      step=\"year\",\n",
    "#                      stepmode=\"backward\"),\n",
    "#                 dict(step=\"all\")\n",
    "#             ])\n",
    "#         ),\n",
    "# #         rangeslider=dict(\n",
    "# #             visible=True\n",
    "#         ),\n",
    "# #         type=\"date\"\n",
    "#     )\n",
    "# # )\n",
    "\n",
    "# fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ihelp(widgets.SelectionRangeSlider)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# skip=False\n",
    "# # if skip==False:\n",
    "#     # Make a Layout with the range slider and labels as two different boxes, one on top of the other\n",
    "\n",
    "# start_date = stock_df.index[0]\n",
    "# end_date  = stock_df.index[-1]\n",
    "\n",
    "# def make_date_range_slider(start_date,end_date,freq='D'):\n",
    "\n",
    "#     from ipywidgets import interact, interactive, interaction, Label, Box, Layout\n",
    "#     import ipywidgets as iw\n",
    "#     from datetime import datetime\n",
    "\n",
    "#     # specify the date range from user input\n",
    "#     dates = pd.date_range(start_date, end_date,freq=freq)\n",
    "\n",
    "#     # specify formatting based on frequency code\n",
    "#     date_format_lib={'D':'%m/%d/%Y','H':'%m/%d/%Y: %T'}\n",
    "#     freq_format = date_format_lib[freq]\n",
    "\n",
    "\n",
    "#     # creat options list and index for SelectionRangeSlider\n",
    "#     options = [(date.strftime(date_format_lib[freq]),date) for date in dates]\n",
    "#     index = (0, len(options)-1)\n",
    "\n",
    "#     #     # Create out function to display outputs (not needed?)\n",
    "#     #     out = iw.Output(layout={'border': '1px solid black'})\n",
    "#     #     #     @out.capture()\n",
    "\n",
    "#     # Instantiate the date_range_slider\n",
    "#     date_range_slider = iw.SelectionRangeSlider(\n",
    "#         options=options, index=index, description = 'Date Range',\n",
    "#         orientation = 'horizontal',layout={'width':'500px','grid_area':'main'},#layout=Layout(grid_area='main'),\n",
    "#         readout=True)\n",
    "\n",
    "#     # Save the labels for the date_range_slider as separate items\n",
    "#     date_list = [date_range_slider.label[0], date_range_slider.label[-1]]\n",
    "#     date_label = iw.Label(f'{date_list[0]} -- {date_list[1]}',\n",
    "#                          layout=Layout(grid_area='header'))\n",
    "#     return date_range_slider\n",
    "\n",
    "# dl = widgets.dlink((source, 'value'), (target1, 'value'))\n",
    "# display(caption, source, target1)\n",
    "# # def updateXAxis(change):\n",
    "# #     #Update X-axis min/max value here\n",
    "# #     if change['type'] == 'change' and change['name'] == 'value':\n",
    "# #         x_start = change['new'][0]\n",
    "# #         x_end = change['new'][1]\n",
    "# # date_range_slider.observe(updateXAxis)\n",
    "\n",
    "# #             x_sc.min = change['new'][0]\n",
    "# #             x_sc.max = change['new'][1]\n",
    "\n",
    "# #     source1, target1 = date_range_slider, date_label\n",
    "# #     dl = iw.dlink((soruce1,'label'),(target1,'value'))\n",
    "\n",
    "# #     ## ADJUST LABEL OUTPUT TO MATCH SLIDER\n",
    "# #     output2 = date_label#widgets.Output()\n",
    "# #     def on_value_change(change):\n",
    "# #         with output2:\n",
    "# #             print(change['new'])\n",
    "# #     header  = date_range_slider\n",
    "# #     main    = date_label      \n",
    "\n",
    "# #     slider_items=[date_range_slider, date_label]\n",
    "# #     output = iw.GridBox(children=[date_range_slider,date_label],\n",
    "# #                        layout=Layout(\n",
    "# #                        grid_template_rows='auto auto',\n",
    "# #                        grid_template_columns='auto auto',\n",
    "# #                        grid_template_areas='''\n",
    "# #                        \"header header\"\n",
    "# #                        \"main main\"\n",
    "# #                        '''))# display='flex','flex_flow'\n",
    "#     return date_range_slider\n",
    "# fig = stock_df['BidClose'].plot()\n",
    "# make_date_range_slider(start_date, end_date,'D')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Instantiate the date_range_slider\n",
    "# date_range_slider = iw.SelectionRangeSlider(\n",
    "#     options=options, index=index, description = 'Date Range',\n",
    "#     orientation = 'horizontal',layout={'width':'500px'},readout=False)\n",
    "\n",
    "# # Save the labels for the date_range_slider as separate items\n",
    "# # date_list = [date_range_slider.label[0], date_range_slider.label[-1]]\n",
    "# date_label1 = iw.Label(date_range_slider.label[0])\n",
    "# date_label2 = iw.Label(date_range_slider.label[-1])\n",
    "# source_1, target_1 = date_range_slider, iw.Label\n",
    "\n",
    "# def tf_label(label):\n",
    "#     return str(label[0])\n",
    "# dl1 = iw.dlink((source_1,'label',lambda x: tf_label(x)),(target_1,'value'))#,lambda x: str(x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### EX FROM TUTORIAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import ipywidgets as widgets\n",
    "\n",
    "# widgets.Textarea(\n",
    "#     value='Hello World',\n",
    "#     placeholder='Type something',\n",
    "#     description='String:',\n",
    "#     disabled=False\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports_list = [('pandas','pd','High performance data structures and tools'),\n",
    "#                 ('numpy','np','scientific computing with Python'),\n",
    "#                 ('matplotlib','mpl',\"Matplotlib's base OOP module with formatting artists\"),\n",
    "#                 ('matplotlib.pyplot','plt',\"Matplotlib's matlab-like plotting module\"),\n",
    "#                 ('seaborn','sns',\"High-level data visualization library based on matplotlib\"),\n",
    "#                ('IPython.display')]\n",
    "\n",
    "# for package_tuple in imports_list:\n",
    "#     package=package_tuple[0]\n",
    "#     handle=package_tuple[1]\n",
    "#     description=package_tuple[2]\n",
    "#     exec(f'import {package} as {handle}')\n",
    "# df_imported= pd.DataFrame(imports_list,columns=['Package','Handle','Description'])\n",
    "# display(df_imported.sort_values('Package').style.hide_index().set_caption('Loaded Packages and Handles'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from ipywidgets import interact\n",
    "# import functions_combined_BEST as ji\n",
    "\n",
    "# def module_menu(mods=[x for x in dir(ji) if '__' not in x], show_help=False, show_code=True):\n",
    "#     \"\"\"Displays an interactive menu of all functions available in bs_ds\"\"\"\n",
    "#     from functions_combined_BEST import ihelp\n",
    "#     import functions_combined_BEST as ji\n",
    "#     ihelp(eval(f'ji.{mods}'), show_help, show_code)\n",
    "#     return\n",
    "    \n",
    "# interact(module_menu);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## BS_DS MODULE DISPLAY FUNCTION\n",
    "# from ipywidgets import interact\n",
    "# import functions_combined_BEST as ji\n",
    "\n",
    "# mods = [x for x in dir(ji) if '__' not in x]\n",
    "\n",
    "# @interact(bs_ds_functions=mods, show_help=False, show_code=True)\n",
    "# def display_modules(bs_ds_functions, show_help, show_code):\n",
    "# #     display(\"Select Function to Display\")\n",
    "#     from ipywidgets\n",
    "#     ji.ihelp(eval(f'ji.{bs_ds_functions}'),show_help=show_help, show_code=show_code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mHDiTjr5lm_U"
   },
   "source": [
    "# Project Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PROJECT GOAL:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wofiIIk6hQtc"
   },
   "source": [
    "* **Use President Trump's tweets (NLP and other features) to predict fluctuations in the stock market (using S&P 500 as index).**\n",
    "\n",
    "    \n",
    "### Data to Use:\n",
    "\n",
    "* All Donald Trump tweets from inaugaration day 2017 to today (for now) - 06/20/19\n",
    "\n",
    "    *          Extracted from http://www.trumptwitterarchive.com/\n",
    "*     Minute-resolution data for the S&P500 covering the same time period.\n",
    "\n",
    "    *         IVE S&P500 Index from - http://www.kibot.com/free_historical_data.aspx\n",
    "    \n",
    "## MAJOR REFERENCES / INSPIRATION / PRIOR WORK IN FIELD:\n",
    "\n",
    "1. **Stanford Scientific Poster Using NLP ALONE to predict if stock prices increase or decrease 5 mins after Trump tweets.**  [Poster PDF LINK](http://cs229.stanford.edu/proj2017/final-posters/5140843.pdf)\n",
    "    - [Evernote Summary Notes Link](https://www.evernote.com/l/AAoL1CyhPV1GoIzSgq59GO10x6xfEeVDo5s/)\n",
    "\n",
    "2. **TowardsDataScience Blog Plost on \"Using the latest advancements in deep learning to predict stock price movements.\"** [Blog Post link](https://towardsdatascience.com/aifortrading-2edd6fac689d)\n",
    "    - [Evernote Summary](https://www.evernote.com/l/AApvQ8Xh8b9GBLhrD0m8w4H1ih1oVM8wkEw/)\n",
    "\n",
    "\n",
    "## OUTLINE FOR DATA TO PRODUCE & MODEL FOR FINAL PROJECT:\n",
    "\n",
    "### TWITTER DATA:\n",
    "\n",
    "* [ENGINEER FEATURES] **Extract features from Trump's tweets: perform the NLP analysis to generate the features about his tweets to use in final model**\n",
    "\n",
    "    * [x] Tweet sentiment score\n",
    "    * [ ] Tweet frequency per timebin\n",
    "    * [x] upper-to-lowercase-ratio\n",
    "    * [x] retweet-count\n",
    "    * [x] favorite-count\n",
    "    \n",
    "* [PREDICTIVE MODEL] **Generate Binary Stock Market Predictions based on Trump's Tweets.**\n",
    "\n",
    "    * [x] Create a neural network model like the Stanford guys, where my model JUST uses the content of trump's tweets with word embeddings and a binary label (-1, 0,1) for direction of stock market change at a fixed time delta (they did 5 mins, I will do 1 hour) [ See reference #1 - stanford poster]\n",
    "  \n",
    "\n",
    "\n",
    "\n",
    "### STOCK MARKET DATA (S&P 500):\n",
    "\n",
    "* [ENGINEER FEATURES] **Extract features about the stock data -calculate the technical indices for the S&P 500 discussed in his article.**  [ see reference #2 - blog post ]\n",
    "\n",
    "    * [x] 7 days moving average \n",
    "    * [x]  21 days moving average\n",
    "    * [x] exponential moving average\n",
    "    * [x] momentum\n",
    "    * [x] Bollinger bands\n",
    "    * [x] MACD\n",
    "    * (Maybe) FFT / time series decomp for trend lines\n",
    "    \n",
    "* [PREDICTIVE MODEL] **Generate stock price predictions based only historical data using....**\n",
    "\n",
    "    * [x] a SARIMA model[?]\n",
    "    * [ ] a FB Prophet model[?] \n",
    "    * [x] an LSTM neural network like other blog post?  [!!!] [Predicting the Stock Market Using Machine Learning and Deep Learning](https://www.evernote.com/l/AAq1azRmt2dANq_Oye-MBZQr-OU5lA5APl8/)\n",
    "    \n",
    "### FINAL MODEL - FEED ALL ABOVE FEATURES INTO:\n",
    "\n",
    "- **Plan A: NEURAL NETWORK *REGRESSION* MODEL TO PREDICT *ACTUAL S&P 500 PRICE* AT 1 HOUR-1 DAY FOLLOWING TWEETS**\n",
    "    - Final Model Target is based more on blog post's construction (ref#2), but takes output of model like ref#1\n",
    "    \n",
    "    <img src=\"https://raw.githubusercontent.com/jirvingphd/dsc-5-capstone-project-online-ds-ft-021119/master/figures/annotated_GAN_for_stock_market.jpeg\" width=800>\n",
    "    \n",
    "- **PLAN B: NEURAL NETWORK *BINARY CLASSIFICATION*  MODEL TO PREDICT THE *DIRECTION OF CHANGE FOR S&P 500 PRICE* AT 1 HOUR - 1 DAY FOLLOWING TWEETS**\n",
    "    - Final Model Target is based on stanford poster's methods/goal. (ref #1)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FXUD8NkFm5r8"
   },
   "source": [
    "## DATA ANALYSIS DETAILS AND Equations/Code "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "s_ogprIz6TZ-"
   },
   "source": [
    "#### Technical Indicators - Explanation & Equations\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0B4Jeml3m8zQ"
   },
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "* 7 and 21 day moving averages \n",
    "```python\n",
    "df['ma7'] df['price'].rolling(window = 7 ).mean() #window of 7 if daily data\n",
    "df['ma21'] df['price'].rolling(window = 21).mean() #window of 21 if daily data\n",
    "```    \n",
    "* MACD(Moving Average Convergence Divergence)\n",
    "\n",
    "> Moving Average Convergence Divergence (MACD) is a trend-following momentumindicator that shows the relationship between two moving averages of a security’s price. The MACD is calculated by subtracting the 26-period Exponential Moving Average (EMA) from the 12-period EMA.\n",
    "\n",
    ">The result of that calculation is the MACD line. A nine-day EMA of the MACD, called the \"signal line,\" is then plotted on top of the MACD line, which can function as a trigger for buy and sell signals. \n",
    "\n",
    "> Traders may buy the security when the MACD crosses above its signal line and sell - or short - the security when the MACD crosses below the signal line. Moving Average Convergence Divergence (MACD) indicators can be interpreted in several ways, but the more common methods are crossovers, divergences, and rapid rises/falls.  - _[from Investopedia](https://www.investopedia.com/terms/m/macd.asp)_\n",
    "\n",
    "```python\n",
    "df['ewma26'] = pd.ewma(df['price'], span=26)\n",
    "df['ewma12'] = pd.ewma(df['price'], span=12)\n",
    "df['MACD'] = (df['12ema']-df['26ema'])\n",
    "```\n",
    "- **Exponentially weighted moving average**\n",
    "```python\n",
    "dataset['ema'] = dataset['price'].ewm(com=0.5).mean()\n",
    "```\n",
    "\n",
    "- **Bollinger bands**\n",
    "    > \"Bollinger Bands® are a popular technical indicators used by traders in all markets, including stocks, futures and currencies. There are a number of uses for Bollinger Bands®, including determining overbought and oversold levels, as a trend following tool, and monitoring for breakouts. There are also some pitfalls of the indicators. In this article, we will address all these areas.\"\n",
    "> Bollinger bands are composed of three lines. One of the more common calculations of Bollinger Bands uses a 20-day simple moving average (SMA) for the middle band. The upper band is calculated by taking the middle band and adding twice the daily standard deviation, the lower band is the same but subtracts twice the daily std. - _[from Investopedia](https://www.investopedia.com/trading/using-bollinger-bands-to-gauge-trends/)_\n",
    "\n",
    "    - Boilinger Upper Band:<br>\n",
    "    $BOLU = MA(TP, n) + m * \\sigma[TP, n ]$<br><br>\n",
    "    - Boilinger Lower Band<br>\n",
    "    $ BOLD = MA(TP,n) - m * \\sigma[TP, n ]$\n",
    "    - Where:\n",
    "        - $MA$  = moving average\n",
    "        - $TP$ (typical price) = $(High + Low+Close)/ 3$\n",
    "        - $n$ is number of days in smoothing period\n",
    "        - $m$ is the number of standard deviations\n",
    "        - $\\sigma[TP, n]$ = Standard Deviations over last $n$ periods of $TP$\n",
    "\n",
    "```python\n",
    "# Create Bollinger Bands\n",
    "dataset['20sd'] = pd.stats.moments.rolling_std(dataset['price'],20)\n",
    "dataset['upper_band'] = dataset['ma21'] + (dataset['20sd']*2)\n",
    "dataset['lower_band'] = dataset['ma21'] - (dataset['20sd']*2)\n",
    "```\n",
    "\n",
    "\n",
    "- **Momentum**\n",
    "> \"Momentum is the rate of acceleration of a security's price or volume – that is, the speed at which the price is changing. Simply put, it refers to the rate of change on price movements for a particular asset and is usually defined as a rate. In technical analysis, momentum is considered an oscillator and is used to help identify trend lines.\" - _[from Investopedia](https://www.investopedia.com/articles/technical/081501.asp)_\n",
    "\n",
    "    - $ Momentum = V - V_x$\n",
    "    - Where:\n",
    "        - $V$ = Latest Price\n",
    "        - $V_x$ = Closing Price\n",
    "        - $x$ = number of days ago\n",
    "\n",
    "```python\n",
    "# Create Momentum\n",
    "dataset['momentum'] = dataset['price']-1\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "u5DvoInw7Pha"
   },
   "source": [
    "###  STOCK DATA INFO - S&P500"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZlLWKYf9oDOf"
   },
   "source": [
    "- Q: What is the format of your aggregate bid/ask data?\n",
    "    - **The order of the fields in our aggregate bid/ask files is: <br>Date, Time, BidOpen, BidHigh, BidLow, BidClose, AskOpen, AskHigh, AskLow, AskClose**\n",
    "\n",
    "    - The format is very similar to our standard one minute and higher interval files which are constructed by aggregating the execution price and volume. The difference here is that instead of using execution price we use \"national best bid and offer\" (NBBO) prices to get their open, high, low and close prices for any given time interval. Bids and asks in our files are from multiple markets and they represent the best prices or the highest bid and the lowest ask.\n",
    "\n",
    "    - We record best bid/ask values whenever there is trade activity. Like with our regular tick data, current best bid/ask values are recorded for every trade.\n",
    "\n",
    "    - A potential usage scenario may include using this data in your analysis to try to simulate the execution price for market orders and to estimate the potential slippage that may occur.\n",
    "\n",
    "You can download sample aggregate bid/ask data for free from the free historical data section on our Buy web page."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GIu3htoroCi1"
   },
   "source": [
    "**HELPFUL RESOURCES**\n",
    "- **Dateime Guide Article:**\n",
    "    - https://medium.com/jbennetcodes/dealing-with-datetimes-like-a-pro-in-pandas-b80d3d808a7f\n",
    "- **Getting business day MINUTE resolution**\n",
    "    - Create minute index, then limit it to business times\n",
    "        - https://stackoverflow.com/questions/19373759/python-pandas-business-day-range-bdate-range-doesnt-take-1min-freq\n",
    "    - Try !pip install pandas_market_calendars too"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LOAD & PROCESS RAW STOCK DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "bs.ihelp()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# DISPLAY CODE TO BE USED BELOW TO LOAD AND PROCESS STOCK DATA\n",
    "ji.ihelp( pd.DataFrame, show_help=True)#ji.load_raw_stock_data_from_txt, show_help=True,show_code=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# # LOAD DATASET\n",
    "try:\n",
    "    stock_df\n",
    "except NameError: \n",
    "    stock_df=None\n",
    "else:\n",
    "    print('stock_df exists')\n",
    "if stock_df is None:\n",
    "    stock_df = load_processed_stock_data()\n",
    "    \n",
    "\n",
    "display(stock_df.head(2))\n",
    "# Plot the resulting 'price' column\n",
    "fig = ji.plotly_time_series(stock_df,title='S&P500 Hourly Closing Price',\n",
    "                     x_col='date_time_index', y_col='price',name='S&P500 Price');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig2 = ji.plot_technical_indicators(stock_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## SAVE FULL CSV WITH TECH INDICATORS\n",
    "def save_df_to_csv_ask_to_overwrite(stock_df, filename = '_stock_df_with_technical_indicators.csv',):\n",
    "    import os\n",
    "    current_files = os.listdir()\n",
    "\n",
    "    processed_data_filename = filename#'_stock_df_with_technical_indicators.csv'\n",
    "\n",
    "    # check if csv already exists\n",
    "    if processed_data_filename not in current_files:\n",
    "\n",
    "        stock_df.to_csv(processed_data_filename)\n",
    "\n",
    "    # Ask the user to overwrite existing file\n",
    "    else:\n",
    "\n",
    "        print('File already exists.')\n",
    "        check = input('Overwrite?(y/n):')\n",
    "\n",
    "        if check.lower() == 'y':\n",
    "            stock_df.to_csv(processed_data_filename)\n",
    "            print(f'File {processed_data_filename} was saved.')\n",
    "    \n",
    "        else:\n",
    "            print('No file was saved.')\n",
    "            \n",
    "# save_df_to_csv_ask_to_overwrite(stock_df,'stock_df_for_modeling.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eE3D-Avztybj"
   },
   "source": [
    "## Checking for Stationarity and Trends"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from functions_combined_BEST import stationarity_check, adf_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 465
    },
    "colab_type": "code",
    "id": "mVw0kD3VpbiD",
    "outputId": "4301830b-220e-4829-952a-fdbec70bbfd0"
   },
   "outputs": [],
   "source": [
    "# Check if stationary\n",
    "ji.stationarity_check(stock_df, col='price',freq=ji.custom_BH_freq())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ji.adf_test(stock_df['price'], title='Price')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# Adding a seasonal decomposition\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "result = seasonal_decompose(stock_df['price'], freq=35)#, freq=custom_BH_freq())\n",
    "\n",
    "mpl.rcParams['figure.figsize'] = (8,5)\n",
    "result.plot();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from statsmodels.graphics.tsaplots import plot_acf,plot_pacf\n",
    "plot_df = stock_df['price']\n",
    "\n",
    "fig,ax = plt.subplots(nrows=3,ncols=1,figsize=(10,6))\n",
    "\n",
    "plot_df.plot(ax=ax[0])\n",
    "# df2.plot(ax=ax[0,1])\n",
    "\n",
    "num_lags = 70\n",
    "plot_acf(plot_df,ax=ax[1] , lags=num_lags);\n",
    "# plot_acf(df2,ax=ax[1,1],lags=40);\n",
    "\n",
    "plot_pacf(plot_df,ax=ax[2],lags=num_lags);\n",
    "# plot_pacf(df2,ax=ax[2,1],lags=40);\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# INITIAL MODEL "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Pyramind ARIMA GridSearch \n",
    "#### **Using `pmdarima` to grid search for best ARIMA model type and orders**\n",
    "\n",
    "- Using `auto_arima` from `pmdarima`\n",
    "\n",
    "- SARIMA involves multiple sets of hyperparameters:\n",
    "    - For base model: a set of p,d,q parameters\n",
    "    - For seasonal model:a set of P,D,Q for the seasonal components.\n",
    "    - m = the number of time periods to include in the season \n",
    "    \n",
    "- Set start and max values for each parameter\n",
    "    - i.e. `start_p=0, max_p = 10`\n",
    "- For seasonal data, setting `m` indicates how many periods should be considered part of the season.\n",
    "\n",
    "\n",
    "- **Below is from [\"Predicitng the Sotck Market Using Machine Learning and Deep Learning.\"](https://www.analyticsvidhya.com/blog/2018/10/predicting-stock-price-machine-learningnd-deep-learning-techniques-python/):**\n",
    "> Below are the steps you should follow for implementing auto ARIMA:\n",
    "    1. Load the data: This step will be the same. Load the data into your notebook\n",
    "    2. Preprocessing data: The input should be univariate, hence drop the other columns\n",
    "    3. Fit Auto ARIMA: Fit the model on the univariate series\n",
    "    4. Predict values on validation set: Make predictions on the validation set\n",
    "    5. Calculate RMSE: Check the performance of the model using the predicted values against the actual values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "skip=True\n",
    "if skip==False:\n",
    "    from pmdarima.arima import auto_arima\n",
    "\n",
    "    model = auto_arima(train_data['price'], start_p=1, start_q=1, D=1, max_p=4,max_q=4,trace=True,seasonal=True,m=35)\n",
    "    model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "**Results of the Auto-Arima searching with training data set:**\n",
    "- Training Set= all data less the last 90 days:\n",
    "\n",
    "`Model:\t\tSARIMAX(0, 1, 0)x(1, 0, 1, 35)\t`\n",
    "- So what is this really saying?\n",
    "    - p,d,q = (0,1,0)\n",
    "        - AR (p) = 0\n",
    "        - Difference=1\n",
    "        - MA order(q)=0 \n",
    "        \n",
    "    - P,D,Q,m = (1,0,1,35)\n",
    "        - Seasonal AR(P) = 1\n",
    "        - Seasonal Differencing =0\n",
    "        - Seasonal MA(Q)\n",
    "        - Steps per season (m) = 35 (set by user)\n",
    "\n",
    "`statsmodels` version of SARIMA called SARIMAX adds an eXogenous variable to the regression.\n",
    "- Read this: https://www.analyticsvidhya.com/blog/2018/08/auto-arima-time-series-modeling-python-r/\n",
    "\n",
    "\n",
    "- WHEN I SWITCH TO DAILY DATA:\n",
    "`Model:\tSARIMAX(1, 1, 1)x(1, 0, 1, 5)`\n",
    "\n",
    "#### First try a SARIMA model before SARIMAX\n",
    "\n",
    "-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from statsmodels.tsa.arima_model import ARIMA\n",
    "# model = ARIMA(train_data['price'], order=(1,1,1),freq='B')\n",
    "# results = model.fit()\n",
    "# results.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # evaluate model\n",
    "# start = len(train_data)\n",
    "# end = len(train_data) + len(test_data) - 1\n",
    "# predictions = results.predict(start=start,end=end).rename('ARIMA Model')\n",
    "\n",
    "# train_data['price'].iloc[-120*day_freq:].plot()\n",
    "# test_data['price'].plot()\n",
    "# predictions.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skip=True\n",
    "if skip==False:\n",
    "    from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "    # SARIMAX(0, 1, 0)x(1, 0, 1, 35)\t\n",
    "    # Fit ARIMA: order=(1, 0, 0) seasonal_order=(1, 1, 0, 35); AIC=2881.238, BIC=2905.991, Fit time=52.449 seconds\n",
    "    model = SARIMAX(train_data['price'], order= (1,1,0),seasonal_order=(0,1,1,35), enforce_invertibility=False ) \n",
    "    results=model.fit()\n",
    "    results.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skip=True\n",
    "if skip==False:\n",
    "    # evaluate model\n",
    "    from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "    start = len(train_data)\n",
    "    end = len(train_data) + len(test_data) - 1\n",
    "    predictions = results.predict(start=start,end=end).rename('SARIMA Model')\n",
    "    days_per_period=7\n",
    "\n",
    "    # Combine preds and true into one df\n",
    "    df_predictions_sarima = pd.concat([test_data['price'],pd.Series(predictions, index=test_data.index,name='predicted_price')],axis=1)\n",
    "    df_predictions_sarima.dropna(inplace=True)\n",
    "    display(df_predictions_sarima.head())\n",
    "\n",
    "    # calculate rmse\n",
    "    rmse_sarima = np.sqrt(mean_squared_error(df_predictions_sarima['price'],df_predictions_sarima['predicted_price']))\n",
    "    print(f\"RMSE: {rmse_sarima.round(3)}\")\n",
    "\n",
    "    # Plot results\n",
    "    plt.plot(train_data['price'].iloc[-120*days_per_period:], label='Training Price')#.plot(label='price - train')\n",
    "    plt.plot(df_predictions_sarima['price'], label='True Price')\n",
    "    plt.plot(df_predictions_sarima['predicted_price'], label='Predicted Price')\n",
    "    plt.ylabel('Stock Price')\n",
    "    plt.xlabel('Business Day - Hour Resolution')\n",
    "    plt.title('SARIMA Model')\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.text('2018-10-01',113,f\"RMSE: {rmse_sarima.round(3)}\", fontdict={'fontsize':10, 'fontweight':'medium'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ksn-EM_wDxIl"
   },
   "source": [
    "# Data Prep/Split Forecasting Stock Market Data\n",
    "- https://www.evernote.com/l/AApgV1EaL9lEYpujWnwHq62W9QyEKkH4hwA/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def make_scaler_library, transform_cols_from_library, inverse_transform_series<br>\n",
    "def make_X_y_timeseries_data, make_df_timeseries_bins_by_column<br>\n",
    "def predict_model_make_results_dict<br>\n",
    "def save_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LOAD DATA\n",
    "- use prior split `train_data` and `test_data`\n",
    "- use `make_X_y_timeseries_data` to format data for LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# import matplotlib.pyplot as plt\n",
    "# import matplotlib as mpl\n",
    "\n",
    "\n",
    "# try: stock_df\n",
    "# except NameError: stock_df = None\n",
    "    \n",
    "# if stock_df is None:\n",
    "    \n",
    "#     stock_df = pd.read_csv('data/stock_df_with_tech_indicators_CBH_index.csv', index_col=0, parse_dates=True)\n",
    "#     stock_df = set_timeindex_freq(stock_df,verbose=0)\n",
    "#     stock_df = stock_df.iloc[:,11:]\n",
    "\n",
    "#     # DIsply input stock data\n",
    "#     display(stock_df.head().style.set_caption('Raw Data'))\n",
    "#     plot_time_series(stock_df['price'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Test Split Data By Date,MinMaxScale\n",
    "### EDIT HERE TO CHANGE WINDOWS AND DAYS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CHANGING ORDER OF PROCESSING PER UDEMY COURSE RECOMMENDATION\n",
    "- Fit the MinMaxScaler on the TRAINING data\n",
    "    - `train_test_split_by_last_days`\n",
    "- THEN use that fit scaler to transform the test data\n",
    "    - `make_scaler_library`,`transform_cols_from_library`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load and process stock data \n",
    "# stock_df = ji.load_raw_stock_data_from_txt(filename=\"IVE_bidask1min.txt\", start_index='2016-12-01',\n",
    "#                                         clean=True,fill_or_drop_null='drop', freq='CBH',verbose=1)\n",
    "\n",
    "# stock_df = ji.get_technical_indicators(stock_df)\n",
    "\n",
    "# # Remove timepoints without enough time periods for all indicators\n",
    "# na_idx = stock_df.loc[stock_df['upper_band'].isna() == True].index\n",
    "# stock_df = stock_df.loc[na_idx[-1]+1*na_idx.freq:]\n",
    "# stock_df = stock_df.iloc[:,10:]\n",
    "# print(stock_df.index[[0,-1]])\n",
    "\n",
    "# display(stock_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Plot Stock Data\n",
    "# stock_index = stock_df.index.to_series()\n",
    "# df = pd.concat([stock_index,stock_df],axis=1)\n",
    "\n",
    "# ji.plotly_time_series(df,x_col='date_time_index', y_col='price');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ji.plot_technical_indicators(stock_df, 30*6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "# import pandas_profiling as pprof \n",
    "# pprof.ProfileReport(stock_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stock_df_decomposed = ji.seasonal_decompose_and_plot(stock_df, col='price', window=35);\n",
    "# stock_df_decomposed.plot();\n",
    "# plt.xlim(['2019-01-01','2019-06-01'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reload(functions_combined_BEST)\n",
    "# del train_test_split_by_last_days\n",
    "# from functions_combined_BEST import train_test_split_by_last_days\n",
    "# pause"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stock_df.index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KERAS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########## SET TRAINING AND TEST SET PARAMETERS: ############\n",
    "num_test_days=45 # Number of days for test data \n",
    "num_train_days=365 # Number of days for training data - 5 days/week * 52 weeks\n",
    "days_for_x_window = 5 # Number of days to \n",
    "############################################################\n",
    "\n",
    "# Calculate number of rows to bin for x_windows\n",
    "periods_per_day = ji.get_day_window_size_from_freq(stock_df, ji.custom_BH_freq()) # get the # of rows that == 1 day\n",
    "x_window = periods_per_day * days_for_x_window \n",
    "print(f'X_window size = {x_window} -- ({days_for_x_window} day(s) * {periods_per_day} rows/day)\\n')\n",
    "    \n",
    "###### TRAIN TEST SPLIT BY NUMBER OF DAYS ######\n",
    "df_train, df_test = ji.train_test_split_by_last_days(stock_df, periods_per_day=periods_per_day, num_test_days=num_test_days,\n",
    "                                                  num_train_days=num_train_days,verbose=1, plot=True)\n",
    "\n",
    "\n",
    "###### RESCALE DATA USING MinMaxScalers FIT ON TRAINING DATA's COLUMNS ######\n",
    "scaler_library , df_train = ji.make_scaler_library(df_train,transform=True)\n",
    "# Transform test data with train data's scaler_library\n",
    "df_test = ji.transform_cols_from_library(df_test,scaler_library)\n",
    "\n",
    "## Display Preview of Scaled Dataset\n",
    "# display(df_train.head(2).style.set_caption('df_train'),df_test.head(2).style.set_caption('df_test') )\n",
    "\n",
    "## REPLACE MADE_DF_TIMESERIES_BINS_BY_COLUMN with Kera's TimeseriesGenerator\n",
    "from keras.preprocessing.sequence import TimeseriesGenerator\n",
    "\n",
    "# Define shape of data by specifing these vars:\n",
    "n_input = x_window # Number of timebins to analyze at once. Note: try to choose # greater than length of seasonal cycles\n",
    "n_features= 1 # Number of columns\n",
    "batch_size = 1 # Generally 1 for sequence data\n",
    "\n",
    "# RESHAPING TRAINING AND TEST DATA \n",
    "train_data = df_train['price'].values.reshape(-1,1)\n",
    "test_data = df_test['price'].values.reshape(-1,1)\n",
    "train_data_index =  df_train['price'].index\n",
    "test_data_index = df_test['price'].index\n",
    "\n",
    "\n",
    "## Create Generator for Training Data\n",
    "train_generator = TimeseriesGenerator(data=train_data, targets=train_data,length=n_input, batch_size=batch_size )\n",
    "test_generator = TimeseriesGenerator(data=test_data, targets=test_data,length=n_input, batch_size=batch_size )\n",
    "\n",
    "\n",
    "# What does the first batch look like?\n",
    "X,y = train_generator[0]\n",
    "print(f'Given the Array: \\t(with shape={X.shape}) \\n{X.flatten()}')\n",
    "print(f'\\nPredict this y: \\n {y}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KERAS MODEL 1\n",
    "- **Predicting Price with Keras LSTM - Using ONLY Price**\n",
    "def predict_model_make_results_dict & def plot_price_vs_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     33
    ]
   },
   "outputs": [],
   "source": [
    "def color_cols(df, subset=None, matplotlib_cmap='Greens', rev=False):\n",
    "    from IPython.display import display\n",
    "    import seaborn as sns\n",
    "    \n",
    "    if rev==True:\n",
    "        cm = matplotlib_cmap+'_r'\n",
    "    else:\n",
    "        cm = matplotlib_cmap\n",
    "    \n",
    "    if subset is None:\n",
    "        return  df.style.background_gradient(cmap=cm)\n",
    "    else:\n",
    "        return df.style.background_gradient(cmap=cm,subset=subset)\n",
    "\n",
    "\n",
    "def arr2series(array,series_index=[],series_name='predictions'):\n",
    "    \"\"\"Accepts an array, an index, and a name. If series_index is longer than array:\n",
    "    the series_index[-len(array):] \"\"\"\n",
    "    if len(series_index)==0:\n",
    "        series_index=list(range(len(array)))\n",
    "        \n",
    "    if len(series_index)>len(array):\n",
    "        new_index= series_index[-len(array):]\n",
    "        series_index=new_index\n",
    "        \n",
    "    series_out = pd.Series(array.ravel(), index=series_index, name=series_name)\n",
    "    return series_out\n",
    "\n",
    "\n",
    "def evaluate_regression(y_true, y_pred):\n",
    "    \"\"\"Calculates and displays the following evaluation metrics:\n",
    "    RMSE, R2_score, \"\"\"\n",
    "    from sklearn.metrics import r2_score, mean_squared_error\n",
    "    import numpy as np\n",
    "    from bs_ds import list2df\n",
    "    results=[['Metric','Value']]\n",
    "    \n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    results.append(['R_squared',r2])\n",
    "    \n",
    "    RMSE = np.sqrt(mean_squared_error(y_true,y_pred))\n",
    "    results.append(['Root Mean Squared Error ',RMSE])\n",
    "    \n",
    "    U = thiels_U(y_true, y_pred)\n",
    "    results.append([\"Thiel's U\", U])\n",
    "    \n",
    "    results_df = list2df(results)#, index_col='Metric')\n",
    "    results_df.set_index('Metric', inplace=True)\n",
    "    return results_df.round(3)\n",
    "\n",
    "def thiels_U(ys_true, ys_pred):\n",
    "    sum_list = []\n",
    "    num_list=[]\n",
    "    denom_list=[]\n",
    "    for t in range(len(ys_true)-1):\n",
    "        num_exp = (ys_pred[t+1] - ys_true[t+1])/ys_true[t]\n",
    "        num_list.append([num_exp**2])\n",
    "        denom_exp = (ys_true[t+1] - ys_true[t])/ys_true[t]\n",
    "        denom_list.append([denom_exp**2])\n",
    "    U = np.sqrt( np.sum(num_list) / np.sum(denom_list))\n",
    "    return U        \n",
    "\n",
    "\n",
    "\n",
    "def compare_u_for_shifts(true_series,pred_series, shift_list=[-2,-1,0,1,2],plot_all=False,plot_best=True,color_coded=True):\n",
    "    ## SHIFT THE TRUE VALUES, PLOT, AND CALC THIEL's U\n",
    "    from bs_ds import list2df\n",
    "    df = pd.concat([true_series, pred_series],axis=1)\n",
    "    \n",
    "    true_colname = 'true'\n",
    "    pred_colname = 'pred'\n",
    "    \n",
    "    df.columns=[true_colname,pred_colname]#.dropna(axis=0,subset=[[true_colname,pred_colname]])\n",
    "    results=[['# of Bins Shifted','U']]\n",
    "    \n",
    "    if plot_all or plot_best:\n",
    "        plt.figure()\n",
    "\n",
    "    if plot_all==True:\n",
    "        df[true_colname].plot(color='black',lw=3,label = 'True Values')\n",
    "        plt.legend()\n",
    "        plt.title('Shifted Time Series vs Predicted')\n",
    "        \n",
    "        \n",
    "    for i,shift in enumerate(shift_list):\n",
    "        if plot_all==True:\n",
    "            df[pred_colname].shift(shift).plot(label = f'Predicted-Shifted({shift})')\n",
    "\n",
    "        df_shift=pd.DataFrame()\n",
    "        df_shift['pred'] = df[pred_colname].shift(shift)\n",
    "        df_shift['true'] =df[true_colname]\n",
    "        df_shift.dropna(inplace=True)\n",
    "\n",
    "        U =thiels_U(df_shift['true'], df_shift['pred'])\n",
    "        results.append([shift,U])\n",
    "    \n",
    "    \n",
    "    df_results = list2df(results, index_col='# of Bins Shifted')\n",
    "    \n",
    "    if plot_best==True:\n",
    "        shift = df_results.idxmin()[0]\n",
    "        df[true_colname].plot(label = 'True Values')\n",
    "        df[pred_colname].shift(shift).plot(ls='--',label = f'Predicted-Shifted({shift})')\n",
    "        plt.legend()\n",
    "        plt.title(\"Best Thiel's U for Shifted Time Series\")\n",
    "        plt.tight_layout()\n",
    "#         plt.show()\n",
    "\n",
    "    if color_coded==True:\n",
    "        dfs_results = color_cols(df_results, rev=True)\n",
    "        return dfs_results.set_caption(\"Thiel's U - Shifting Prediction Time bins\")\n",
    "    else:\n",
    "        return df_results.style.set_caption(\"Thiel's U - Shifting Prediction Time bins\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def compare_eval_metrics_for_shifts(true_series,pred_series, shift_list=[-2,-1,0,1,2],plot_all=False,plot_best=True):\n",
    "    ## SHIFT THE TRUE VALUES, PLOT, AND CALC THIEL's U\n",
    "    from bs_ds import list2df\n",
    "    df = pd.concat([true_series, pred_series],axis=1)\n",
    "    \n",
    "    true_colname = 'true'\n",
    "    pred_colname = 'pred'\n",
    "    \n",
    "    df.columns=[true_colname, pred_colname]#.dropna(axis=0,subset=[[true_colname,pred_colname]])\n",
    "\n",
    "    results=[['Bins Shifted','Metric','Value']]\n",
    "    combined_results = pd.DataFrame(columns=results[0])\n",
    "    \n",
    "    for shift in shift_list:\n",
    "\n",
    "        df_shift=pd.DataFrame()\n",
    "        df_shift[pred_colname] = df[pred_colname].shift(shift)\n",
    "        df_shift[true_colname] =df[true_colname]\n",
    "        df_shift.dropna(inplace=True)      \n",
    "        \n",
    "        shift_results = evaluate_regression(df_shift[true_colname],df_shift[pred_colname]).reset_index()\n",
    "        shift_results.insert(0,'Bins Shifted',shift)\n",
    "        \n",
    "        combined_results = pd.concat([combined_results,shift_results], axis=0)\n",
    "    \n",
    "    combined_results.set_index(['Bins Shifted','Metric'], inplace=True)\n",
    "    return combined_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # results=[['# of Bins Shifted','Metric','Value']]\n",
    "# # shift_df = pd.DataFrame(columns=results[0])\n",
    "# # shift_df\n",
    "# results_tf = evaluate_regression(true_test_series, pred_test_series).reset_index()\n",
    "# results_tf.insert(0,'Bin #',1)\n",
    "# results_tf.set_index(['Bin #','Metric'],inplace=True)\n",
    "# display(results_tf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiating clock timer\n",
    "import bs_ds as bs\n",
    "clock = bs.Clock(verbose=0)\n",
    "\n",
    "# defining loss function to use\n",
    "def my_rmse(y_true,y_pred):\n",
    "    \"\"\"RMSE calculation using keras.backend\"\"\"\n",
    "    from keras import backend as kb\n",
    "    sq_err = kb.square(y_pred - y_true)\n",
    "    mse = kb.mean(sq_err,axis=-1)\n",
    "    rmse =kb.sqrt(mse)\n",
    "    return rmse\n",
    "\n",
    "# CREATING CALLBACKS\n",
    "from keras import callbacks\n",
    "filepath = 'model1_weights.{epoch:02d}.hdf5'\n",
    "checkpoint = callbacks.ModelCheckpoint(filepath=filepath, monitor=my_rmse,mode='min',\n",
    "                                       save_best_only=True, verbose=1)\n",
    "early_stop = callbacks.EarlyStopping(monitor=my_rmse,mode='min',patience=1,min_delta=.001,verbose=1)\n",
    "callbacks = [checkpoint,early_stop]\n",
    "\n",
    "\n",
    "# creating LOG list for forthcoming results\n",
    "LOG = [['Test #','Defining difference','RunTime','Test_RMSE_keras','Test_RMSE_price','acc','Train_RMSE_keras','Train_RMSE_price']]\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Bidirectional, Dense, LSTM, Dropout\n",
    "from keras import optimizers\n",
    "\n",
    "# Specifying input shape (size of samples, rank of samples?)\n",
    "input_shape=(n_input, n_features)\n",
    "\n",
    "# Create model architecture\n",
    "model = Sequential()\n",
    "\n",
    "model.add(LSTM(units=50, return_sequences=True, input_shape =input_shape))\n",
    "model.add(LSTM(units=50, activation='relu'))\n",
    "model.add(Dense(1))\n",
    "\n",
    "# nadam = optimizers.Nadam(lr=0.002,)\n",
    "\n",
    "model.compile(loss=my_rmse, optimizer=optimizers.Nadam(), metrics=['acc', my_rmse])#optimizer=optimizers.Nadam()\n",
    "display(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('---'*40)\n",
    "print('\\tFITTING MODEL:')\n",
    "print('---'*40,'\\n')     \n",
    "# start the timer\n",
    "clock.tic('')\n",
    "\n",
    "# Fit the model\n",
    "history = model.fit_generator(train_generator,epochs=2,verbose=2,callbacks=callbacks,workers=3)\n",
    "\n",
    "model_results = model.history.history\n",
    "\n",
    "# print('Loss per epoch: ',loss_per_epoch)\n",
    "# stop clock\n",
    "clock.toc('')\n",
    "\n",
    "def evaluate_model_plot_history(model, train_generator, test_generator):\n",
    "    \"\"\"Takes a keras model fit using fit_generator(), a train_generator and test generator.\n",
    "    Extracts and plots Keras model.history's metrics.\"\"\"\n",
    "    # # EVALUATE MODEL PREDICTIONS FROM GENERATOR \n",
    "    model_metrics_train = model.evaluate_generator(train_generator)\n",
    "    model_metrics_test = model.evaluate_generator(test_generator)\n",
    "    print('\\n')\n",
    "    print('---'*40)\n",
    "    print('\\tEVALUATE MODEL:')\n",
    "    print('---'*40)\n",
    "    eval_gen_dict = {}\n",
    "    eval_gen_dict['Training Data'] = dict(zip(model.metrics_names,model_metrics_train))\n",
    "    eval_gen_dict['Test Data'] = dict(zip(model.metrics_names,model_metrics_test))\n",
    "\n",
    "    display(pd.DataFrame(eval_gen_dict))\n",
    "\n",
    "    # duration = print(clock._lap_duration_)\n",
    "    model_results = model.history.history\n",
    "    plt.figure(figsize=(6,3))\n",
    "    for k,v in model_results.items():\n",
    "        plt.plot(range(len(v)),v, label=k);\n",
    "    plt.legend()\n",
    "\n",
    "    \n",
    "evaluate_model_plot_history(model, train_generator, test_generator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding Forecasting Quality Statistic - Thiel's U\n",
    "- https://docs.oracle.com/cd/E57185_01/CBREG/ch06s02s03s04.html\n",
    "\n",
    "|Thiel's U Value | Interpretation |\n",
    "| --- | --- |\n",
    "| <1 | Forecasting is better than guessing|\n",
    "| 1 | Forecasting is about as good as guessing|\n",
    "|>1 | Forecasting is worse than guessing|\n",
    "\n",
    "$$U = \\sqrt{\\frac{ \\sum_{t=1 }^{n-1}\\left(\\frac{\\bar{Y}_{t+1} - Y_{t+1}}{Y_t}\\right)^2}{\\sum_{t=1 }^{n-1}\\left(\\frac{Y_{t+1} - Y_{t}}{Y_t}\\right)^2}}$$\n",
    "- ALSO EXAMINE RECOMMENDATIONS FROM THIS FORUM: https://stats.stackexchange.com/questions/351808/lstm-time-series-forecasting-accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.style.use('seaborn-notebook')\n",
    "\n",
    "print('---'*40)\n",
    "print('\\tEVALUATE MODEL:')\n",
    "print('---'*40)\n",
    "# from mod4functions_JMI import plot_keras_history_custom\n",
    "# plot_keras_history_custom(history);\n",
    "# [print(f'{k}={np.round(v,4)}') for k,v in model_results.items()];\n",
    "def get_true_vs_model_preds_df(model, train_generator, test_generator,\n",
    "                         train_data_index, test_data_index, x_window,\n",
    "                         true_test_data = df_test['price'], true_train_data=df_train['price']):\n",
    "    \n",
    "    # GET PREDICTIONS FOR TRAINING DATA AND TEST DATA\n",
    "    test_predictions = arr2series( model.predict_generator(test_generator),\n",
    "                                  test_data_index[x_window:], 'Predicted Test Price')\n",
    "    \n",
    "    train_predictions = arr2series( model.predict_generator(train_generator),\n",
    "                                   train_data_index[x_window:], 'Predicted Train Price')\n",
    "\n",
    "    # GET TRUE TEST AND TRAIN DATA AS SERIES\n",
    "    true_test_price = pd.Series( true_test_data.iloc[x_window:],\n",
    "                                index= test_data_index[x_window:], name='True Test Price')\n",
    "    \n",
    "    true_train_price = pd.Series(true_train_data.iloc[x_window:],\n",
    "                                 index = train_data_index[x_window:], name='True Train Price')\n",
    "\n",
    "    \n",
    "    # COMBINE TRAINING DATA AND TESTING DATA INTO 2 DFS (with correct date axis)\n",
    "    df_true_v_preds_train = pd.concat([true_train_price, train_predictions],axis=1)\n",
    "    df_true_v_preds_test= pd.concat([true_test_price, test_predictions],axis=1)\n",
    "    \n",
    "    return df_true_v_preds_train, df_true_v_preds_test\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "def get_predictions_df_and_evaluate_model(model, train_generator, test_generator,\n",
    "                                          train_data_index, test_data_index, x_window,\n",
    "                                          true_test_data = df_test['price'], true_train_data=df_train['price'],\n",
    "                                          inverse_scale =True, scaler=scaler_library['price'],\n",
    "                                         return_separate=True, plot_results = True):\n",
    "    \n",
    "    # Call helper to get predictions and return as dataframes \n",
    "    df_true_v_preds_train, df_true_v_preds_test = get_true_vs_model_preds_df(\\\n",
    "        model, train_generator, test_generator,\n",
    "        train_data_index, test_data_index, x_window,\n",
    "        true_test_data, true_train_data)\n",
    "    \n",
    "#     # GET PREDICTIONS FOR TRAINING DATA AND TEST DATA\n",
    "#     test_predictions = arr2series( model.predict_generator(test_generator),\n",
    "#                                   test_data_index[x_window:], 'Predicted Test Price')\n",
    "    \n",
    "#     train_predictions = arr2series( model.predict_generator(train_generator),\n",
    "#                                    train_data_index[x_window:], 'Predicted Train Price')\n",
    "\n",
    "#     # GET TRUE TEST AND TRAIN DATA AS SERIES\n",
    "#     true_test_price = pd.Series( true_test_data.iloc[x_window:],\n",
    "#                                 index= test_data_index[x_window:], name='True Test Price')\n",
    "    \n",
    "#     true_train_price = pd.Series(true_train_data.iloc[x_window:],\n",
    "#                                  index = train_data_index[x_window:], name='True Train Price')\n",
    "\n",
    "    \n",
    "#     # COMBINE TRAINING DATA AND TESTING DATA INTO 2 DFS (with correct date axis)\n",
    "#     df_true_v_preds_train = pd.concat([true_train_price, train_predictions],axis=1)\n",
    "#     df_true_v_preds_test = pd.concat([true_test_price, test_predictions],axis=1)\n",
    "    \n",
    "    \n",
    "    # COMBINE TRUE/PRED TRAIN/TEST DATA\n",
    "#     df_model_preds_old = pd.concat([true_train_price,train_predictions,true_test_price,test_predictions], axis=1)\n",
    "    df_model_preds = pd.concat([df_true_v_preds_train, df_true_v_preds_test],axis=1)\n",
    "\n",
    "    ## CONVERT BACK TO DOLLARS AND PLOT\n",
    "    if inverse_scale==True:\n",
    "        df_model = pd.DataFrame()\n",
    "        for col in df_model_preds.columns:\n",
    "            df_model[col] = ji.inverse_transform_series(df_model_preds[col],scaler_library['price']) \n",
    "    else:\n",
    "        df_model = df_model_preds\n",
    "\n",
    "        \n",
    "    if plot_results:\n",
    "        # PLOTTING TRAINING + TRUE/PRED TEST DATA\n",
    "        ji.plot_true_vs_preds_subplots(df_model['True Train Price'],df_model['True Test Price'], \n",
    "                                    df_model['Predicted Test Price'], subplots=True);\n",
    "\n",
    "\n",
    "    # GET EVALUATION METRICS FROM PREDICTIONS\n",
    "    true_test_series = df_model['True Test Price'].dropna()\n",
    "    pred_test_series = df_model['Predicted Test Price'].dropna()\n",
    "    \n",
    "    # Get and display regression statistics\n",
    "    results_tf = evaluate_regression(true_test_series, pred_test_series)\n",
    "    display(results_tf)\n",
    "\n",
    "    return df_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## GET TRUE VS PRED DATA AND INVERSE TRANSFORM RESULTS BACK TO DOLLARS\n",
    "df_true_v_preds_train, df_true_v_preds_test = \\\n",
    "get_true_vs_model_preds_df(model, train_generator, test_generator,\n",
    "                           train_data_index, test_data_index, x_window,\n",
    "                           true_test_data = df_test['price'], \n",
    "                           true_train_data=df_train['price'])\n",
    "\n",
    "df_true_v_preds_train = df_true_v_preds_train.apply(\n",
    "    lambda x: ji.inverse_transform_series(x, scaler_library['price']) )\n",
    "\n",
    "\n",
    "df_true_v_preds_test = df_true_v_preds_test.apply(\n",
    "    lambda x: ji.inverse_transform_series(x, scaler_library['price']) )\n",
    "\n",
    "\n",
    "bs.display_side_by_side(df_true_v_preds_train.agg([min,max]),df_true_v_preds_test.agg([min,max]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check timepoints of train vs test dataframes \n",
    "print(df_true_v_preds_train.index[[0,-1]], df_true_v_preds_test.index[[0,-1]])\n",
    "print(df_true_v_preds_test.index.freq,df_true_v_preds_train.index.freq)\n",
    "\n",
    "bs.display_side_by_side(df_true_v_preds_train.head(2), df_true_v_preds_test.head(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get and display regression statistics\n",
    "results_tf_train = evaluate_regression(\n",
    "    df_true_v_preds_train['Predicted Train Price'], df_true_v_preds_train['True Train Price']) #true_test_series, pred_test_series)\n",
    "\n",
    "results_tf_test = evaluate_regression(\n",
    "    df_true_v_preds_test['Predicted Test Price'], df_true_v_preds_test['True Test Price']) #true_test_series, pred_test_series)\n",
    "\n",
    "\n",
    "bs.display_side_by_side(results_tf_train, results_tf_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #TROUBLESHOOTING INVERSE TRANSFORMING DATA\n",
    "# ji.reload(ji)\n",
    "\n",
    "# # Examining contents of scaler \n",
    "# scaler = scaler_library['price']\n",
    "# scaler.data_min_, scaler.data_max_, scaler.feature_range\n",
    "\n",
    "# # DISPLAY THE RANGE OF THE PRICE DATA IN THE TEST AND TRAINING SET\n",
    "# train_ranges = df_true_v_preds_train.agg([min, max]).style.set_caption('TRAINING DATA RANGES')\n",
    "# test_ranges = df_true_v_preds_test.agg([min, max]).style.set_caption('TEST DATA RANGES')\n",
    "\n",
    "# bs.display_side_by_side(train_ranges,test_ranges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Original All-in-One Function \n",
    "df_model1 = get_predictions_df_and_evaluate_model(model, train_generator, test_generator, train_data_index,\n",
    "                                       test_data_index, x_window )\n",
    "display(df_model1.head(2),df_model1.tail(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def compare_time_shifted_model(df_model,true_colname='True Test Price',pred_colname='Predicted Test Price',\n",
    "                               shift_list=[-4,-3,-2,-1,0,1,2,3,4]):\n",
    "    \n",
    "    # GET EVALUATION METRICS FROM PREDICTIONS\n",
    "    true_test_series = df_model[true_colname].dropna()\n",
    "    pred_test_series = df_model[pred_colname].dropna()\n",
    "    \n",
    "    # Comparing Shifted Timebins\n",
    "    res_df = compare_eval_metrics_for_shifts(true_test_series, pred_test_series,shift_list=np.arange(-4,4,1))\n",
    "    res_U = compare_u_for_shifts(true_test_series,pred_test_series,shift_list=np.arange(-4,4,1),\n",
    "                                 plot_all=True)#,plot_best=True)\n",
    "    display(res_U)\n",
    "    \n",
    "    res_df = res_df.swaplevel(i=-2, j=-2, axis=0).unstack(-1)\n",
    "    res_df = color_cols(res_df)#, subset=['R_squared'])\n",
    "    display(res_df)\n",
    "    return res_df;\n",
    "\n",
    "compare_time_shifted_model(df_model1);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipywidgets import interact\n",
    "functions = {'compare_time_shifted_model':compare_time_shifted_model,\n",
    "    'compare_eval_metrics_for_shifts':compare_eval_metrics_for_shifts,\n",
    "              'compare_u_for_shifts':compare_u_for_shifts}\n",
    "interact(ji.ihelp, any_function = functions);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bookmark -fixing time series ticks\n",
    "\n",
    "- https://matplotlib.org/examples/api/date_demo.html\n",
    "- https://stackoverflow.com/questions/11264521/date-ticks-and-rotation-in-matplotlib ***\n",
    "- https://python4astronomers.github.io/plotting/advanced.html#controlling-the-appearance-of-plots\n",
    "\n",
    "### Notes: Using Matplotlib Tick Formatters\n",
    "\n",
    "`fig.autofmt_xdate(which='both',rotation=30)`\n",
    "\n",
    "- *Using `matplotlib.dates` and `AutoDateLocator`\n",
    "```python\n",
    "import matplotlib.dates as mdates\n",
    "locator = mdates.AutoDateLocator()\n",
    "ax1.xaxis.set_major_locator(locator)\n",
    "ax1.tick_params(axis='x',rotation=30)\n",
    "# ax2.xaxis.set_major_locator(locator)\n",
    "ax2.tick_params(axis='x',rotation=30)\n",
    "fig\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig_,ax1_,ax2_ = ji.plot_true_vs_preds_subplots(df_model1['True Train Price'],df_model1['True Test Price'], \n",
    "#                                 df_model1['Predicted Test Price'], subplots=True,figsize=(14,5));\n",
    "# import matplotlib.dates as mdates\n",
    "# locator = mdates.AutoDateLocator()\n",
    "# ax1_.xaxis.set_major_locator(locator)\n",
    "# ax1_.tick_params(axis='x',rotation=30)\n",
    "# # ax2.xaxis.set_major_locator(locator)\n",
    "# ax2_.tick_params(axis='x',rotation=30)\n",
    "\n",
    "# plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ji.reload(ji)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## REFORMAT AND IMPROVE PLOT VISUALS\n",
    "fig,ax1,ax2 = ji.plot_true_vs_preds_subplots(\n",
    "    df_model1['True Train Price'], df_model1['True Test Price'],\n",
    "    df_model1['Predicted Test Price'], subplots=True,figsize=(14,5));\n",
    "\n",
    "\n",
    "## MY BEST WORKING REFORNATTED TICKS\n",
    "import matplotlib.dates as mdates\n",
    "import datetime\n",
    "\n",
    "# Instantiate Locators to be used\n",
    "years = mdates.YearLocator()   # every year\n",
    "months = mdates.MonthLocator()#interval=2)  # every month\n",
    "quarters = mdates.MonthLocator(interval=3)#interval=2)  # every month\n",
    "\n",
    "# Define various date formatting to be used\n",
    "monthsFmt = mdates.DateFormatter('%Y-%b')\n",
    "yearsFmt = mdates.DateFormatter('%Y') #'%Y')\n",
    "yr_mo_day_fmt = mdates.DateFormatter('%Y-%m')\n",
    "monthDayFmt = mdates.DateFormatter('%m-%d-%y')\n",
    "\n",
    "\n",
    "## AX2 SET TICK LOCATIONS AND FORMATTING\n",
    "\n",
    "# Set locators (since using for both location and formatter)\n",
    "auto_major_loc = mdates.AutoDateLocator(minticks=5)\n",
    "auto_minor_loc = mdates.AutoDateLocator(minticks=10)\n",
    "\n",
    "# Set Major X Axis Ticks\n",
    "ax1.xaxis.set_major_locator(auto_major_loc)\n",
    "ax1.xaxis.set_major_formatter(mdates.AutoDateFormatter(auto_major_loc))\n",
    "\n",
    "# Set Minor X Axis Ticks\n",
    "ax1.xaxis.set_minor_locator(auto_minor_loc)\n",
    "ax1.xaxis.set_major_formatter(mdates.AutoDateFormatter(auto_minor_loc))\n",
    "\n",
    "\n",
    "ax1.tick_params(axis='x',which='both',rotation=30)\n",
    "# ax1.tick_params(axis='x',which='major',pad=15)\n",
    "ax1.grid(axis='x',which='major')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## AX2 SET TICK LOCATIONS AND FORMATTING\n",
    "\n",
    "# Major X-Axis Ticks\n",
    "ax2.xaxis.set_major_locator(months) #mdates.DayLocator(interval=5))\n",
    "ax2.xaxis.set_major_formatter(mdates.DateFormatter('%b-%Y')) #monthsFmt) #mdates.DateFormatter('%m-%Y')) #AutoDateFormatter(locator=locator))#yearsFmt)\n",
    "\n",
    "# Minor X-Axis Ticks\n",
    "ax2.xaxis.set_minor_locator(mdates.DayLocator(interval=5))#,interval=5))\n",
    "ax2.xaxis.set_minor_formatter(mdates.DateFormatter('%d')) #, fontDict={'weight':'bold'})\n",
    "\n",
    "# Changing Tick spacing and rotation.\n",
    "ax2.tick_params(axis='x',which='major',rotation=90, direction='inout',length=10, pad=5)\n",
    "ax2.tick_params(axis='x',which='minor',length=4,pad=2, direction='in') #,horizontalalignment='right')#,ha='left')\n",
    "ax2.grid(axis='x',which='major')\n",
    "\n",
    "# fig.autofmt_xdate(which='both',rotation=30)\n",
    "\n",
    "plt.show()\n",
    "# fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prior attempts at editing ticks\n",
    "\n",
    "```python\n",
    "ax1.xaxis.set_major_locator(plt.MaxNLocator(6))\n",
    "\n",
    "ax1.xaxis.set_major_formatter(mpl.ticker.StrMethodFormatter(\"{x}\"))\n",
    "ax1.tick_params(axis='x',rotation=45) #get_majorticklabels(), rotation=70 )\n",
    "ax1.xaxis.set_major_formatter(mpl.ticker.StrMethodFormatter(\"{x}\"))\n",
    "\n",
    "ax1.format_xdata = mdates.DateFormatter('%Y-%m-%d')\n",
    "fig.autofmt_xdate(rotation=30)\n",
    "\n",
    "# StrMethod formatter\n",
    "# ax.xaxis.set_major_locator(ticker.MultipleLocator(1.00))\n",
    "ax.xaxis.set_minor_locator(mpl.ticker.MultipleLocator(0.25))\n",
    "ax.xaxis.set_major_formatter(mpl.ticker.StrMethodFormatter(\"{x}\"))\n",
    "ax.text(0.0, 0.1, \"StrMethodFormatter('{x}')\",\n",
    "        fontsize=15, transform=ax.transAxes)\n",
    "# import \n",
    "\n",
    "ax2.xaxis.set_major_locator(mpl.ticker.NullLocator())\n",
    "\n",
    "# fig\n",
    "fig\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make a plotly version of `plot_true_vs_preds_subplots`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "help(px.line())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ji.ihelp(ji.plotly_time_series, show_help=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotly_true_vs_preds_suplots(stock_df,\n",
    "                                 WORK IN PROGRESS\n",
    "                                 title=None,x_col='date_time_index', y_col='price',name='S&P500 Price'):\n",
    "    import plotly\n",
    "    import plotly.offline as py\n",
    "    import plotly.tools as tls\n",
    "    import plotly.graph_objs as go\n",
    "\n",
    "    py.init_notebook_mode(connected=True)\n",
    "\n",
    "    # %matplotlib inline\n",
    "\n",
    "    # LEARNING HOW TO CUSTOMIZE SLIDER\n",
    "    # ** https://plot.ly/python/range-slider/    \n",
    "    fig = go.Figure()\n",
    "\n",
    "    # Set title\n",
    "    if title is None:\n",
    "        title = \"Time series with range slider and selectors\"\n",
    "\n",
    "    fig.update_layout(\n",
    "        title_text=title\n",
    "    )\n",
    "\n",
    "    fig.add_trace(go.Scatter(x=stock_df[x_col], y=stock_df[y_col], name=name)) #df.Date, y=df['AAPL.Low'], name=\"AAPL Low\",\n",
    "    #                          line_color='dimgray'))\n",
    "    # Add range slider\n",
    "    fig.update_layout(\n",
    "        xaxis=go.layout.XAxis(\n",
    "\n",
    "            rangeselector=dict(\n",
    "                buttons=list([\n",
    "                    dict(count=1,\n",
    "                        label=\"1m\",\n",
    "                        step=\"month\",\n",
    "                        stepmode=\"backward\"),\n",
    "                    dict(count=6,\n",
    "                        label=\"6m\",\n",
    "                        step=\"month\",\n",
    "                        stepmode=\"backward\"),\n",
    "                    dict(count=1,\n",
    "                        label=\"YTD\",\n",
    "                        step=\"year\",\n",
    "                        stepmode=\"todate\"),\n",
    "                    dict(count=1,\n",
    "                        label=\"1y\",\n",
    "                        step=\"year\",\n",
    "                        stepmode=\"backward\"),\n",
    "                    dict(step=\"all\")\n",
    "                ])\n",
    "            ),\n",
    "            rangeslider=dict(\n",
    "                visible=True\n",
    "            ),\n",
    "            type=\"date\"\n",
    "        ),\n",
    "\n",
    "        yaxis = go.layout.YAxis(\n",
    "                    title=go.layout.yaxis.Title(\n",
    "                        text = 'S&P500 Price',\n",
    "                        font=dict(\n",
    "                            # family=\"Courier New, monospace\",\n",
    "                            size=18,\n",
    "                            color=\"#7f7f7f\")\n",
    "                    )\n",
    "            )\n",
    "    )\n",
    "    fig.show()\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "## OLD FUNCTION (TOO MUCH IN ONE)\n",
    "    # from functions_combined_BEST import get_true_vs_model_pred_df\n",
    "    # df_model = get_true_vs_model_pred_df(model, n_input, test_generator,test_data_index,df_test,\n",
    "    #                                      train_generator,train_data_index, df_train,\n",
    "    #                                      scaler_library['price'],inverse_tf=True, plot=True)\n",
    "\n",
    "    # # plot_true_vs_preds_subplots(df_model_preds['True Train Price'],df_model_preds['True Test Price'], df_model_preds['Predicted Test Price'], subplots=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KERAS MODEL 2 - COMBINING STOCK PRICE AND INDICATORS\n",
    "\n",
    "- [ ] already have data in df_train/test just take full dataframe as X (and just 'price' as y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stock_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########## SET TRAINING AND TEST SET PARAMETERS: ############\n",
    "num_test_days=45 # Number of days for test data \n",
    "num_train_days=365 # Number of days for training data - 5 days/week * 52 weeks\n",
    "days_for_x_window = 1 # Number of days to \n",
    "############################################################\n",
    "\n",
    "# Calculate number of rows to bin for x_windows\n",
    "periods_per_day = get_day_window_size_from_freq(stock_df, custom_BH_freq()) # get the # of rows that == 1 day\n",
    "x_window = periods_per_day * days_for_x_window \n",
    "print(f'X_window size = {x_window} -- ({days_for_x_window} day(s) * {periods_per_day} rows/day)\\n')\n",
    "    \n",
    "###### TRAIN TEST SPLIT BY NUMBER OF DAYS ######\n",
    "df_train, df_test = train_test_split_by_last_days(stock_df.drop('filled_timebin',axis=1), periods_per_day=periods_per_day, num_test_days=num_test_days,\n",
    "                                                  num_train_days=num_train_days,verbose=1, plot=True)\n",
    "\n",
    "\n",
    "###### RESCALE DATA USING MinMaxScalers FIT ON TRAINING DATA's COLUMNS ######\n",
    "scaler_library , df_train = make_scaler_library(df_train,transform=True)\n",
    "# Transform test data with train data's scaler_library\n",
    "df_test = transform_cols_from_library(df_test,scaler_library)\n",
    "\n",
    "## Display Preview of Scaled Dataset\n",
    "# display(df_train.head(2).style.set_caption('df_train'),df_test.head(2).style.set_caption('df_test') )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## REPLACE MADE_DF_TIMESERIES_BINS_BY_COLUMN with Kera's TimeseriesGenerator\n",
    "from keras.preprocessing.sequence import TimeseriesGenerator\n",
    "\n",
    "# Define shape of data by specifing these vars:\n",
    "n_input = x_window # Number of timebins to analyze at once. Note: try to choose # greater than length of seasonal cycles\n",
    "n_features= df_train.shape[1] # Number of columns\n",
    "batch_size = 1 # Generally 1 for sequence data\n",
    "\n",
    "# RESHAPING TRAINING AND TEST DATA \n",
    "# train_data = df_train.values #.reshape(-1,1)\n",
    "# test_data = df_testvalues #.reshape(-1,1)\n",
    "train_data_index =  df_train.index\n",
    "test_data_index = df_test.index\n",
    "\n",
    "\n",
    "## Create Generator for Training Data\n",
    "train_generator = TimeseriesGenerator(data=df_train.values, targets=df_train['price'].values,# reshape(-1,1),\n",
    "                                      length=n_input, batch_size=batch_size )\n",
    "\n",
    "test_generator = TimeseriesGenerator(data=df_test.values, targets=df_test['price'].values, #reshape(-1,1),\n",
    "                                     length=n_input, batch_size=batch_size )\n",
    "\n",
    "\n",
    "# What does the first batch look like?\n",
    "X,y = train_generator[0]\n",
    "print(f'Given the Array: \\t(with shape={X.shape})')#' \\n{X.flatten()}')\n",
    "print(f'\\nPredict this y: \\n {y}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Bidirectional, Dense, LSTM\n",
    "\n",
    "# Specifying input shape (size of samples, rank of samples?)\n",
    "# input_shape =  (X_train_in.shape[1],1)\n",
    "n_input = x_window # number of time bins in a day * number of days\n",
    "n_features = df_train.shape[1]\n",
    "\n",
    "input_shape=(n_input, n_features)\n",
    "\n",
    "\n",
    "# Create model architecture\n",
    "model2 = Sequential()\n",
    "\n",
    "\n",
    "model2.add(LSTM(units=100, return_sequences=True, input_shape =input_shape))\n",
    "model2.add(LSTM(units=50))\n",
    "model.add(Dense(10))\n",
    "model2.add(Dense(1))\n",
    "\n",
    "model2.compile(loss=my_rmse, optimizer=optimizers.Nadam(), metrics=['acc'])#,my_rmse])#,metrics=['acc'])\n",
    "display(model2.summary())\n",
    "\n",
    "# start the timer\n",
    "clock.tic('')\n",
    "\n",
    "# Fit the model\n",
    "# history = model2.fit_generator(X_train_in, y_train, epochs=2, verbose=0, validation_split=(0.25))\n",
    "history = model2.fit_generator(train_generator,epochs=3,verbose=2,callbacks=callbacks,workers=3)\n",
    "\n",
    "# stop clock\n",
    "clock.toc('')\n",
    "duration = print(clock._lap_duration_)\n",
    "\n",
    "# from mod4functions_JMI import plot_keras_history\n",
    "# plot_keras_history(history);\n",
    "\n",
    "\n",
    "# # EVALUATE MODEL PREDICTIONS FROM GENERATOR \n",
    "model_metrics_train = model2.evaluate_generator(train_generator)\n",
    "model_metrics_test = model2.evaluate_generator(test_generator)\n",
    "print('\\n')\n",
    "print('---'*40)\n",
    "print('\\tEVALUATE MODEL:')\n",
    "print('---'*40)\n",
    "eval_gen_dict = {}\n",
    "eval_gen_dict['Training Data'] = dict(zip(model2.metrics_names,model_metrics_train))\n",
    "eval_gen_dict['Test Data'] = dict(zip(model2.metrics_names,model_metrics_test))\n",
    "\n",
    "display(pd.DataFrame(eval_gen_dict))\n",
    "\n",
    "# duration = print(clock._lap_duration_)\n",
    "plt.figure(figsize=(6,3))\n",
    "for k,v in model_results.items():\n",
    "    plt.plot(range(len(v)),v, label=k);\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ji.reload(ji)\n",
    "print('---'*40)\n",
    "print('\\tEVALUATE MODEL:')\n",
    "print('---'*40)\n",
    "# from mod4functions_JMI import plot_keras_history_custom\n",
    "# plot_keras_history_custom(history);\n",
    "# [print(f'{k}={np.round(v,4)}') for k,v in model_results.items()];\n",
    "\n",
    "# GET PREDICTIONS FOR TRAINING DATA AND TEST DATA\n",
    "test_predictions = ji.arr2series(model2.predict_generator(test_generator), test_data_index[x_window:], 'Predicted Test Price')\n",
    "train_predictions = ji.arr2series(model2.predict_generator(train_generator), train_data_index[x_window:], 'Predicted Train Price')\n",
    "\n",
    "# GET TRUE TEST AND TRAIN DATA AS SERIES\n",
    "true_test_price = pd.Series( df_test['price'].iloc[n_input:], index= test_data_index[x_window:], name='True Test Price')\n",
    "true_train_price = pd.Series(df_train['price'].iloc[n_input:],index = train_data_index[x_window:], name='True Train Price')\n",
    "\n",
    "# COMBINE TRUE/PRED TRAIN/TEST DATA\n",
    "df_model_preds = pd.concat([true_train_price,train_predictions,true_test_price,test_predictions], axis=1)\n",
    "\n",
    "## CONVERT BACK TO DOLLARS AND PLOT\n",
    "df_model_preds_tf = pd.DataFrame()\n",
    "for col in df_model_preds.columns:\n",
    "    df_model_preds_tf[col] = inverse_transform_series(df_model_preds[col],scaler_library['price']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "# PLOTTING TRAINING + TRUE/PRED TEST DATA\n",
    "ji.plot_true_vs_preds_subplots(df_model_preds_tf['True Train Price'],df_model_preds_tf['True Test Price'], \n",
    "                            df_model_preds_tf['Predicted Test Price'], subplots=True);    \n",
    "\n",
    "\n",
    "# GET EVALUATION METRICS FROM PREDICTIONS\n",
    "true_test_series = df_model_preds_tf['True Test Price'].dropna()\n",
    "pred_test_series = df_model_preds_tf['Predicted Test Price'].dropna()\n",
    "results_tf = evaluate_regression(true_test_series, pred_test_series)\n",
    "display(results_tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# COMPARE SHIFTED VALUES FOR THIEL's U\n",
    "res_u = compare_u_for_shifts(true_test_price,test_predictions,shift_list=np.arange(-4,4,1), plot_all=False,plot_best=True)\n",
    "display(res_u)\n",
    "# compare_u_for_shifts(true_test_price,test_predictions,shift_list=np.arange(-4,4,1), plot_all=False,plot_best=True)\n",
    "res_df = compare_eval_metrics_for_shifts(true_test_price, test_predictions,shift_list=np.arange(-4,4,1))\n",
    "\n",
    "res_df_ = res_df.swaplevel(i=-2, j=-2, axis=0).unstack(-1)\n",
    "# res_df_.index\n",
    "display(res_df_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_model_and_weights(model2,'stock_plus_indicators_U39_shift-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "___\n",
    "\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Blog Post Code\n",
    "- NEW BLOG POST - LSTM TIME SERIES FORECASTING\n",
    "    - https://machinelearningmastery.com/how-to-develop-lstm-models-for-time-series-forecasting/\n",
    "- https://www.analyticsvidhya.com/blog/2018/10/predicting-stock-price-machine-learningnd-deep-learning-techniques-python/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from keras.layers import TimeDistributed, Conv1D, MaxPool1D, Flatten, LSTM, Dense, MaxPooling1D\n",
    "model3 = Sequential()\n",
    "# input_shape =  (X_train_in.shape[0],X_train_in.shape[1],1)\n",
    "input_shape=(n_input, n_features,1)\n",
    "\n",
    "model3.add(TimeDistributed(Conv1D(filters=64, kernel_size=1, activation='relu'), input_shape=input_shape)) #(None, n_steps, n_features)))\n",
    "model3.add(TimeDistributed(MaxPooling1D(pool_size=2)))\n",
    "model3.add(TimeDistributed(Flatten()))\n",
    "model3.add(LSTM(50, activation='relu'))\n",
    "model3.add(Dense(1))\n",
    "model3.compile(optimizer='adam', loss=my_rmse)\n",
    "model3.summary()\n",
    "# fit model\n",
    "\n",
    "clock.tic('')\n",
    "history3 = model3.fit_generator( train_generator,epochs=3,verbose=2,callbacks=callbacks,workers=3)\n",
    "\n",
    "clock.toc('')\n",
    "evaluate_model_plot_history(model3, train_generator, test_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#(X_train_in, y_train, epochs=500, verbose=0)\n",
    "# demonstrate prediction\n",
    "# x_input = array([60, 70, 80, 90])\n",
    "# x_input = x_input.reshape((1, n_seq, n_steps, n_features))\n",
    "\n",
    "# yhat = model.predict(X_test_in, verbose=0)\n",
    "# print(yhat)\n",
    "\n",
    "# predictions = pd.Series(predictions.ravel(),name='predicted_price',index=index_test)\n",
    "# true_price =  pd.Series(y_test,name='true_price',index=index_test)\n",
    "\n",
    "# df_predictions = pd.concat([predictions,true_price],axis=1)#, columns=['predicted_price','true_price'], index=index_test)\n",
    "# display(df_predictions.head())\n",
    "\n",
    "# # Plot outcome\n",
    "# mpl.rcParams['figure.figsize']=(12,4)\n",
    "# plt.plot(df_train_bins['price_labels'])\n",
    "# plt.plot(df_predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "print('---'*40)\n",
    "print('\\tEVALUATE MODEL:')\n",
    "print('---'*40)\n",
    "# from mod4functions_JMI import plot_keras_history_custom\n",
    "# plot_keras_history_custom(history);\n",
    "# [print(f'{k}={np.round(v,4)}') for k,v in model_results.items()];\n",
    "\n",
    "# GET PREDICTIONS FOR TRAINING DATA AND TEST DATA\n",
    "test_predictions = arr2series(model3.predict_generator(test_generator), test_data_index[x_window:], 'Predicted Test Price')\n",
    "train_predictions = arr2series(model3.predict_generator(train_generator), train_data_index[x_window:], 'Predicted Train Price')\n",
    "\n",
    "# GET TRUE TEST AND TRAIN DATA AS SERIES\n",
    "true_test_price = pd.Series( df_test['price'].iloc[n_input:], index= test_data_index[x_window:], name='True Test Price')\n",
    "true_train_price = pd.Series(df_train['price'].iloc[n_input:],index = train_data_index[x_window:], name='True Train Price')\n",
    "\n",
    "# COMBINE TRUE/PRED TRAIN/TEST DATA\n",
    "df_model_preds = pd.concat([true_train_price,train_predictions,true_test_price,test_predictions], axis=1)\n",
    "\n",
    "## CONVERT BACK TO DOLLARS AND PLOT\n",
    "df_model_preds_tf = pd.DataFrame()\n",
    "for col in df_model_preds.columns:\n",
    "    df_model_preds_tf[col] = inverse_transform_series(df_model_preds[col],scaler_library['price']) \n",
    "\n",
    "    \n",
    "# PLOTTING TRAINING + TRUE/PRED TEST DATA\n",
    "plot_true_vs_preds_subplots(df_model_preds_tf['True Train Price'],df_model_preds_tf['True Test Price'], \n",
    "                            df_model_preds_tf['Predicted Test Price'], subplots=True);    \n",
    "\n",
    "\n",
    "# GET EVALUATION METRICS FROM PREDICTIONS\n",
    "true_test_series = df_model_preds_tf['True Test Price'].dropna()\n",
    "pred_test_series = df_model_preds_tf['Predicted Test Price'].dropna()\n",
    "results_tf = evaluate_regression(true_test_series, pred_test_series)\n",
    "display(results_tf)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "STOP\n",
    "# PREVIEW DATA CONTENTS\n",
    "# display(df_train_bins.head())\n",
    "df_train_bins['price_labels'].plot()\n",
    "df_test_bins['price_labels'].plot()\n",
    "\n",
    "\n",
    "print('Train Index: ',df_train_bins.index[[0,-1]])\n",
    "print('Test Index: ',df_test_bins.index[[0,-1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#importing required libraries\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, LSTM\n",
    "\n",
    "time_index =stock_df.index.values\n",
    "stock_df['date'] = time_index# .astype('str')\n",
    "\n",
    "#creating dataframe\n",
    "data = stock_df[['date','price']]\n",
    "\n",
    "new_data = pd.DataFrame(index=range(0,len(stock_df)),columns=['date', 'price'])\n",
    "display(new_data.head()), display(data.head())\n",
    "\n",
    "for i in range(0,len(data)):\n",
    "    new_data['date'][i] = data['date'][i]\n",
    "    new_data['price'][i] = data['price'][i]\n",
    "\n",
    "#setting index\n",
    "new_data.index = new_data.date\n",
    "new_data.drop('date', axis=1, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "new_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#creating train and test sets\n",
    "dataset = new_data.values\n",
    "\n",
    "train = dataset[0:987,:]\n",
    "valid = dataset[987:,:]\n",
    "\n",
    "#converting dataset into x_train and y_train\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "scaled_data = scaler.fit_transform(dataset)\n",
    "\n",
    "x_train, y_train = [], []\n",
    "for i in range(60,len(train)):\n",
    "    x_train.append(scaled_data[i-60:i,0])\n",
    "    y_train.append(scaled_data[i,0])\n",
    "x_train, y_train = np.array(x_train), np.array(y_train)\n",
    "print(x_train.shape, y_train.shape)\n",
    "x_train = np.reshape(x_train, (x_train.shape[0],x_train.shape[1],1))\n",
    "print(x_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# create and fit the LSTM network\n",
    "model = Sequential()\n",
    "model.add(LSTM(units=50, return_sequences=True, input_shape=(x_train.shape[1],1)))\n",
    "model.add(LSTM(units=50))\n",
    "model.add(Dense(1))\n",
    "\n",
    "model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "model.fit(x_train, y_train, epochs=1, batch_size=1, verbose=2)\n",
    "\n",
    "#predicting 246 values, using past 60 from the train data\n",
    "inputs = new_data[len(new_data) - len(valid) - 60:].values\n",
    "inputs = inputs.reshape(-1,1)\n",
    "inputs  = scaler.transform(inputs)\n",
    "\n",
    "X_test = []\n",
    "for i in range(60,inputs.shape[0]):\n",
    "    X_test.append(inputs[i-60:i,0])\n",
    "X_test = np.array(X_test)\n",
    "\n",
    "X_test = np.reshape(X_test, (X_test.shape[0],X_test.shape[1],1))\n",
    "closing_price = model.predict(X_test)\n",
    "closing_price = scaler.inverse_transform(closing_price)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "rms=np.sqrt(np.mean(np.power((valid-closing_price),2)))\n",
    "rms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#for plotting\n",
    "train = new_data[:987]\n",
    "valid = new_data[987:]\n",
    "valid['Predictions'] = closing_price\n",
    "plt.plot(train['price'])\n",
    "plt.plot(valid[['price','Predictions']])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9Pu-SNrTHodf"
   },
   "source": [
    "# ✔ BOOKMARK - RESUMING ANALYSIS WITH NEW FUNCTIONS\n",
    "- Check Evernote log: https://www.evernote.com/l/AApgV1EaL9lEYpujWnwHq62W9QyEKkH4hwA/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load In NLP Twitter Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "STOP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from functions_combined_BEST import *\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns',None)\n",
    "df_tokenize = pd.read_csv('twitter_df_tokenized_with_price.csv',index_col=0, parse_dates=True)\n",
    "df_tokenize.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word2Vec \n",
    "filename = 'word2vec_model_twitter.pickle'\n",
    "import pickle\n",
    "with open(filename,'rb') as f:\n",
    "    word_model,vector_size,embedding_matrix = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Loading in twitter LSTM model\n",
    "from keras.models import model_from_json\n",
    "with open ('bestNLPmodel_best_stock_increase.json','r') as f:\n",
    "    twitter_model = model_from_json(f.read())\n",
    "    twitter_model.load_weights('bestNLPmodel_best_stock_increase.h5')\n",
    "    \n",
    "for i, model_layer in enumerate(twitter_model.layers):\n",
    "    twitter_model.get_layer(index=i).trainable=False\n",
    "    print(model_layer,twitter_model.get_layer(index=i).trainable)\n",
    "    \n",
    "    \n",
    "twitter_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LOADING IN TWITTER DATA TO GROUP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ihelp(ji.load_raw_twitter_file), ihelp(ji.full_twitter_df_processing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import functions_combined_BEST as ji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ji.ihelp(ji.full_twitter_df_processing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### LOAD IN AND PROCESS TWITTER DATA\n",
    "ji.reload(ji)\n",
    "twitter_df= ji.load_raw_twitter_file()\n",
    "twitter_df = ji.full_twitter_df_processing(twitter_df,cleaned_tweet_col='clean_content')\n",
    "\n",
    "# cols_to_drop = ['has_RT','starts_RT','content_starts_RT','content_hashtags','content_mentions']\n",
    "# twitter_df.drop(cols_to_drop,axis=1,inplace=True)\n",
    "\n",
    "display(twitter_df.head())\n",
    "print(twitter_df.index[[0,-1]],twitter_df.index.freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ofst = pd.offsets.CustomBusinessHour(start='09:30',end='16:30',)\n",
    "# # ofstBH = pd.offsets.BusinessHour(n=1, start='09:30',end='16:30')\n",
    "# # ts = pd.to_datetime('2019-07-11 14:23')\n",
    "# # ofst.rollback(ts)\n",
    "# num_offset=1\n",
    "# twitter_df['date']\n",
    "# freq=pd.offsets.CustomBusinessHour(n=num_offset,start='09:30',end='16:30')\n",
    "# ofst = pd.offsets.CustomBusinessHour(n=num_offset,start='09:30',end='16:30') #freq=ji.custom_BH_freq()\n",
    "# ofst.rollback(twitter_df['date'].iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     103,
     143
    ]
   },
   "outputs": [],
   "source": [
    "\n",
    "def make_time_index_intervals(twitter_df,col ='date', start=None,end=None, freq='CBH',num_offset=1):\n",
    "    \"\"\"Takes a df, rounds first timestamp down to nearest hour, last timestamp rounded up to hour.\n",
    "    Creates 30 minute intervals based that encompass all data.\"\"\"\n",
    "    import pandas as pd\n",
    "    \n",
    "    \n",
    "    if freq=='CBH':\n",
    "        freq=pd.offsets.CustomBusinessHour(n=num_offset,start='09:30',end='16:30')\n",
    "        ofst = pd.offsets.CustomBusinessHour(n=num_offset,start='09:30',end='16:30') #freq=ji.custom_BH_freq()\n",
    "        ofst_early = pd.offsets.CustomBusinessHour(n=-num_offset,start='09:30',end='16:30') #freq=ji.custom_BH_freq()\n",
    "    if freq=='T':\n",
    "        ofst = pd.offsets.Minute(n=num_offset)\n",
    "        ofst_early = pd.offsets.Minute(n=-num_offset)\n",
    "        \n",
    "    if freq=='H':\n",
    "        ofst = pd.offsets.Hour(n=num_offset)\n",
    "        ofst_early=pd.offsets.Hour(n=-num_offset)\n",
    "\n",
    "#     print(ofst)\n",
    "    if start is None:\n",
    "        # Get timebin before the first timestamp that starts     \n",
    "        start_idx = ofst.rollback(twitter_df[col].iloc[0].floor('H'))\n",
    "    else:\n",
    "        start_idx = pd.to_datetime(start)\n",
    "\n",
    "    if end is None:\n",
    "        # Get timbin after last timestamp that starts 30m into the hour.\n",
    "        end_idx= ofst.rollforward(twitter_df[col].iloc[-1].ceil('H'))\n",
    "    else:\n",
    "        end_idx = pd.to_datetime(end)\n",
    "\n",
    "\n",
    "    # Make time bins using the above start and end points \n",
    "    print(f'start:{start_idx}, end:{end_idx}, freq:{freq}')\n",
    "    time_range = pd.date_range(start =start_idx, end = end_idx, freq=freq)#.to_period()\n",
    "    time_intervals = pd.interval_range(start=start_idx, end=end_idx,freq=freq,name='CBH_intervals',closed='left')\n",
    "    \n",
    "    return time_intervals\n",
    "\n",
    "\n",
    "def int_to_ts(int_list, as_datetime=False, as_str=True):\n",
    "    \"\"\"Helper function: accepts one Panda's interval and returns the left and right ends as either strings or Timestamps.\"\"\"\n",
    "    import pandas as pd\n",
    "    if as_datetime & as_str:\n",
    "        raise Exception('Only one of `as_datetime`, or `as_str` can be True.')\n",
    "    \n",
    "    left_edges =[]\n",
    "    right_edges= []\n",
    "    \n",
    "    for interval in int_list:\n",
    "        \n",
    "        int_str = interval.__str__()[1:-1]\n",
    "        output = int_str.split(',')\n",
    "        left_edges.append(output)\n",
    "#         right_edges.append(right)\n",
    "        \n",
    "    \n",
    "    if as_str:\n",
    "        return left_edges#, right_edges\n",
    "    \n",
    "    elif as_datetime:\n",
    "        left = pd.to_datetime(left)\n",
    "        right = pd.to_datetime(right)\n",
    "        return left,right\n",
    "    \n",
    "\n",
    "def bin_df_by_date_intervals(test_df,time_intervals,column='date',roll_freq='CBH'):\n",
    "    \"\"\"Uses pd.cut with half_hour_intervals on specified column.\n",
    "    Creates a dictionary/map of integer bin codes. \n",
    "    Adds column\"int_bins\" with int codes.\n",
    "    Adds column \"left_edge\" as datetime object representing the beginning of the time interval. \n",
    "    Returns the updated test_df and a list of bin_codes.\"\"\"\n",
    "    import pandas as pd\n",
    "    \n",
    "    if roll_freq=='CBH':\n",
    "        ofst = pd.offsets.CustomBusinessHour(start='09:30',end='16:30')\n",
    "        \n",
    "        # Make a rolled_date column to use for bining\n",
    "        test_df['rolled_date'] = test_df[column].apply(lambda x: ofst.rollforward(x))\n",
    "    \n",
    "    # Cut The Date column into interval bins, \n",
    "    cut_date = pd.cut(test_df['rolled_date'], bins=time_intervals)#,labels=list(range(len(half_hour_intervals))), retbins=True)\n",
    "    test_df['int_times'] = cut_date    \n",
    "    print(test_df.isna().sum())\n",
    "    test_df.dropna(inplace=True)\n",
    "    # convert to str to be used as group names/codes\n",
    "    unique_bins = cut_date.astype('str').unique()\n",
    "    num_code = list(range(len(unique_bins)))\n",
    "    \n",
    "    # Dictioanry of number codes to be used for interval groups\n",
    "    bin_codes = dict(zip(num_code,unique_bins))#.astype('str')\n",
    "\n",
    "    \n",
    "    # Mapper dictionary to convert intervals into number codes\n",
    "    bin_codes_mapper = {v:k for k,v in bin_codes.items()}\n",
    "\n",
    "    # Add column to the dataframe, then map integer code onto it\n",
    "    test_df['int_bins'] = test_df['int_times'].astype('str').map(bin_codes_mapper)\n",
    "    \n",
    "    # Get the left edge of the bins to use later as index (after grouped)\n",
    "    edges_out =int_to_ts(test_df['int_bins'])#.apply(lambda x: int_to_ts(x))    \n",
    "    test_df['bin_edges'] = edges_out#pd.to_datetime(edges_out)\n",
    "\n",
    "    # bin codes to labels \n",
    "    bin_codes = [(k,v) for k,v in bin_codes.items()]\n",
    "    \n",
    "    return test_df, bin_codes\n",
    "\n",
    "\n",
    "\n",
    "def concatenate_group_data(group_df_or_series):\n",
    "    \"\"\"Accepts a series or dataframe from a groupby.get_group() loop.\n",
    "    Adds TweetFreq column for # of rows concatenate. If input is series, \n",
    "    TweetFreq=1 and series is returned.\"\"\"\n",
    "    \n",
    "    import pandas as pd\n",
    "    from pandas.api import types as tp\n",
    "    \n",
    "    if isinstance(group_df_or_series, pd.Series):\n",
    "        \n",
    "        group_data = group_df_or_series\n",
    "        \n",
    "#         group_data.index = group_df_or_series.index\n",
    "        group_data['TweetFreq'] = 1\n",
    "\n",
    "        return group_data\n",
    "    \n",
    "    # if the group is a dataframe:\n",
    "    elif isinstance(group_df_or_series, pd.DataFrame):\n",
    "        \n",
    "        df = group_df_or_series\n",
    "        \n",
    "        # create an output series to collect combined data\n",
    "        group_data = pd.Series(index=df.columns)\n",
    "        group_data['TweetFreq'] = df.shape[0]\n",
    "        \n",
    "\n",
    "        for col in df.columns:\n",
    "            \n",
    "            combined=[]\n",
    "            col_data = []\n",
    "            \n",
    "            col_data = df[col]\n",
    "            combined=col_data.values\n",
    "            \n",
    "            group_data[col] = combined\n",
    "\n",
    "    return group_data\n",
    "\n",
    "\n",
    "def collapse_df_by_group_index_col(twitter_df,group_index_col='int_bins', new_col_order=None):\n",
    "    \"\"\"Loops through the group_indices provided to concatenate each group into\n",
    "    a single row and combine into one dataframe with the ______ as the index\"\"\"\n",
    "\n",
    "    import pandas as pd\n",
    "\n",
    "\n",
    "    # Create a Panel to temporarily hold the group series and dataframes\n",
    "    # group_dict_to_df = {}\n",
    "    # create a dataframe with same columns as twitter_df, and index=group ids from twitter_groups\n",
    "        \n",
    "    group_indices = twitter_df.groupby(group_index_col).groups\n",
    "    group_indices = [(k,v) for k,v in group_indices.items()]\n",
    "    group_df_index = [x[0] for x in group_indices]\n",
    "    \n",
    "    \n",
    "    # Create empty shell of twitter_grouped dataframe\n",
    "    twitter_grouped = pd.DataFrame(columns=twitter_df.columns, index=group_df_index)\n",
    "    twitter_grouped['TweetFreq'] =0\n",
    "\n",
    "    \n",
    "    # Loop through each group_indices\n",
    "    for (idx,group_members) in group_indices:\n",
    "\n",
    "        group_df = twitter_df.loc[group_members]\n",
    "\n",
    "        # Call on concatenate_group_data to handle the merging of rows\n",
    "        combined_series = concatenate_group_data(group_df)\n",
    "\n",
    "#         twitter_grouped.loc[idx,:] = combined_series\n",
    "        twitter_grouped.loc[idx] = combined_series#.values\n",
    "\n",
    "    # Update Column order, if requested, otherwise return twitter_grouped\n",
    "    if new_col_order==None:\n",
    "        return twitter_grouped\n",
    "    else:\n",
    "        df_out = twitter_grouped[new_col_order].copy()\n",
    "        df_out.index = group_df_index#twitter_grouped.index\n",
    "        return df_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# ofst.rollback(twitter_df.index[0].)\n",
    "def test_ofst_rolling(i=0):\n",
    "    \"\"\"Tool to help visualize the effect of offsets when deterimining a time interval range.\"\"\"\n",
    "    ofst = pd.offsets.CustomBusinessHour(start='09:30',end='16:30',)\n",
    "    print('TS: \\t\\t',twitter_df.index[i])\n",
    "    print('TS.floor(H): \\t',twitter_df.index[i].floor('H'))\n",
    "    print('TS rollback: \\t\\t',ofst.rollback(twitter_df.index[i]))\n",
    "    print('TS rollforward: \\t',ofst.rollforward(twitter_df.index[i]))\n",
    "    print('\\n')\n",
    "    print('TS floor(H)-rollback: \\t',ofst.rollback(twitter_df.index[i].floor('H')))\n",
    "    print('TS floor(H)-rollforward: ',ofst.rollforward(twitter_df.index[i].floor('H')))\n",
    "\n",
    "test_ofst_rolling(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# ## ORIGINAL CODE FRAMEWORK\n",
    "# half_hour_intervals = make_half_hour_range(twitter_df)\n",
    "\n",
    "# twitter_df, bin_codes = bin_df_by_date_intervals(twitter_df, half_hour_intervals)\n",
    "\n",
    "# ## NOT A FUNCTION:\n",
    "# group_indices = twitter_df.groupby('int_bins').groups\n",
    "# group_indices = [(k,v) for k,v in group_indices.items()]\n",
    "\n",
    "# twitter_grouped = collapse_df_by_group_indices(twitter_df, group_indices)#, new_col_order=new_col_order)\n",
    "# # twitter_grouped.head(2)\n",
    "\n",
    "# twitter_grouped['time_bin'] = twitter_grouped['left_edge'].apply(lambda x: pd.to_datetime(x[0]))\n",
    "# twitter_grouped.set_index('time_bin',drop=True, inplace=True)\n",
    "# twitter_grouped.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns',None)\n",
    "# display(twitter_df.head(2), twitter_df.tail(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## NEW CODE FRAMEWORK\n",
    "time_intervals = make_time_index_intervals(twitter_df)\n",
    "\n",
    "twitter_df, bin_codes = bin_df_by_date_intervals(twitter_df,time_intervals)\n",
    "\n",
    "\n",
    "twitter_grouped = collapse_df_by_group_index_col(twitter_df, group_index_col='int_bins')\n",
    "display(twitter_grouped.head())\n",
    "# twitter_grouped['time_bin'] = twitter_grouped['left_edge'].apply(lambda x: pd.to_datetime(x[0]))\n",
    "# twitter_grouped.set_index('time_bin',drop=True, inplace=True)\n",
    "# twitter_grouped.head(2)\n",
    "\n",
    "# CBH = custom_BH_freq()\n",
    "\n",
    "# grouped_resampled = twitter_grouped.loc['02-21-2017':].asfreq(CBH)\n",
    "# stock_resampled = stock_df.loc['02-21-2017':].asfreq(CBH)\n",
    "# # grouped_resampled = grouped_resampled.loc[stock_df.index[0]:stock_df.index[-1]]\n",
    "\n",
    "# grouped_resampled['join_date'] = grouped_resampled.index\n",
    "# stock_df['join_date'] =  stock_df.index\n",
    "\n",
    "# print(grouped_resampled.join_date)\n",
    "# print(stock_df.join_date)\n",
    "\n",
    "# # print(len(grouped_resampled),len(twitter_grouped))\n",
    "\n",
    "# # compare_indices = [True for i in grouped_resampled.index if i in stock_df.index]\n",
    "# # np.sum(compare_indices==False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #***#\n",
    "# new_col_order = ['date','left_edge','content_raw','content_stopped','tokens_stopped',\n",
    "#                   'retweet_count','favorite_count','case_ratio','sentiment_scores','compound_score','int_bins']\n",
    "# for col in new_col_order:\n",
    "#     print(col,'\\t',col in twitter_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# twitter_grouped = collapse_df_by_group_indices(twitter_df, group_indices)#, new_col_order=new_col_order)\n",
    "# twitter_grouped.head(2)\n",
    "\n",
    "twitter_grouped['time_bin'] = twitter_grouped['left_edge'].apply(lambda x: pd.to_datetime(x[0]))\n",
    "twitter_grouped.set_index('time_bin',drop=True, inplace=True)\n",
    "twitter_grouped.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ADDING TWITTER EXTRACTION TO MODEL-PREPARATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NEED TO HAVE DATA COMBINED ALREADY (THO DONT WANT TO SCALE??)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########## SET TRAINING AND TEST SET PARAMETERS: ############\n",
    "num_test_days=45 # Number of days for test data \n",
    "num_train_days=365 # Number of days for training data - 5 days/week * 52 weeks\n",
    "days_for_x_window = 5 # Number of days to \n",
    "############################################################\n",
    "\n",
    "# Calculate number of rows to bin for x_windows\n",
    "periods_per_day = ji.get_day_window_size_from_freq(stock_df, ji.custom_BH_freq()) # get the # of rows that == 1 day\n",
    "x_window = periods_per_day * days_for_x_window \n",
    "print(f'X_window size = {x_window} -- ({days_for_x_window} day(s) * {periods_per_day} rows/day)\\n')\n",
    "    \n",
    "###### TRAIN TEST SPLIT BY NUMBER OF DAYS ######\n",
    "df_train, df_test = ji.train_test_split_by_last_days(stock_df, periods_per_day=periods_per_day, num_test_days=num_test_days,\n",
    "                                                  num_train_days=num_train_days,verbose=1, plot=True)\n",
    "\n",
    "\n",
    "###### RESCALE DATA USING MinMaxScalers FIT ON TRAINING DATA's COLUMNS ######\n",
    "scaler_library , df_train = ji.make_scaler_library(df_train,transform=True)\n",
    "# Transform test data with train data's scaler_library\n",
    "df_test = ji.transform_cols_from_library(df_test,scaler_library)\n",
    "\n",
    "## Display Preview of Scaled Dataset\n",
    "# display(df_train.head(2).style.set_caption('df_train'),df_test.head(2).style.set_caption('df_test') )\n",
    "\n",
    "## REPLACE MADE_DF_TIMESERIES_BINS_BY_COLUMN with Kera's TimeseriesGenerator\n",
    "from keras.preprocessing.sequence import TimeseriesGenerator\n",
    "\n",
    "# Define shape of data by specifing these vars:\n",
    "n_input = x_window # Number of timebins to analyze at once. Note: try to choose # greater than length of seasonal cycles\n",
    "n_features= 1 # Number of columns\n",
    "batch_size = 1 # Generally 1 for sequence data\n",
    "\n",
    "# RESHAPING TRAINING AND TEST DATA \n",
    "train_data = df_train['price'].values.reshape(-1,1)\n",
    "test_data = df_test['price'].values.reshape(-1,1)\n",
    "train_data_index =  df_train['price'].index\n",
    "test_data_index = df_test['price'].index\n",
    "\n",
    "\n",
    "## Create Generator for Training Data\n",
    "train_generator = TimeseriesGenerator(data=train_data, targets=train_data,length=n_input, batch_size=batch_size )\n",
    "test_generator = TimeseriesGenerator(data=test_data, targets=test_data,length=n_input, batch_size=batch_size )\n",
    "\n",
    "\n",
    "# What does the first batch look like?\n",
    "X,y = train_generator[0]\n",
    "print(f'Given the Array: \\t(with shape={X.shape}) \\n{X.flatten()}')\n",
    "print(f'\\nPredict this y: \\n {y}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ycaO9AYe2bwX"
   },
   "source": [
    "# Twitter Processing \n",
    "\n",
    "- NLP Pre-Processing \n",
    "    - stopword removal and regexp tokenization\n",
    "    - extraction and removal of hashtags, @'s and urls' using regex\n",
    "- Word Frequency Disributions and Bigrams\n",
    "    - Word Frequencies - tweet, #'s\n",
    "    - Bigrams - tweets\n",
    "    - WordClouds - tweet, #'s, @'s\n",
    "\n",
    "- Upper to lower case ratio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "g2NuArSq3F0C"
   },
   "source": [
    "### NLP Pre-Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "e1ucmouPDnjo"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "colab": {},
    "colab_type": "code",
    "id": "m6Hfo5pn_rWG"
   },
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# folder_path = '/content/drive/My Drive/Colab Notebooks/Mod 5 Project/'\n",
    "\n",
    "# file = folder_path +'data/trump_tweets_01202017_06202019.csv'\n",
    "# df = pd.read_csv(file, encoding='utf-8')\n",
    "# df.rename(axis=1,mapper={'text':'content','created_at':'date'},inplace=True)\n",
    "# df['date']=pd.to_datetime(df['date'])\n",
    "\n",
    "# display(df.head())\n",
    "# print(' First tweet:',df.date.min(),'\\n','Last tweet:',df.date.max())\n",
    "# print('\\nRange of Dates: ',df.date.max() - df.date.min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "colab": {},
    "colab_type": "code",
    "id": "7VeZDUus2qWi"
   },
   "outputs": [],
   "source": [
    "# Generate Stopwords List from nltk + punctuation + custom list\n",
    "from nltk import regexp_tokenize\n",
    "\n",
    "pattern = \"([a-zA-Z]+(?:'[a-z]+)?)\"\n",
    "\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import string\n",
    "\n",
    "stopwords_list = stopwords.words('english')\n",
    "stopwords_list += list(string.punctuation)\n",
    "stopwords_list += ['http','https','...','…','``','co','“','’','‘','”',\"n't\",\"''\",'u','s',\"'s\",'|','\\\\|','amp',\"i'm\"]\n",
    "stopwords_list += [0,1,2,3,4,5,6,7,8,9]\n",
    "\n",
    "## Adding in stopword removal to the actual dataframe\n",
    "def apply_stopwords(stopwords_list,  text, tokenize=True, pattern = \"([a-zA-Z]+(?:'[a-z]+)?)\"):\n",
    "\n",
    "    if tokenize==True:\n",
    "        from nltk import regexp_tokenize\n",
    "        \n",
    "        text = regexp_tokenize(text,pattern)\n",
    "        \n",
    "    stopped = [x.lower() for x in text if x.lower() not in stopwords_list]\n",
    "    return ' '.join(stopped)\n",
    "\n",
    "# # Remove stopwords using function apply_stopwords)\n",
    "# df['text_stopped'] = df['content'].apply(lambda x: apply_stopwords(stopwords_list,x))\n",
    "\n",
    "# # Tokenize using regexp_tokenize from nltk\n",
    "# df['tokens_stopped'] = df['content'].apply(lambda x: regexp_tokenize(x,pattern))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "colab": {},
    "colab_type": "code",
    "id": "TxRjqjxF2w5c"
   },
   "outputs": [],
   "source": [
    "# Save 'hashtags' column containing all hastags\n",
    "import re\n",
    "df['content_raw'] = df['content'].copy()\n",
    "\n",
    "# Add has_RT and starts_RT columns\n",
    "# Creating columns for tweets that `has_RT` or `starts_RT`\n",
    "df['has_RT']=df['content_raw'].str.contains('RT')\n",
    "df['starts_RT']=df['content_raw'].str.contains('^RT')\n",
    "\n",
    "## FIRST REMOVE THE RT HEADERS\n",
    "\n",
    "# Remove `RT @Mentions` FIRST:\n",
    "re_RT = re.compile('RT [@]?\\w*:')\n",
    "\n",
    "raw_col =  'content_raw'\n",
    "check_content_col =raw_col\n",
    "fill_content_col = 'content'\n",
    "\n",
    "df['content_starts_RT'] = df[check_content_col].apply(lambda x: re_RT.findall(x))\n",
    "df[fill_content_col] =  df[check_content_col].apply(lambda x: re_RT.sub(' ',x))\n",
    "\n",
    "\n",
    "## SECOND REMOVE URLS\n",
    "# Remove urls with regex\n",
    "urls = re.compile(r\"(http[s]?://\\w*\\.\\w*/+\\w+)\")\n",
    "\n",
    "check_content_col = 'content'\n",
    "fill_content_col = 'content'\n",
    "\n",
    "# df_full['content_urls'] = df_full[check_content_col].apply(lambda x: urls.findall(x))\n",
    "df[fill_content_col] =  df[check_content_col].apply(lambda x: urls.sub(' ',x))\n",
    "\n",
    "## SAVE THIS MINIMALLY CLEANED CONTENT AS 'content_min_clean'\n",
    "df['content_min_clean'] =  df[fill_content_col]\n",
    "\n",
    "\n",
    "\n",
    "## REMOVE AND SAVE HASHTAGS, MENTIONS\n",
    "# Remove and save Hashtags\n",
    "hashtags = re.compile(r'\\#\\w*')\n",
    "\n",
    "check_content_col = 'content'\n",
    "fill_content_col = 'content'\n",
    "\n",
    "df['content_hashtags'] =  df[check_content_col].apply(lambda x: hashtags.findall(x))\n",
    "df[fill_content_col] =  df[check_content_col].apply(lambda x: hashtags.sub(' ',x))\n",
    "\n",
    "\n",
    "# Remove and save mentions (@)'s\n",
    "mentions = re.compile(r'\\@\\w*')\n",
    "\n",
    "check_content_col = 'content'\n",
    "fill_content_col = 'content'\n",
    "\n",
    "df['content_mentions'] =  df[check_content_col].apply(lambda x: mentions.findall(x))\n",
    "df[fill_content_col] =  df[check_content_col].apply(lambda x: mentions.sub(' ',x))\n",
    "\n",
    "\n",
    "# Creating content_stopped columns and then tokens_stopped column\n",
    "df['content_stopped'] = df['content'].apply(lambda x: apply_stopwords(stopwords_list,x))\n",
    "df['tokens_stopped'] = df['content_stopped'].apply(lambda x: regexp_tokenize(x,pattern))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "colab": {},
    "colab_type": "code",
    "id": "lKUUzr7b2zK7"
   },
   "outputs": [],
   "source": [
    "# # Issue of empty entries for hashtags interfering with joining.\n",
    "# def empty_lists_to_strings(x):\n",
    "#     \"\"\"Takes a series and replaces any empty lists with an empty string instead.\"\"\"\n",
    "#     if len(x)==0:\n",
    "#         return ' '\n",
    "#     else:\n",
    "#         return ' '.join(x) #' '.join(tokens)\n",
    "    \n",
    "    \n",
    "# # Apply empty_lists_to_strings to hashtags\n",
    "# df['hashtag_strings'] = df['content_hashtags'].apply(lambda x: empty_lists_to_strings(x))\n",
    "\n",
    "# # Apply empty_lists_to_strings to mentions\n",
    "# df['mention_strings'] = df['content_mentions'].apply(lambda x: empty_lists_to_strings(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5XmuzPuE3RVi"
   },
   "source": [
    "### Word Frequency Distributions & Bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nJp9qwlf3UOM"
   },
   "outputs": [],
   "source": [
    "# FOR TWEET WORD CONTENT\n",
    "column = 'content_stopped'\n",
    "pattern = \"([a-zA-Z]+(?:'[a-z]+)?)\"\n",
    "\n",
    "tweets_combined = df[column]\n",
    "tweets_combined = ' '.join(tweets_combined)\n",
    "tweets_tokenized = regexp_tokenize(tweets_combined, pattern)\n",
    "\n",
    "# CREATING TEXT DICT FOR FREQUENCY DISTRIBUTIONS\n",
    "TEXT = dict()\n",
    "TEXT['tokens'] = tweets_tokenized\n",
    "TEXT['text'] = tweets_combined\n",
    "\n",
    "# Frequency Distributions with NLTK FreqDist\n",
    "from nltk import FreqDist\n",
    "freq_tweets = FreqDist(TEXT['tokens'])\n",
    "with plt.style.context('seaborn-notebook'):\n",
    "    freq_tweets.plot(25)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "V6TnSmLc5g5W"
   },
   "outputs": [],
   "source": [
    "# MAKE BIGRAMS\n",
    "from nltk.collocations import BigramAssocMeasures, BigramCollocationFinder, TrigramAssocMeasures, TrigramCollocationFinder\n",
    "import mod4functions_JMI as jmi\n",
    "\n",
    "bigram_measures =BigramAssocMeasures()\n",
    "\n",
    "tweet_finder = BigramCollocationFinder.from_words(TEXT['tokens'])\n",
    "tweets_scored = tweet_finder.score_ngrams(bigram_measures.raw_freq)\n",
    "\n",
    "df_1 = quick_table(tweets_scored[:25], col_names =['Bigram','Frequency'],\n",
    "                       caption='Tweet Bigrams', display_df=False)\n",
    "df_1['Bigram'] = df_1['Bigram'].apply(lambda x: ' '.join(x))\n",
    "df_1.set_index('Bigram',inplace=True)\n",
    "df_1.columms=['Frequency']\n",
    "df_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BbDVZ_p33aI3"
   },
   "outputs": [],
   "source": [
    "# CREATING HASHTAG DICT\n",
    "column = 'hashtag_strings'\n",
    "tag_pattern ='(#\\w*)'\n",
    "tags_combined = df[column]\n",
    "tags_combined = ' '.join(tags_combined)\n",
    "tags_tokenized = regexp_tokenize(tags_combined, tag_pattern)\n",
    "\n",
    "TAGS = dict()\n",
    "TAGS['tokens'] = tags_tokenized\n",
    "TAGS['text'] = tags_combined\n",
    "\n",
    "# Frequency Distributions with NLTK FreqDist\n",
    "from nltk import FreqDist\n",
    "freq_tags = FreqDist(TAGS['tokens'])\n",
    "with plt.style.context('seaborn-notebook'):\n",
    "    freq_tags.plot(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MoENSWp53Zus"
   },
   "outputs": [],
   "source": [
    "# CREATING METNION DICT\n",
    "column = 'mention_strings'\n",
    "at_pattern ='(@\\w*)'\n",
    "ats_combined = df[column]\n",
    "ats_combined = ' '.join(ats_combined)\n",
    "ats_tokenized = regexp_tokenize(ats_combined, at_pattern)\n",
    "\n",
    "ATS = dict()\n",
    "ATS['tokens'] = ats_tokenized\n",
    "ATS['text'] = ats_combined\n",
    "\n",
    "# Frequency Distributions with NLTK FreqDist\n",
    "from nltk import FreqDist\n",
    "freq_ats = FreqDist(ATS['tokens'])\n",
    "with plt.style.context('seaborn-notebook'):\n",
    "    freq_ats.plot(25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zcT2d6BM34tw"
   },
   "source": [
    "### Word Clouds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WBl4VbWA39QT"
   },
   "outputs": [],
   "source": [
    "# Run this cell to mount your Google Drive.\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "byu_8zlD36ru"
   },
   "outputs": [],
   "source": [
    "# Import mask images for shaped wordclouds\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "mask_folder_path = '/content/drive/My Drive/Images/fig_masks/'\n",
    "## Twitter Bird masks\n",
    "mask_f_right = np.array(Image.open(mask_folder_path+'twitter1.png'))\n",
    "mask_f_left = np.array(Image.open(mask_folder_path+'twitter1flip.png'))\n",
    "\n",
    "# Hashtag and mentions mask \n",
    "mask_at = np.array(Image.open(mask_folder_path+'Hashtags and Ats Masks-04.jpg'))\n",
    "mask_hashtag = np.array(Image.open(mask_folder_path+'Hashtags and Ats Masks-03.jpg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PrwgjEAD5CH6"
   },
   "outputs": [],
   "source": [
    "# Define wordcloud plotting function\n",
    "def plot_fit_cloud(troll_cloud,label1='Most Common Words',figsize=(4,4)):\n",
    "    fig,ax = plt.subplots(nrows=1,ncols=1,figsize=figsize)\n",
    "\n",
    "    ax.imshow(troll_cloud, interpolation='gaussian')\n",
    "    # ax[0].set_aspect(1.5)\n",
    "    ax.axis(\"off\")\n",
    "    ax.set_title(label1, fontsize=20)\n",
    "    return fig, ax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "moHVtJZE5Ud5"
   },
   "source": [
    "#### Creating multiple kinds of wordclouds\n",
    "- tweet words, tweet bigrams\n",
    "- #'s, @'s\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "F0iWyiWb5M8L"
   },
   "outputs": [],
   "source": [
    "# Instantiazting wordcloud and defining properties\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "max_font_size=100\n",
    "width=300\n",
    "height=300\n",
    "max_words=100\n",
    "background_color='white'\n",
    "cloud_stopwords=[]\n",
    "# collocations=False\n",
    "\n",
    "wordcloud = WordCloud(max_font_size=max_font_size, width=width, height=height, max_words=max_words, background_color=background_color,\n",
    "                        stopwords=cloud_stopwords,collocations=False,\n",
    "                       mask=mask_f_right, contour_color='cornflowerblue', contour_width=2)\n",
    "\n",
    "wordcloud.generate(TEXT['text'])\n",
    "fig,ax = plot_fit_cloud(wordcloud)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xssXsAiW5ZOf"
   },
   "outputs": [],
   "source": [
    "# Plotting bigram wordcloud\n",
    "wordcloud_bigram= WordCloud(max_font_size=max_font_size, width=width, height=height, max_words=max_words,\n",
    "                            background_color=background_color, collocations=True,normalize_plurals=False,\n",
    "                            stopwords=[], mask=mask_f_left, contour_color='cornflowerblue', contour_width=2)\n",
    "wordcloud_bigram.generate(TEXT['text'])\n",
    "fig,ax = plot_fit_cloud(wordcloud_bigram, 'Most Common Bigrams')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "leIwmAXh5bpr"
   },
   "outputs": [],
   "source": [
    "# Combine and Tokenize hashtags\n",
    "tags = df['hashtag_strings']\n",
    "tags = ' '.join(tags)\n",
    "tags_tokens = regexp_tokenize(tags,'(#\\w*)')\n",
    "\n",
    "\n",
    "# WordClouds for Hashtags\n",
    "max_font_size=300\n",
    "width=300\n",
    "height=300\n",
    "max_words=100\n",
    "background_color='white'\n",
    "cloud_stopwords=[]\n",
    "collocations=False\n",
    "regexp=r'(#\\w*)'\n",
    "\n",
    "\n",
    "tag_cloud = WordCloud(max_font_size=max_font_size, width=width, height=height,\n",
    "                            max_words=max_words, background_color=background_color,\n",
    "                        stopwords=cloud_stopwords,collocations=collocations, regexp=regexp,\n",
    "                            mask=mask_hashtag, contour_color='black', contour_width=2)#, include_numbers=True)\n",
    "\n",
    "\n",
    "tag_cloud.generate(' '.join(tags_tokens))\n",
    "plot_fit_cloud(tag_cloud,'Most Common Hashtags',figsize=(5,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eBzbqDQ75cy1"
   },
   "outputs": [],
   "source": [
    "# Combine and Tokenize hashtags\n",
    "mentions = df['mention_strings']\n",
    "mentions = ' '.join(mentions)\n",
    "mentions_tokens = regexp_tokenize(mentions,'(@\\w*)')\n",
    "\n",
    "\n",
    "# WordClouds for Mentions\n",
    "max_font_size=300\n",
    "width=500\n",
    "height=500\n",
    "max_words=200\n",
    "background_color='white'\n",
    "cloud_stopwords=[]\n",
    "collocations=False\n",
    "regexp=r'(@\\w*)'\n",
    "\n",
    "mentions_cloud = WordCloud(\n",
    "    \n",
    "    max_font_size=max_font_size, width=width, height=height,\n",
    "    \n",
    "    max_words=max_words, background_color=background_color,\n",
    "    \n",
    "    stopwords=cloud_stopwords,collocations=collocations, regexp=regexp,\n",
    "    \n",
    "    mask=mask_at, contour_color='black', contour_width=2\n",
    ")#, include_numbers=True)\n",
    "\n",
    "\n",
    "mentions_cloud.generate(' '.join(mentions_tokens))\n",
    "\n",
    "plot_fit_cloud(mentions_cloud,\"Most Common @'s\",figsize=(3,3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Tnj6xKrr3ffM"
   },
   "source": [
    "### Additional Tweet Processing and Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XVSPinbb2fBg"
   },
   "outputs": [],
   "source": [
    "def case_ratio(msg):\n",
    "    \"\"\"Accepts a twitter message (or used with .apply(lambda x:)).\n",
    "    Returns the ratio of capitalized characters out of the total number of characters.\"\"\"\n",
    "    import numpy as np\n",
    "    msg_length = len(msg)\n",
    "    test_upper = [1 for x in msg if x.isupper()]\n",
    "    test_lower = [1 for x in msg if x.islower()]\n",
    "    test_ratio = np.round(sum(test_upper)/msg_length,5)\n",
    "    return test_ratio\n",
    "\n",
    "# df['case_ratio'] = df['content'].apply(lambda x: case_ratio(x))\n",
    "# df.sort_values('case_ratio',ascending=False).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-IvNp_Ov3mla"
   },
   "source": [
    "### Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## NEW 07/11/19 - function for all sentiment analysis\n",
    "\n",
    "def full_sentiment_analysis(twitter_df, source_column='content_min_clean',separate_cols=True):#, plot_results=True):\n",
    "    from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "    sid = SentimentIntensityAnalyzer()\n",
    "\n",
    "    source_column='content_min_clean'\n",
    "    twitter_df['sentiment_scores'] = twitter_df[source_column].apply(lambda x: sid.polarity_scores(x))\n",
    "    twitter_df['compound_score'] = twitter_df['sentiment_scores'].apply(lambda dict: dict['compound'])\n",
    "    twitter_df['sentiment_class'] = twitter_df['compound_score'].apply(lambda score: 'pos' if score >=0 else 'neg')\n",
    "    \n",
    "    \n",
    "    # Separate result dictionary into columns  (optional)\n",
    "    if separate_cols==True:\n",
    "        # Separate Scores into separate columns in df\n",
    "        twitter_df_out = get_group_sentiment_scores(twitter_df)\n",
    "    else:\n",
    "        twitter_df_out = twitter_df\n",
    "        \n",
    "    \n",
    "#     # plot results (optional)\n",
    "#     if plot_results==True:\n",
    "        \n",
    "#         print(\"RESULTS OF SENTIMENT ANALYSIS BINARY CLASSIFICATION:\\n\",'-'*60)\n",
    "#         # Normalized % of troll sentiment classes\n",
    "#         plot_sent_class = twitter_df_out['sentiment_class'].value_counts()\n",
    "#         plot_sent_class_norm = plot_sent_class/(sum(plot_sent_class))\n",
    "#         print('\\tNormalized Troll Classes:\\n',plot_sent_class_norm)\n",
    "\n",
    "\n",
    "#         with plt.style.context('seaborn-notebook'):\n",
    "#             boxplot = df_sents.boxplot(column=['neg','neu','pos'],notch=True,figsize=(6,4))\n",
    "#             boxplot.set_xticklabels(['Negative','Neutral','Positive']);\n",
    "#             boxplot.set_title('Sentiment Scores By Word Type')\n",
    "#             boxplot.set_ylabel('Sentiment Score')\n",
    "    \n",
    "    return twitter_df_out\n",
    "        \n",
    "        \n",
    "\n",
    "# Write a function to extract the group scores from the dataframe\n",
    "def get_group_sentiment_scores(df, score_col='sentiment_scores'):\n",
    "    import pandas as pd\n",
    "    series_df = df[score_col]\n",
    "    series_neg = series_df.apply(lambda x: x['neg'])\n",
    "    series_pos = series_df.apply(lambda x: x['pos'])\n",
    "    series_neu = series_df.apply(lambda x: x['neu'])\n",
    "    \n",
    "    series_neg.name='neg'\n",
    "    series_pos.name='pos'\n",
    "    series_neu.name='neu'\n",
    "    \n",
    "    df = pd.concat([df,series_neg,series_neu,series_pos],axis=1)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9_usSqQn3pVw"
   },
   "outputs": [],
   "source": [
    "# import bs_ds as bs\n",
    "# import mod4functions_JMI as jmi\n",
    "# # from bs_ds.imports import *\n",
    "\n",
    "# from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "# sid = SentimentIntensityAnalyzer()\n",
    "\n",
    "# source_column='content_min_clean'\n",
    "# df['sentiment_scores'] = df[source_column].apply(lambda x: sid.polarity_scores(x))\n",
    "# df['compound_score'] = df['sentiment_scores'].apply(lambda dict: dict['compound'])\n",
    "# df['sentiment_class'] = df['compound_score'].apply(lambda score: 'pos' if score >=0 else 'neg')\n",
    "\n",
    "\n",
    "# # Separate Scores into separate columns in df\n",
    "# df = get_group_sentiment_scores(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uNFTfMnI6Fy4"
   },
   "outputs": [],
   "source": [
    "\n",
    "print(\"RESULTS OF SENTIMENT ANALYSIS BINARY CLASSIFICATION:\\n\",'-'*60)\n",
    "# Normalized % of troll sentiment classes\n",
    "plot_sent_class = df['sentiment_class'].value_counts()\n",
    "plot_sent_class_norm = plot_sent_class/(sum(plot_sent_class))\n",
    "print('\\tNormalized Troll Classes:\\n',plot_sent_class_norm)\n",
    "\n",
    "\n",
    "with plt.style.context('seaborn-notebook'):\n",
    "    boxplot = df_sents.boxplot(column=['neg','neu','pos'],notch=True,figsize=(6,4))\n",
    "    boxplot.set_xticklabels(['Negative','Neutral','Positive']);\n",
    "    boxplot.set_title('Sentiment Scores By Word Type')\n",
    "    boxplot.set_ylabel('Sentiment Score')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3JKCfxH48q6E"
   },
   "source": [
    "# COMBINING TWITTER AMD STOCK MARKET DATAFRAMES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HNGo3NiA9Lvt"
   },
   "outputs": [],
   "source": [
    "try: twitter_df\n",
    "except NameError: twitter_df = None\n",
    "    \n",
    "if twitter_df is None:\n",
    "    print('loading twitter_df')\n",
    "    twitter_df = pd.read_csv('data/trump_twitter_archive_df.csv', encoding='utf-8',index_col='date',parse_dates=True)\n",
    "    twitter_df.drop('Unnamed: 0',axis=1,inplace=True)\n",
    "\n",
    "    # Fill in missing values before merging with stock data\n",
    "    twitter_df.fillna('', inplace=True)\n",
    "    twitter_df.sort_index(ascending=True, inplace=True)\n",
    "\n",
    "    # Check header and daterange of index\n",
    "    display(twitter_df.head(2))\n",
    "    twitter_df.index[[0,-1]]\n",
    "else:\n",
    "    print('twitter_df already exists.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WRITING FUNCTION TO BIN TWEETS BY HOUR PRE-NLP (and add Freq column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ltdTPiv18UiQ"
   },
   "outputs": [],
   "source": [
    "# twitter_df = pd.read_csv(folder_path+'data/trump_twitter_archive_df.csv', encoding='utf-8',index_col='date',parse_dates=True)\n",
    "# twitter_df.drop('Unnamed: 0',axis=1,inplace=True)\n",
    "\n",
    "# # Fill in missing values before merging with stock data\n",
    "# twitter_df.fillna('', inplace=True)\n",
    "# twitter_df.sort_index(ascending=True, inplace=True)\n",
    "\n",
    "# # Check header and daterange of index\n",
    "# display(twitter_df.head(2))\n",
    "# twitter_df.index[[0,-1]]"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "xNf73gC9daS-",
    "eE3D-Avztybj"
   ],
   "name": "Capstone Project Outline + Analysis.ipynb",
   "provenance": [],
   "toc_visible": true,
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "learn-env-ext",
   "language": "python",
   "name": "learn-env-ext"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "389.125px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": "25"
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "oldHeight": 265.340454,
   "position": {
    "height": "40px",
    "left": "751.591px",
    "right": "20px",
    "top": "49px",
    "width": "607.773px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "varInspector_section_display": "none",
   "window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
